{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview of the documentation Package list","title":"Overview"},{"location":"#overview-of-the-documentation","text":"Package list","title":"Overview of the documentation"},{"location":"generated/package_list/","text":"Package list a ABINIT Amber ANTs archspec arpack-ng ASE AutoDock_Vina b baselibs BCFtools BEDTools BerkeleyGW binutils BioTools-Python BioTools BLAST+ Bowtie2 buildtools-systempython buildtools BWA c canu CD-HIT ClonalFrameML CP2K d DFTB+ double-conversion DSSP e Eigen Elk ELPA ELSI f fastp FLAC Flye g GATK GenomeTools gnuplot GPAW GROMACS h HTSeq HTSlib i i-PI IntelPython3-Packages IntelPython3 IOzone j Java k kim-api Kraken2 l LAMMPS libgd libogg librosa libsndfile libtheora libvdwxc libvorbis LMDB m Maple Mathematica MATLAB MEGAHIT miniasm minimap2 molmod monitor n NAMD NEST netCDF NetPyNE NTPoly numba o OpenFOAM OpenMX p Pandoc parallel ParaView Perl phonopy PLUMED PROJ protobuf Pysam Python q Quantum-KITE QuantumESPRESSO r R Racon RAxML-NG re2c Roary s SAMtools ScaFaCos SCons SCOTCH SMALT snappy SPAdes SuperLU_DIST t TensorFlow Towhee Trimmomatic u Unicycler v VSEARCH w Wannier90 WannierTools x X11 y yaff Yambo","title":"Package list"},{"location":"generated/package_list/#package-list","text":"","title":"Package list"},{"location":"generated/package_list/#a","text":"ABINIT Amber ANTs archspec arpack-ng ASE AutoDock_Vina","title":"a"},{"location":"generated/package_list/#b","text":"baselibs BCFtools BEDTools BerkeleyGW binutils BioTools-Python BioTools BLAST+ Bowtie2 buildtools-systempython buildtools BWA","title":"b"},{"location":"generated/package_list/#c","text":"canu CD-HIT ClonalFrameML CP2K","title":"c"},{"location":"generated/package_list/#d","text":"DFTB+ double-conversion DSSP","title":"d"},{"location":"generated/package_list/#e","text":"Eigen Elk ELPA ELSI","title":"e"},{"location":"generated/package_list/#f","text":"fastp FLAC Flye","title":"f"},{"location":"generated/package_list/#g","text":"GATK GenomeTools gnuplot GPAW GROMACS","title":"g"},{"location":"generated/package_list/#h","text":"HTSeq HTSlib","title":"h"},{"location":"generated/package_list/#i","text":"i-PI IntelPython3-Packages IntelPython3 IOzone","title":"i"},{"location":"generated/package_list/#j","text":"Java","title":"j"},{"location":"generated/package_list/#k","text":"kim-api Kraken2","title":"k"},{"location":"generated/package_list/#l","text":"LAMMPS libgd libogg librosa libsndfile libtheora libvdwxc libvorbis LMDB","title":"l"},{"location":"generated/package_list/#m","text":"Maple Mathematica MATLAB MEGAHIT miniasm minimap2 molmod monitor","title":"m"},{"location":"generated/package_list/#n","text":"NAMD NEST netCDF NetPyNE NTPoly numba","title":"n"},{"location":"generated/package_list/#o","text":"OpenFOAM OpenMX","title":"o"},{"location":"generated/package_list/#p","text":"Pandoc parallel ParaView Perl phonopy PLUMED PROJ protobuf Pysam Python","title":"p"},{"location":"generated/package_list/#q","text":"Quantum-KITE QuantumESPRESSO","title":"q"},{"location":"generated/package_list/#r","text":"R Racon RAxML-NG re2c Roary","title":"r"},{"location":"generated/package_list/#s","text":"SAMtools ScaFaCos SCons SCOTCH SMALT snappy SPAdes SuperLU_DIST","title":"s"},{"location":"generated/package_list/#t","text":"TensorFlow Towhee Trimmomatic","title":"t"},{"location":"generated/package_list/#u","text":"Unicycler","title":"u"},{"location":"generated/package_list/#v","text":"VSEARCH","title":"v"},{"location":"generated/package_list/#w","text":"Wannier90 WannierTools","title":"w"},{"location":"generated/package_list/#x","text":"X11","title":"x"},{"location":"generated/package_list/#y","text":"yaff Yambo","title":"y"},{"location":"generated/a/ABINIT/package/","text":"ABINIT Technical documentation","title":"ABINIT"},{"location":"generated/a/ABINIT/package/#abinit","text":"Technical documentation","title":"ABINIT"},{"location":"generated/a/ANTs/package/","text":"ANTs Technical documentation","title":"ANTs"},{"location":"generated/a/ANTs/package/#ants","text":"Technical documentation","title":"ANTs"},{"location":"generated/a/ASE/package/","text":"ASE Technical documentation","title":"ASE"},{"location":"generated/a/ASE/package/#ase","text":"Technical documentation","title":"ASE"},{"location":"generated/a/Amber/package/","text":"Amber Technical documentation","title":"Amber"},{"location":"generated/a/Amber/package/#amber","text":"Technical documentation","title":"Amber"},{"location":"generated/a/AutoDock_Vina/package/","text":"AutoDock_Vina Technical documentation","title":"AutoDock_Vina"},{"location":"generated/a/AutoDock_Vina/package/#autodock_vina","text":"Technical documentation","title":"AutoDock_Vina"},{"location":"generated/a/archspec/package/","text":"archspec Technical documentation","title":"archspec"},{"location":"generated/a/archspec/package/#archspec","text":"Technical documentation","title":"archspec"},{"location":"generated/a/arpack-ng/package/","text":"arpack-ng Technical documentation","title":"arpack-ng"},{"location":"generated/a/arpack-ng/package/#arpack-ng","text":"Technical documentation","title":"arpack-ng"},{"location":"generated/b/BCFtools/package/","text":"BCFtools Technical documentation","title":"BCFtools"},{"location":"generated/b/BCFtools/package/#bcftools","text":"Technical documentation","title":"BCFtools"},{"location":"generated/b/BEDTools/package/","text":"BEDTools Technical documentation","title":"BEDTools"},{"location":"generated/b/BEDTools/package/#bedtools","text":"Technical documentation","title":"BEDTools"},{"location":"generated/b/BLAST%2B/package/","text":"BLAST+ Technical documentation","title":"BLAST+"},{"location":"generated/b/BLAST%2B/package/#blast","text":"Technical documentation","title":"BLAST+"},{"location":"generated/b/BWA/package/","text":"BWA Technical documentation","title":"BWA"},{"location":"generated/b/BWA/package/#bwa","text":"Technical documentation","title":"BWA"},{"location":"generated/b/BerkeleyGW/package/","text":"BerkeleyGW Technical documentation","title":"BerkeleyGW"},{"location":"generated/b/BerkeleyGW/package/#berkeleygw","text":"Technical documentation","title":"BerkeleyGW"},{"location":"generated/b/BioTools/package/","text":"BioTools Technical documentation","title":"BioTools"},{"location":"generated/b/BioTools/package/#biotools","text":"Technical documentation","title":"BioTools"},{"location":"generated/b/BioTools-Python/package/","text":"BioTools-Python Technical documentation","title":"BioTools-Python"},{"location":"generated/b/BioTools-Python/package/#biotools-python","text":"Technical documentation","title":"BioTools-Python"},{"location":"generated/b/Bowtie2/package/","text":"Bowtie2 Technical documentation","title":"Bowtie2"},{"location":"generated/b/Bowtie2/package/#bowtie2","text":"Technical documentation","title":"Bowtie2"},{"location":"generated/b/baselibs/package/","text":"baselibs Technical documentation","title":"baselibs"},{"location":"generated/b/baselibs/package/#baselibs","text":"Technical documentation","title":"baselibs"},{"location":"generated/b/binutils/package/","text":"binutils Technical documentation","title":"binutils"},{"location":"generated/b/binutils/package/#binutils","text":"Technical documentation","title":"binutils"},{"location":"generated/b/buildtools/package/","text":"buildtools Technical documentation","title":"buildtools"},{"location":"generated/b/buildtools/package/#buildtools","text":"Technical documentation","title":"buildtools"},{"location":"generated/b/buildtools-systempython/package/","text":"buildtools-systempython Technical documentation","title":"buildtools-systempython"},{"location":"generated/b/buildtools-systempython/package/#buildtools-systempython","text":"Technical documentation","title":"buildtools-systempython"},{"location":"generated/c/CD-HIT/package/","text":"CD-HIT Technical documentation","title":"CD-HIT"},{"location":"generated/c/CD-HIT/package/#cd-hit","text":"Technical documentation","title":"CD-HIT"},{"location":"generated/c/CP2K/package/","text":"CP2K Technical documentation","title":"CP2K"},{"location":"generated/c/CP2K/package/#cp2k","text":"Technical documentation","title":"CP2K"},{"location":"generated/c/ClonalFrameML/package/","text":"ClonalFrameML Technical documentation","title":"ClonalFrameML"},{"location":"generated/c/ClonalFrameML/package/#clonalframeml","text":"Technical documentation","title":"ClonalFrameML"},{"location":"generated/c/canu/package/","text":"canu Technical documentation","title":"canu"},{"location":"generated/c/canu/package/#canu","text":"Technical documentation","title":"canu"},{"location":"generated/d/DFTB%2B/package/","text":"DFTB+ Technical documentation","title":"DFTB+"},{"location":"generated/d/DFTB%2B/package/#dftb","text":"Technical documentation","title":"DFTB+"},{"location":"generated/d/DSSP/package/","text":"DSSP Technical documentation","title":"DSSP"},{"location":"generated/d/DSSP/package/#dssp","text":"Technical documentation","title":"DSSP"},{"location":"generated/d/double-conversion/package/","text":"double-conversion Technical documentation","title":"double-conversion"},{"location":"generated/d/double-conversion/package/#double-conversion","text":"Technical documentation","title":"double-conversion"},{"location":"generated/e/ELPA/package/","text":"ELPA Technical documentation","title":"ELPA"},{"location":"generated/e/ELPA/package/#elpa","text":"Technical documentation","title":"ELPA"},{"location":"generated/e/ELSI/package/","text":"ELSI Technical documentation","title":"ELSI"},{"location":"generated/e/ELSI/package/#elsi","text":"Technical documentation","title":"ELSI"},{"location":"generated/e/Eigen/package/","text":"Eigen Technical documentation","title":"Eigen"},{"location":"generated/e/Eigen/package/#eigen","text":"Technical documentation","title":"Eigen"},{"location":"generated/e/Elk/package/","text":"Elk Technical documentation","title":"Elk"},{"location":"generated/e/Elk/package/#elk","text":"Technical documentation","title":"Elk"},{"location":"generated/easyconfigs/","text":"UAntwerpen-easyconfigs This repository contains the EasyConfig files used at the UAntwerpen VSC site. They differ from the standard ones available in the easybuilders repositories in multiple ways. * We tend to include more information in the (LMOD) module files for module spider and module help * We tend to include design decisions in our EasyConfig files * We sometimes use bundles to put related software and libraries in a single module to reduce module clutter for our users or to simply make things work better. An example of the former are our buildtools modules that combine many standard build tools (CMake, autoconfig, GNU make, ...) compiled with the standard OS compilers and OS libraries. An example of the latter are our netCDF modules that include the base libraries and the Fortran and dual C++ interfaces in a single module. Not all EasyConfig files included in this repository are recipes that we actually install. Some were only used during the development of some of our bigger bundles but included in this repository for those who it may be useful for, as it does include packages that are not yet supported by the official EasyBuilders distribution. These recipes typically have the version suffix -baslibsDevel or -X11Devel after the bundle they are included in. See the Wiki of this repository for more informaton. It may be useful to also check our \"EasyBuild - The Missing Manual\" OneNote . Toolchains / build set 2019b toolchains / build set Given that we started building before EasyBuild 4.0 was released, we decided to stick with the 3.9.4 version as otherwise we got an unpleasant number of warnings about deprecated features. Yet we did start the EasyConfig files in such a way that they would be easily portable to the 4.x versions of EasyBuild. 2020a toolchains / build set For this build set we switched to EasyBuild 4.1 and later.","title":"UAntwerpen-easyconfigs"},{"location":"generated/easyconfigs/#uantwerpen-easyconfigs","text":"This repository contains the EasyConfig files used at the UAntwerpen VSC site. They differ from the standard ones available in the easybuilders repositories in multiple ways. * We tend to include more information in the (LMOD) module files for module spider and module help * We tend to include design decisions in our EasyConfig files * We sometimes use bundles to put related software and libraries in a single module to reduce module clutter for our users or to simply make things work better. An example of the former are our buildtools modules that combine many standard build tools (CMake, autoconfig, GNU make, ...) compiled with the standard OS compilers and OS libraries. An example of the latter are our netCDF modules that include the base libraries and the Fortran and dual C++ interfaces in a single module. Not all EasyConfig files included in this repository are recipes that we actually install. Some were only used during the development of some of our bigger bundles but included in this repository for those who it may be useful for, as it does include packages that are not yet supported by the official EasyBuilders distribution. These recipes typically have the version suffix -baslibsDevel or -X11Devel after the bundle they are included in. See the Wiki of this repository for more informaton. It may be useful to also check our \"EasyBuild - The Missing Manual\" OneNote .","title":"UAntwerpen-easyconfigs"},{"location":"generated/easyconfigs/#toolchains-build-set","text":"","title":"Toolchains / build set"},{"location":"generated/easyconfigs/#2019b-toolchains-build-set","text":"Given that we started building before EasyBuild 4.0 was released, we decided to stick with the 3.9.4 version as otherwise we got an unpleasant number of warnings about deprecated features. Yet we did start the EasyConfig files in such a way that they would be easily portable to the 4.x versions of EasyBuild.","title":"2019b toolchains / build set"},{"location":"generated/easyconfigs/#2020a-toolchains-build-set","text":"For this build set we switched to EasyBuild 4.1 and later.","title":"2020a toolchains / build set"},{"location":"generated/easyconfigs/packages_2020a/","text":"Package Last version 2020a version Where ABINIT 8.10.2-intel-2016b-hybrid-mkl 8.10.3-intel-2020a-hybrid-mkl, 9.0.4-* buildset ACTC 1.1-GCCcore-8.3.0 1.1-GCCcore-9.3.0 buildset Amber 18-intel-2019b-AmberTools-19-patchlevel-17-9 18-intel-2020a-AmberTools-19-patchlevel-17-12, 20 buildset ANSYS 17.2 Toolchain-independent x86_64 ant 1.10.1-Java-8 SYSTEM toolchain system ANTs 2.3.2-intel-2019b 2.3.3-intel-2020a buildset arpack-ng 3.6.3-intel-2019b 3.6.3-intel-2020a buildset ASE 3.18.1-intel-2019b-Python-3.7.4 3.19.1-intel-2020a-* buildset atools 1.4.7 SYSTEM toolchain system Autoconf 2.69 Moved to buildtools/2020a / AutoDock_Vina 1.1.2 SYSTEM toolchain system Automake 1.15 Moved to buildtools/2020a / Autotools 20161231 Moved to buildtools/2020a / BALi-Phy 3.0-beta4-intel-2017a-sequential Installed on demand buildset baslibs 2019b-GCCcore-8.3.0 2020a-GCCcore-9.3.0 buildset Bazel 0.5.3 SYSTEM toolchain system BCFtools 1.10.2-intel-2019b Moved into BioTools/2020a.00-intel-2020a / bcl2fastq2 2.20.0-intel-2018b Installed on demand buildset beagle-lib 3.1.2-intel-2019b(-OpenMP) 3.1.2-intel-202020a(-OpenMP) buildset Beast 2.6.0-intel-2019b 2.6.2-intel-2020a buildset BEDtools 2.27.1-intel-2018b Moved into BioTools/2020a.00-intel-2020a / behave 1.2.5-intel-2017a-Python-2.7.13 Installed on demand buildset BerkeleyGW 2.1-intel-2019b-hybrid 2.1-intel-2020a-hybrid buildset binutils 2.32-GCCcore-8.3.0 2.34-GCCcore-9.3.0 buildset BioPerl 1.7.2-intel-2018b-Perl-5.26.1 1.7.7-intel-2020a-Perl-5.30.2 buildset Biopython 1.74-intel-2019b-Python-3.7.4 Moved into BioTools-Python/2020a.00-intel-2020a-* / Bismark 0.22.1-intel-2019b 0.22.3-intel-2020a buildset Bison 3.0.5 Moved to buildtools/2020a / BLAST+ 2.9.0-intel-2018b-Python-3.7.1 2.10.0-intel-2020a buildset BoltzTraP 1.2.5-intel-2018a Installed on demand (suggest BoltzTraP2) buildset Boost 1.70.0-intel-2019b 1.73.0-intel-2020a buildset Bowtie2 2.3.5.1-intel-2019b 2.4.1-intel-2020a buildset buildtools 2019b 2020a system BWA 0.7.17-intel-2019b 0.7.17-intel-2020a buildset byacc 20170201 Moved to buildtools/2020a / bzip2 1.0.6-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / cairo 1.17.2-GCCcore-8.3.0 1.17.2-GCCcore-9.3.0 buildset canu 1.9-intel-2019b 2.0-intel-2020a buildset CD-HIT 4.6.8-intel-2018b 4.8.1, moved into BioTools buildset CellRanger 2.2.0-intel-2018b Move to system toolchain, update by user needed x86_64 CellRanger-DNA 1.1.0 SYSTEM toolchain TODO? x86_64 CGAL 4.10.2-intel-2019b-forOpenFOAM6 4.10.2-intel-2020a-forOpenFOAM6 buildset ClonalFrameML 1.11-intel-2018b 1.12, moved into BioTools buildset CMake 3.9.1 Moved to buildtools/2020a / COMSOL 5.3 Toolchain-independent x86_64 core-counter 1.1 1.1.1 (SYSTEM toolchain) system CP2K 7.1 in various variants 7.1 in various variants buildset CPMD 4.3-intel-2019b 4.3-intel-2020a-pl4624 buildset cppcheck 1.83-intel-2018a Installed on demand (and needs Qt5) buildset Cube 4.3.5-intel-2018a TODO - Awaiting Qt5 buildset CUDA 10.2.89 Toolchain-independent system cuDNN 7.6.5.32-CUDA-10.1.243 Toolchain-independent system cURL 7.61.0-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / Dacapo 476-intel-2018b Installed on demand, web site outdated buildset Dalton 2018.2-intel-2018b-i8 Who requested this???? buildset Darknet 20180326-intel-2018a-* Installed on demand buildset darshan-runtime 3.1.7-intel-2018b Installed on demand buildset darshan-util 3.1.7-intel-2018b Installed on demand buildset DDSCAT 7.3.2_170125-intel-2018a Installed on demand buildset Delft3D 7565-intel-2018b Installed on demand buildset DFTB+ 19.1-intel-2019b-Python-2.7.16(-MPI) 19.1-intel-2020a-Python-3.8.2(-MPI) buildset DIAMOND 0.9.22-intel-2018a Installed on demand buildset DLCpar 1.0-intel-2019b-Python-3.7.4 Moved into BioTools-Python/2020a.00-intel-2020a-* / DMTCP 2.6.0 Toolchain-independent? x86_64 Doxygen 1.8.13 Moved to buildtools/2020a / DSSP 2.2.1-intel-2017a 2.3.0-intel-2020a, 3.1.4-intel-2020a buildset EasyBuild 4.2.0 Toolchain-independent x86_64 eclipse parallel-2018-12 Installed on demand x86_64 Eigen 3.3.7-intel-2019b 3.3.7 x86_64 Elk 5.2.14-intel-2019b 6.3.2-intel-2020a buildset ELPA 2019.11.001-intel-2019b 2019.11.001-intel-2020a buildset ELSI 2.5.0-intel-2019b 2.5.0-intel-2020a buildset Exonerate 2.4.0-intel-2017a Installed on demand (unmaintained) buildset expat 2.2.5-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / Faiss 1.5.0-intel-2018b-Python-3.7.1-CPU Installed on demand buildset FastME 2.1.6.1-intel-2018a Installed on demand buildset fastp 0.20.0-intel-2019b Moved to BioTools/2020a.00-intel-2020a / FastTree 2.1.10-intel-2018b Moved to BioTools/2020a.00-intel-2020a buildset FFmpeg 4.1.4 (Intel and GCCcore) 4.2.2 (Intel and GCCcore) buildset FFTW 3.3.8-intel-2019b 3.3.38 (Intel 2 versions) buildset file 5.30-intel-2017a Moved to baselibs/2020a-GCCcore-9.3.0 / FINE-Marine 5.2 Installed on demand x86_64 flex 2.6.4 Moved to buildtools/2020a / FLTK 1.3.5-GCCcore-8.3.0-viz 1.3.5-GCCcore-9.3.0-viz buildset Flye 2.7-intel-2019b-Python-3.7.4 Moved into BioTools-Python/2020a.00-intel-2020a-* / fontconfig 2.12.6-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / foss 2018b Installed on demand buildset freetype 2.9-intel-2018a Moved to baselibs/2020a-GCCcore-9.3.0 / FriBidi 1.0.5-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / GATK 4.1.4.1-Java-8-minimal SYSTEM toolchain system Gaussian16 g16_A3-AVX2 g16_c01-avx2, g16_a03-avx2 ? GaussView / 6.1.1 ? gc 7.6.4-intel-2018a Moved to baselibs/2020a-GCCcore-9.3.0 / GCC 8.3.0-2.32 9.3.0-2.34 buildset GCCcore 8.3.0 9.3.0 buildset GDAL 3.0.1-intel-2019b 3.1.0-intel-2020a buildset GEOS 3.7.2-intel-2019b 3.8.1-intel-2020a buildset gettext 0.19.8.1-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / Ghostscript 9.27-GCCcore-8.3.0 9.52-GCCcore-9.3.0 buildset git 2.13.3 Moved to buildtools/2020a / GLib 2.61.2-GCCcore-8.3.0 2.64.1-GCCcore-9.3.0 buildset GMAP-GSNAP 2019-06-10-intel-2019b 2020-03-12-intel-2020a buildset g_mmpbsa 1.6 Toolchain-independent x86_64 gnuplot 5.2.7-intel-2019b 5.4.0-intel-2020a buildset Go 1.10.1 Updated on demand x86_64 GObject-Introspection 1.61.1-GCCcore-8.3.0 1.64.1-GCCcore-9.3.0 buildset gompi 2018b Installed on demand buildset GPAW 19.8.1 (multiple configurations) 20.1.0 (multiple configurations) buildset GROMACS 2019.4-intel-2019b 2020.2-intel-2020a and 2019.4 with PLUMED buildset GSL 2.5-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / gtest 1.8.1 (Intel, GCCcore) 1.10.0-intel-2020a buildset Guile 2.2.3-intel-2018a-ncurses Installed on demand buildset Gurobi 9.0.0 Toolchain-independent x86_64 hanythingondemand 3.2.0-intel-2017a-Python-2.7.13 Installed on demand buildset HarfBuzz 2.5.3-GCCcore-8.3.0 2.6.4-GCCcore-9.3.0 buildset HDF5 1.10.5-intel-2019b-MPI (and 1.8.21) 1.12.0-intel-2020a-MPI (and 1.10.6) buildset HEEDS 2018.10 Installed on demand (SimCenter option). x86_64 help2man 1.47.5 Moved to buildtools/2020a / HISAT2 2.0.5-intel-2017a Installed on demand. buildset HTSeq 0.7.2-intel-2017a-Python-2.7.13 Moved into BioTools-Python/2020a.00-intel-2020a-* / hwloc 1.11.5-GCC-6.3.0-2.27 Installed on demand buildset ImageMagick 7.0.9-7-GCCcore-8.3.0 7.0.10-10-GCCcore-9.3.0 buildset intel 2019b 2020a x86_64, buildset inteldevtools 2019b 2020a x86_64, buildset IntelPython2 2019b Python 2 support has ended x86_64, buildset IntelPython3 2019b (3.6 equivalent) 2020a (3.7 equivalent) x86_64, buildset intltool 0.51.0-intel-2018b-Perl-5.26.1 0.51.0-GCCcore-9.3.0-Perl-5.30.2 buildset IOzone 3.489 System maintenance only system ITensor 2.1.1-intel-2018b On demand, not meant for a central install buildset JAGS 4.3.0-intel-2019b 4.3.0-intel-2020a buildset JasPer 2.0.14-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / Java 11 Toolchain-independent x86_64 Jmol 14.30.1-Java-11 Toolchain-independent x86_64 Julia 0.6.0-intel-2017a-Python-2.7.13 Installed on demand buildset JUnit 4.12-Java-8 Installed on demand system Keras 2.2.2 Keras is now part of TensorFlow / KHRplatform 1.0 Toolchain-independent x86_64 Kraken 1.0-intel-2018b-Perl-5.26.1 Installed on demand (there is now a Kraken 2) buildset Kraken2 / 2.0.9-beta-intel-2020a-Perl-5.30.2 buildset kwant 1.2.2-intel-2017a-Python-3.6.1 1.4.2-intel-2020a-Python-3.8.2 buildset kwant-bundle 1.3.2-intel-2018a-Python-3.6.6 1.4.2-intel-2018a-Python-3.8.2 buildset LAME 3.100-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 buildset LAMMPS 16Mar2018-intel-2017a 17Aug2017-intel-2020a buildset libcerf 1.5-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / libffi 3.2.1-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / libgd 2.2.5-intel-2019b 2.3.0-intel-2020a buildset Libint v2.6.0-intel-2019b-cp2k-lmax-4 Installed in function of other packages buildset libjpeg-turbo 1.5.3-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / libmatheval 1.1.11-intel-2017a Installed on demand buildset libpng 1.6.34-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / libreadline 7.0-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / librosa 0.7.2-intel-2019b-Python-3.7.4 Waiting for compatible version buildset libsndfile 1.0.28-GCCcore-8.3.0 Moved to baselibs/2020a-GCCcore-9.3.0 / LibTIFF 4.0.9-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / libtool 2.4.6 Moved to buildtools/2020a / libunistring 0.9.8-intel-2018a Moved to baselibs/2020a-GCCcore-9.3.0 / libvdwxc 0.4.0-intel-2019b-MPI 0.4.0-intel-2020a-MPI buildset libxc 4.3.4-intel-2019b 4.3.4-intel-2020a buildset libxml2 2.9.8-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / libxslt 1.1.32-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / libxsmm 1.14-intel-2019b 1.15-intel-2020a buildset libyaml 0.2.2-GCCcore-8.3.0 Moved to baselibs/2020a-GCCcore-9.3.0 / LittleCMS 2.9-GCCcore-8.3.0 2.9-GCCcore-9.3.0 buildset LLVM 3.9.1-intel-2017a In function of other packages buildset LMDB 0.9.22-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / Lumen 1.2-intel-2018a Installed on demand buildset M4 1.4.18-GCCcore-7.3.0 Moved to buildtools/2020a / MAFFT 7.407-intel-2018b-with-extensions 7.471-intel-2020a-with-extensions buildset make 4.2.1 Moved to buildtools/2020a / Maple 2020.0 Toolchain-independent x86_64 Mathematica 12.0 Toolchain-independent x86_64 MATLAB R2020a Toolchain-independent x86_64 MCL 14.137-intel-2019b Moved into BioTools/2020a.00-intel-2020a / mdust 1.0-intel-2017a Installed on demand buildset MEGAHIT 1.2.9-intel-2019b Moved into BioTools/2020a.00-intel-2020a / Mesa CentOS7 Dummy module needed for EasyBuild system metapub Was Python-bioinformatics 0.5.50intel-2020a-... buildset METIS 5.1.0-intel-2019b-i32-fp64 5.1.0-intel-2020a-i32-fp64 buildset miniasm / Moved into BioTools/2020a.00-intel-2020a / minimap2 Not installed but requested Moved into BioTools/2020a.00-intel-2020a / molmod 1.1-intel-2017a-Python-2.7.13 1.4.7-intel-2020a-... buildset monitor 1.1.2 1.1.2 x86_64 Mono 4.8.0.495-intel-2017a Installed on demand buildset MonolixSuite 2019R2 Toolchain-independent x86_64 MPFR 4.0.1-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 buildset MrBayes 3.2.7-intel-2019b-MPI 3.2.7-intel-2020a-MPI buildset MUMPS 5.2.1-intel-2019b-noOpenMP-noMPI 5.3.1-intel-2020a-noOpenMP-noMPI buildset MUSCLE 3.8.31-intel-2019b Moved into BioTools/2020a.00-intel-2020a buildset NAMD 2.12-verbs, 2.12-verbs-smp 2.14-verbs, 2.14-verbs-smp depends NASM 2.12.02 Moved to buildtools/2020a / ncbi-vdb 2.8.2-intel-2017a Installed on demand buildset ncurses 6.1-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 buildset netCDF 4.7.0-intel-2019b-(no)MPI 4.7.3-intel-2020a-(no)MPI buildset NEST 2.20.1-intel-2019b-Python-3.7.4-* 2.20.1-intel-2020a-Python-3.8.3-* buildset NetPyNE 0.9.1.3-intel-2018b-Python-3.7.1 0.9.6-intel-2020a-... buildset NEURON 7.7.2-intel-2019b-Python-3.7.4 NEURON/7.7.2-intel-2020a-Python-3.8.2 buildset NGS 2.9.6-intel-2019b 2.10.4-intel-2020a buildset NSS-NSPR 3.45-4.22-GCCcore-8.3.0 3.51.1-4.25-GCCcore-9.3.0 buildset NTPoly 2.4-intel-2019b 2.4-intel-2020a buildset numactl 2.0.11-GCC-6.3.0-2.27 Installed with OpenMPI buildset numba 0.48.0-intel-2019b-Python-3.7.4 0.49.1-intel-2020a-Python-3.8.3 buildset NWChem 6.6.r27746-intel-2017a-Python-2.7.13 7.0.0-intel-2020a-Python-3.8.3 buildset OMNIS-LB 3.1-2 Installed on demand system OpenBLAS 0.3.5-GCC-7.3.0-2.30 Installed with foss buildset OpenCV 3.4.0-intel-2018a-CPU-noGUI Installed on demand buildset OpenFOAM 6-intel-2019b 6-intel-2020a buildset OpenMPI 3.1.1-GCC-7.3.0-2.30 Installed with foss buildset OpenMX 3.8.5-intel-2018b 3.8.5-intel-2020a, 3.9.2-intel-2020a buildset OrthoFinder 2.2.6-intel-2018a Installed on demand buildset OTF2 2.1.1-intel-2018a Installed when needed buildset p7zip 16.02 SYSTEM toolchain, unmaintained system Pango 1.44.3-GCCcore-8.3.0 1.44.7-GCCcore-9.3.0 buildset parallel 20180422 20200422 x86_64 ParaView 5.4.1-intel-2018a-viz-Python-3.6.4 Updated on demand buildset ParMETIS 4.0.3-intel-2019b-i32-fp64 4.0.3-intel-2020a-i32-fp64 buildset patchelf 0.9 Moved into buildtools/2020a / pbs_PRISMS 1.0.1-intel-2017a-Python-2.7.13 Not for SLURM / pbs_python 4.6.0-intel-2017a-Python-2.7.13 Not for SLURM / PCRE 8.42-intel-2018b Moved to baselibs/2020a-GCCcore-9.3.0 / Perl 5.30.0-GCCcore-8.3.0 5.30.2-intel-2020a buildset phonopy 2.1.2-intel-2018b-Python-3.6.8 2.6.1-intel-2020a-... buildset Pilon 1.23-Java-11 SYSTEM toolchain system pixman 0.34.0-intel-2018b Moved into baselibs/2020a-GCCcore-9.3.0 / pkg-config 0.29.1 Moved into buildtools/2020a / PLUMED 2.6.0-intel-2019b 2.6.1-intel-2020a buildset PRANK 170427-intel-2018b Installed on demand buildset PROJ 6.1.1-intel-2019b 7.0.1-intel-2020a buildset protobuf 3.7.1-intel-2018b-Python-3.6.8 Installed when needed by a package buildset Python 2.7.18, 3.7.4 3.8.2-intel-2020a buildset Python-bioinformatics 2019b.00-* / now metapub buildset Python-IDLab-networks 2019b.00-* Problems with gevent. buildset Python-IntelMods 2019b.00-intel-2019b-Python-3.7.4 Obsolete, or try IntelPython3 buildset Qt5 5.13.0-GCCcore-8.3.0-noOpenGL TODO - unsolved compilation failures buildset QuantumESPRESSO 6.4.1-intel-2018b-* TODO - Unsolved compilatin problems buildset QUIP 180612-intel-2017a Installed on demand due to past comp. problems buildset qutip 4.3.1-intel-2018b-Python-3.6.8 QuTiP/4.5.1-intel-2020a-... buildset R 3.5.1-intel-2018b 4.0.2-intel-2020a buildset Racon 1.3.3-intel-2018b Moved into BioTools/2020a.00-intel-2020a buildset RAxML 8.2.10-intel-2017a-hybrid Installed on demand buildset RAxML-NG 0.9.0-intel-2019b 0.9.0-intel-2020a buildset Roary 3.12.0-intel-2018b-Perl-5.26.1 Incompatible with other software in the toolchain buildset Ruby 2.4.0 SYSTEM toolchain, updated on demand system SAMtools 1.9-intel-2019b Moved into BioTools/2020a.00-intel-2020a buildset ScaLAPACK 2.0.2-gompi-7.3.0-OpenBLAS-0.2.20 Installed with foss buildset Scalasca 2.4-intel-2018a Installed on demand buildset scikit-umfpack 0.3.2-intel-2019b-Python-3.7.4 0.3.2-intel-2020a-Python-3.8.2 buildset SCons 3.0.0 SYSTEM toolchain system SCOTCH 6.0.7-intel-2019b 6.0.7-intel-2020a buildset SICER2 Installed in user directory Moved into BioTools-Python buildset Siesta 4.0-intel-2017a 4.0.2-intel-2020a, 4.1-b4-intel-2020a buildset Simcenter 12.02 Toolchain-independent x86_64 SIONlib 1.7.2-intel-2018a-tools Installed when needed by a package buildset SMALT 0.7.6-intel-2019b 0.7.6-intel-2020a buildset SPAdes 3.14.0-intel-2019b-Python-3.7.4 3.14.1-intel-2020a-Python-3.8.2 buildset SQLite 3.29.0-intel-2019b 3.31.1-intel-2020a buildset STAR-CCM+ 2019.3.1 Toolchain-independent x86_64 Stata 15MP Toolchain-independent x86_64 SuiteSparse 5.4.0-intel-2019b-METIS-5.1.0 5.7.1-intel-2020a-METIS-5.1.0 buildset SWIG 4.0.0-* 4.0.1-* buildset Szip 2.1.1-intel-2018b Moved into baselibs/2020a-GCCcore-9.3.0 / Tcl 8.6.9-intel-2019b 8.6.10-intel-2020a buildset TELEMAC v8p1r1-intel-2019b v8p1r1-intel-2020a buildset TensorFlow 2.1.0-intel-2019b-* 2.2.0-intel-2020a-* buildset TensorRT 6.0.1.5-CUDA-10.1.243-cuDNN-7.6.5.32 Toolchain-independent system Tk 8.6.9-intel-2019b 8.6.10-intel-2020a buildset torque-tools 1.0.2 Obsolete on SLURM / Towhee / 8.2.0 buildset Trimmomatic 0.39-Java-11 Toolchain-independent x86_64 UDUNITS 2.2.26-intel-2018b Moved into baselibs/2020a-GCCcore-9.3.0 / Unicycler 0.4.7-intel-2018b-Python-3.7.1 0.4.8-intel-2020a-Python-3.8.3 buildset USPEX 10.3-intel-2018a TODO - Needs VASP first buildset util-linux 2.32-intel-2018b Moved into baselibs/2020a-GCCcore-9.3.0 / VASP 5.4.4-intel-2018b-Wannier90-2.1.0 TODO - Awaiting a license for VASP 6 buildset VESTA 3.4.6 Installed on demand x86_64 VMD 1.9.3-intel-2019b-Python-2.7.16-viz 1.9.3-intel-2020a-viz buildset Voro++ 0.4.6-intel-2019b 0.4.6-intel-2020a buildset vsc-base 2.5.1 Obsolete / vsc-install 0.12.7-intel-2019b-Python-3.7.4 Obsolete / vsc-mympirun 4.1.9-intel-2019b-Python-2.7.16 5.0.1 (SYSTEM toolchain) system vsc-tutorial 202003-intel-2019b Installed when the courses are prepared. buildset vsc-vnc 0.1 SYSTEM toolchain system VSEARCH 2.14.2-intel-2019b Moved into BioTools/2020a.00-intel-2020a / Wannier90 2.1.0-intel-2018b 3.1.0-intel-2020a buildset WannierTools / 2.5.1-intel-2020a buildset worker 1.6.8-intel-2019b 1.6.12-intel-2020a buildset X11 2019b-GCCcore-8.3.0 2020a-GCCcore-9.3.0 buildset x264 20180212-intel-2018b Moved into baselibs/2020a-GCCcore-9.3.0 / x265 2.8-intel-2018b Moved into baselibs/2020a-GCCcore-9.3.0 / XML-LibXML 2.0132-intel-2018b-Perl-5.26.1 Integrated into BioPerl / xprop 1.2.4-GCCcore-8.3.0 Integrated in X11/2020a-GCCcore-9.3.0 / Yambo / 4.4.1-intel-2020a buildset XZ 5.2.4-intel-2018b Moved into baselibs/2020a-GCCcore-9.3.0 / ZDPlasKin 2.0a TODO buildset zlib 1.2.11-intel-2018b Moved into baselibs/2020a-GCCcore-9.3.0 /","title":"Packages 2020a"},{"location":"generated/easyconfigs/a/ABINIT/","text":"ABINIT instructions ABINIT web site Available versions ABINIT development on GitHub GitHub releases , not always in sync with the main web site. General information The project lead is Xavier Gonze, LLN. Configurations used to compile on some of the Walloon clusters can be found on the abinit/abiconfig GitHub Note that very few options are installed which shows that it may be tricky\u2026 We compile with -mcmodel=large. However, this is incompatible with the -static-intel link option imposed by the configure script. Therefore we need to patch the configure script. Done because one of our users had problems. We also enforce 64-bit integers which were needed by one of our users. (--enable-64bit-flags and -i8 in the compiler options) This had caused trouble with BigDFT in the past, but it is not the only problem with BigDFT. ABINIT is rather tricky to install. There seem to be numerous conflicts between its dependencies, depending on how things are being compiled. Configure tests that fail with our Python modules, \u2026 It looks like it is one of those packages that one would like to install in a single directory tree with all dependencies tuned to the specific needs of the package, and even that may not always work. EasyBuild There is support for ABINIT in the EasyBuild repository . There is no application-specific EasyBlock. ABINIT in EasyBuild is compiled through a ConfigureMake EasyConfig, with a length xonfigopts to adapt the configuration. We don't really follow that approach. Instead, we let ABINIT compile its own dependencies as much as possible after past compatibility problems when getting the dependencies from other modules. A user mentioned problems with 8.10.2 when compiled with newer compilers than Intel 2016 so be careful; other builds may be faulty! ABINIT 8.10.3 for the 2020a toolchains Compiled with Intel MKL for BLAS and LAPACK. We also use the Intel FFTW interfaces. Compiled with the netCDF fallback code from ABINIT after having ran into compatibility problems before. We don't use ETSF_IO and BigDFT. After problems getting ABINIT 8.10.x to work with GSL, we do no longer include GSL. Levmar hasn't been developed since 2011 so we also skip this as it seems very few users need that functionality. After problems with the configure process in combination with our own Python modules, we no longer include Python as a dependency and instead rely on the system Python. We have not yet tried to compile ABINIT with the ELPA library, though it may be useful according to documentation that we once found on the ABINIT Wiki (but has since moved or has been removed).","title":"ABINIT instructions"},{"location":"generated/easyconfigs/a/ABINIT/#abinit-instructions","text":"ABINIT web site Available versions ABINIT development on GitHub GitHub releases , not always in sync with the main web site.","title":"ABINIT instructions"},{"location":"generated/easyconfigs/a/ABINIT/#general-information","text":"The project lead is Xavier Gonze, LLN. Configurations used to compile on some of the Walloon clusters can be found on the abinit/abiconfig GitHub Note that very few options are installed which shows that it may be tricky\u2026 We compile with -mcmodel=large. However, this is incompatible with the -static-intel link option imposed by the configure script. Therefore we need to patch the configure script. Done because one of our users had problems. We also enforce 64-bit integers which were needed by one of our users. (--enable-64bit-flags and -i8 in the compiler options) This had caused trouble with BigDFT in the past, but it is not the only problem with BigDFT. ABINIT is rather tricky to install. There seem to be numerous conflicts between its dependencies, depending on how things are being compiled. Configure tests that fail with our Python modules, \u2026 It looks like it is one of those packages that one would like to install in a single directory tree with all dependencies tuned to the specific needs of the package, and even that may not always work.","title":"General information"},{"location":"generated/easyconfigs/a/ABINIT/#easybuild","text":"There is support for ABINIT in the EasyBuild repository . There is no application-specific EasyBlock. ABINIT in EasyBuild is compiled through a ConfigureMake EasyConfig, with a length xonfigopts to adapt the configuration. We don't really follow that approach. Instead, we let ABINIT compile its own dependencies as much as possible after past compatibility problems when getting the dependencies from other modules. A user mentioned problems with 8.10.2 when compiled with newer compilers than Intel 2016 so be careful; other builds may be faulty!","title":"EasyBuild"},{"location":"generated/easyconfigs/a/ABINIT/#abinit-8103-for-the-2020a-toolchains","text":"Compiled with Intel MKL for BLAS and LAPACK. We also use the Intel FFTW interfaces. Compiled with the netCDF fallback code from ABINIT after having ran into compatibility problems before. We don't use ETSF_IO and BigDFT. After problems getting ABINIT 8.10.x to work with GSL, we do no longer include GSL. Levmar hasn't been developed since 2011 so we also skip this as it seems very few users need that functionality. After problems with the configure process in combination with our own Python modules, we no longer include Python as a dependency and instead rely on the system Python. We have not yet tried to compile ABINIT with the ELPA library, though it may be useful according to documentation that we once found on the ABINIT Wiki (but has since moved or has been removed).","title":"ABINIT 8.10.3 for the 2020a toolchains"},{"location":"generated/easyconfigs/a/ANTs/","text":"ANTs installation instructions ANTs web site ANTs on GitHub which also includes the installation instructions . General information ANTs needs ITK (or web site itk.org ) or VTK . If it can't find one of those libraries, it will download and install one. By default ANTs will use ITK. ANTs may be very picky about the versions of ITK or VTK. See towards the bottom of the installation instructions for instructions on how to determine the right version number of ITK or VTK. There are separate packages to interface ANTs with R: ANTsR Python: ANTsPy ANTs does include a number of bash, R and Perl scripts. The bash-scripts make heavy use of the ANTSPATH variable and don't search for the ANTs executables in the standard PATH. ToDo: Document the most important CMake flags as that cannot easily be found in the documentation. EasyConfigs There is support for ANTs in EasyBuild. The EasyBuilders recipes tend to use Python and a VTK installed through EasyBuild. It is not clear why Python is included as a dependency as there is no Python code in ANTs. There is however an extension that needs to be installed separately that provides a Python interface to ANTs. This documentation was started with the 2020a build sets, ANTs 2.3.3. Note that the EasyBuilders recipes often contain patches to fix bugs in ANTs. Many of these patches seem to be derived from bug in the ANTs GitHub repository and hence are not needed anymore in the next version or patchlevel of ANTs. ANTs 2.3.3 for the 2020a toolchains We let the code download the right version of ITK. This does imply that not all files needed to install the code are stored in the EasyBuild sources subdirectories! As we can't see where Python is being used, we don't use the Python dependency which is in the EasyBuilders recipes. Including baselibs as a dependency doesn't seem to make sense. ITK keeps using its own internal versions of certain libraries if we make no changes to the CMake flags.","title":"ANTs installation instructions"},{"location":"generated/easyconfigs/a/ANTs/#ants-installation-instructions","text":"ANTs web site ANTs on GitHub which also includes the installation instructions .","title":"ANTs installation instructions"},{"location":"generated/easyconfigs/a/ANTs/#general-information","text":"ANTs needs ITK (or web site itk.org ) or VTK . If it can't find one of those libraries, it will download and install one. By default ANTs will use ITK. ANTs may be very picky about the versions of ITK or VTK. See towards the bottom of the installation instructions for instructions on how to determine the right version number of ITK or VTK. There are separate packages to interface ANTs with R: ANTsR Python: ANTsPy ANTs does include a number of bash, R and Perl scripts. The bash-scripts make heavy use of the ANTSPATH variable and don't search for the ANTs executables in the standard PATH. ToDo: Document the most important CMake flags as that cannot easily be found in the documentation.","title":"General information"},{"location":"generated/easyconfigs/a/ANTs/#easyconfigs","text":"There is support for ANTs in EasyBuild. The EasyBuilders recipes tend to use Python and a VTK installed through EasyBuild. It is not clear why Python is included as a dependency as there is no Python code in ANTs. There is however an extension that needs to be installed separately that provides a Python interface to ANTs. This documentation was started with the 2020a build sets, ANTs 2.3.3. Note that the EasyBuilders recipes often contain patches to fix bugs in ANTs. Many of these patches seem to be derived from bug in the ANTs GitHub repository and hence are not needed anymore in the next version or patchlevel of ANTs.","title":"EasyConfigs"},{"location":"generated/easyconfigs/a/ANTs/#ants-233-for-the-2020a-toolchains","text":"We let the code download the right version of ITK. This does imply that not all files needed to install the code are stored in the EasyBuild sources subdirectories! As we can't see where Python is being used, we don't use the Python dependency which is in the EasyBuilders recipes. Including baselibs as a dependency doesn't seem to make sense. ITK keeps using its own internal versions of certain libraries if we make no changes to the CMake flags.","title":"ANTs 2.3.3 for the 2020a toolchains"},{"location":"generated/easyconfigs/a/ASE/","text":"ASE installation notes ASE stands for Atomic Simulation Environment. The Atomic Simulation Environment (ASE) is a set of tools and Python modules for setting up, manipulating, running, visualizing and analyzing atomistic simulations. The code is freely available under the GNU LGPL license. ASE web site ASE on PyPI Dependencies ASE needs a number of other Python packages: * NumPy - Included in the standard UAntwerp Python bundles * SciPy (See also SciPy.org ) - Included in the standard UAntwerp Python bundles * matplotlib , at least version 2.0.0 (status for ASE 3.18.1) - Included in the standard UAntwerp Python * tkinter, the TK interface, is a part of the Standard Python Library on recent versions * Flask needs to be installed. It brings with it a set of other dependencies: * Werkzeug * Jinja2 , and this one needs * MarkupSafe * itsdangerous * click - Included in recent standard Python packages at UAntwerp, but not in some of the older ones. Some of these packages have other dependencies that are included in the standard UAntwerp Python bundles. EasyConfig Nothing special here. We've chosen to install ASE together with all missing dependencies through a \"PythonBundle\" EasyBlock (from 2020a on) of a \"Bundle\" EasyBlock. The latter requires setting PYTHONPATH by hand. Checking the build result of Python packages Search in the EasyBuild log file for python setup or pip install (depending on the installation method) to see if the compilation did not produce errors. Some packages have Python fallback code if the compilation fails, so the standard EasyBuild sanity check will not detect these problems, but the result will be a much slower Python package.","title":"ASE installation notes"},{"location":"generated/easyconfigs/a/ASE/#ase-installation-notes","text":"ASE stands for Atomic Simulation Environment. The Atomic Simulation Environment (ASE) is a set of tools and Python modules for setting up, manipulating, running, visualizing and analyzing atomistic simulations. The code is freely available under the GNU LGPL license. ASE web site ASE on PyPI","title":"ASE installation notes"},{"location":"generated/easyconfigs/a/ASE/#dependencies","text":"ASE needs a number of other Python packages: * NumPy - Included in the standard UAntwerp Python bundles * SciPy (See also SciPy.org ) - Included in the standard UAntwerp Python bundles * matplotlib , at least version 2.0.0 (status for ASE 3.18.1) - Included in the standard UAntwerp Python * tkinter, the TK interface, is a part of the Standard Python Library on recent versions * Flask needs to be installed. It brings with it a set of other dependencies: * Werkzeug * Jinja2 , and this one needs * MarkupSafe * itsdangerous * click - Included in recent standard Python packages at UAntwerp, but not in some of the older ones. Some of these packages have other dependencies that are included in the standard UAntwerp Python bundles.","title":"Dependencies"},{"location":"generated/easyconfigs/a/ASE/#easyconfig","text":"Nothing special here. We've chosen to install ASE together with all missing dependencies through a \"PythonBundle\" EasyBlock (from 2020a on) of a \"Bundle\" EasyBlock. The latter requires setting PYTHONPATH by hand.","title":"EasyConfig"},{"location":"generated/easyconfigs/a/ASE/#checking-the-build-result-of-python-packages","text":"Search in the EasyBuild log file for python setup or pip install (depending on the installation method) to see if the compilation did not produce errors. Some packages have Python fallback code if the compilation fails, so the standard EasyBuild sanity check will not detect these problems, but the result will be a much slower Python package.","title":"Checking the build result of Python packages"},{"location":"generated/easyconfigs/a/Amber/","text":"Notes for installing Amber Amber web site General information Amber consists of two parts that should be installed together. There is the actual Amber package itself, and AmberTools, with a number of additional tools that can also be used without Amber. AmberTools seems to be updated more frequently than Amber. The software has an EasyBlock that is capable of installing both. Yet even then the software needs a significant amount of patching to make the installation process compatible with the EasyBuild conventions. There are some bad decisions in the configure scripts of Amber, including explicitly looking for X11 libraries in specific directories. Since Amber is semi-commercial software, an account is needed to download the software. Hence Amber and a matching version of AmberTools should be downloaded beforehand to the sources directory as downloading them from EasyBuild will fail, even with the correct URL. The patches for Amber and AmberTools can be downloaded freely though. Amber installation process The Amber documentation process is very badly documented. Hence this text with a number of problems we ran into trying to install Amber on the UAntwerp systems. Basically one needs to investigate installation code and the output of the configuration and build processes to figure out all the options and dependencies of the code. Choice of build processes In Amber 18, there are two build processes that are kind of supported. However, both appear to be very buggy. * A configure -based one that is considered to be the stable build process. Though very similar to traditional autotools-generated configure scripts, it looks like at least part if not all of the configure scripts that are being used are actually hand-written or modified from generated ones. There are definitely hidden parameters that are not shown with --help or --full-help . * A CMake-based process that in version 18 is still considered experimental. It is not guaranteed to work in all environments or for all types of builds, and I indeed could not get it to work. Note that some people have reported that using a parallel make process may fail... CPU build types Not counting the CUDA GPU builds, different types of builds are supported. However, it seems that only the executables that are explicitly supported for that build type are being build and rather than do a fallback to a 'lower' level for the other executables. * Serial build. This seems to build the largest set of executables. * MPI build: Using either -mpi or -itelmpi with configure * OpenMP build: Using -openmp with configure * Hybrid MPI/OpenMP build: Using -openmp in combination with either -mpi or -intelmpi . Adding the Amber and AmberTools patches A new version of Amber is released roughly every two years, a new version of AmberTools roughly every year. They come as two .tar.bz2 files. AmberTools can be installed without Amber, but Amber cannot be installed without AmberTools. However, during their life span, both receive numerous patches while the tar archives are not updated. These can be found on: * AmberTools patches page * Amber patches page Amber does come with a tool that will automatically download and apply the patches. update_amber . With this tool, it is possible to specify up to which patchlevel both Amber and AmberTools should be upgraded to get reproducible installations. Moreover, it is possible to download the patches by hand, put them in a specific directory, and let update_amber then apply the patches without re-downloading them again. That may save some time, but using this mechanism we can also save the patches together with the sources files for later re-installations so that Amber can be reinstalled in a reproducible way also should the patches no longer be available. Patches are stored in: * .patches/Amber<version>_Unapplied_Patches for Amber (replace <version> with the version number of Amber) * .patches/AmberTools<version>_Unapplied_Patches for AmberTools (replace <version> with the version number of AmberTools) Running update_amber with the specification of the right patch level will not re-download the patches but only apply them. E.g., ./update_amber --update-to AmberTools/9,Amber/17 Amber and AmberTools dependencies Discovering the true dependencies requires a lot of engineering as there is no complete list in the manual. It really requires looking into all output of configure and/or cmake and sometimes even figuring it out from the output of the build process and from the source code. libz bzip2, though the CMake build process fails to recognize the library even though it is provided through a module. It is best to use an optimised BLAS and Lapack implementation. MKL is supported. Arpack, but the code comes with an internal version (in AmberTools) FFTW, but the code comes with an internal version and fails to recognize an external FFTW3 library. The configure script in Amber 18 doesn't even try to find one. netCDF, preferably with PnetCDF as the parallel backend for the old netCDF data formats. The PnetCDF support is used by cpptraj . The code contains an internal version; however, it is not clear if and when it tries to compile the internal PnetCDF. (TODO: Check further, maybe only included automatically in MPI builds which is actually very reasonable?) Boost, though the code comes with an internal version Perl 5, as it does install a number of Perl scripts (optional?) Python, as some of the tools come as Python scripts. It is not clear which other packages are needed... The manual mentions that either Python 2.7 or Python 3.4 or later should be used. It is also not fully clear which other Python packages are needed, though the manual does mention: NumPy SciPy matplotlib cython IPython and notebook? Not useful for job scripts, these are tools for interactive work. Other problems I ran into with Amber 18 and AmberTools 19 The cmake build process clearly does not yet work properly when trying to do a MPI/OpenMP build. Moreover, it fails to find libraries that are clearly present, e.g., libbz2. The configure script is considered to be mature even though there are features that are marked as experimental, in particular --prefix . Note that without --prefix , one needs to do an in-place build and then clean up (likely by hand) files that are not needed for running Amber. In fact, all EasyConfigs I found (see further down) for versions of Amber available in December 2019 used in-place builds. In some configurations we got crashes during the build process that may be due to bugs in the Makefiles in combination with a configure with --prefix . Usual practice with cmake or configure generated makefiles using libtool is that VERBOSE=1 or V=1 show the full command being executed. This does not work with the Amber makefiles. To discover what is really happening, set the variable SHELL in calls of make to sh -x , hence add SHELL='sh-s' to the make command line. When selecting Intel as the compiler, the configure process does seem to know how to set suitable compiler options without using environment variables such as CXX_FLAGS. Some files are compiled with -O0 but this may be deliberate since they are known to sometimes fail when optimization is turned on. However, though most modules are compiled with very reasonable options, in some modules only -O or -O3 is used without -xHost. This implies that these modules are only optimized for whatever Intel considers the default processor for the version of the compiler, and this in turn implies that AVX or younger vector instructions typically are not used. Which means that the performance potential of every recent processor (where recent really is less than 6 years old) remains unexploited. Likely suboptimal compile options were found in: * UCPP: AmberTools/src/ucpp-1.3: No optimization options given * CIFPARSE: AmberTools/src/cifparse: No optimization options given * ANTECHAMBER: AmberTools/src/antechamber: No optimization options given * REDUCE, * AmberTools/src/reduce/toolclasses: Uses only -O , without -xHost . * AmberTools/src/reduce/libpdb: idem * AmberTools/src/reduce/reduce_src: Uses -O3 but no -xHost * LEAP: AmberTools/src/leap/src/leap: No optimization options given * EMIL: AmberTools/src/emil: Uses -O3 but no -xHost * NMRAUX: * AmberTools/src/nmr_aux/prepare_input: No optimization options * CPPTRAJ: AmberTools/src/cpptraj/src and subdirectories: Uses -O3 but no -xHost * AMBPDB: AmberTools/src/ambpdb: Uses -O3 but no -xHost * NAB: AmberTools/src/nab: No optimization options * ETC: AmberTools/src/etc: No optimization options * RISM: AmberTools/src/rism: The build process returns repeatedly to this directory, and somtimes files get compiled with very reasonable optimization options while sometimes these options appear to be completely missing. * SAXS: AmberTools/src/saxs: Uses -O3 but no -xHost * CPHSTATS: AmberTools/src/cphstats: No optimization options * NFE: AmberTools/src/nfe-umbrella-slice: No optimization options Programs that are compiled during the configure phase in a not so optimal way: * Boost is compiled with the GNU C++ compiler instead of the Intel one. -O3 is used but without -march=native . Though that probably does not matter to much for most of what is in Boost. Building Amber with EasyBuild The first version covered by this documentation is the installation of Amber 18 with AmberTools 19 in the 2019b toolchains. There is a EasyBlock for Amber that makes sure that updates are applied, etc. It does seem to require a remarkable number of patches mostly to the configure scripts of Amber to work properly. Places where other EasyConfigs for Amber can be found: EasyBuilder EasyConfig repository on github , with the matching EasyBlock . These EasyConfigs require a lot of patching of the Amber and AmberTools sources. One problem with this is that even just a patch to Amber or AmberTools may invalidate one or more of the EasyBuild-provided patches, making upgrading to a newer version a real pain in the ass. ComputeCanada github repository . ComputeCanada starts from the default EasyBuilder EasyConfigs. IT4I github repository . IT4I does use the EasyBlock for Amber, but doesn't seem to need the pletora of patches used in the EasyBuilder EasyConfigs. CSCS github repository . In their typical way, they avoid using the application-specific EasyBlock and instead push all steps in a traditional MakeCp-based EasyConfig. In their newer versions they build several build types together in one module, which may be a good thing as otherwise some modules would be missing some commands (e.g., the ones that don't support MPI, OpenMP or hybrid mode). Amber 18 / AmberTools 19 in the 2019b and 2020a toolchains For the development of the EasyConfig used at UAntwerp, we started from the CSCS one but extended it in several ways. As --prefix still works unreliably in Amber18, we do an in-place build and remove the sources afterwards. As with the regular EasyConfig for Amber, one should set the number of patches for AmberTools and Amber at the start of the EasyConfig file, see the variable patchlevel. To determine the correct number, go to AmberTools patches page Amber patches page We specify the extract command for the tar files so that the amberXX/-prefix is already removed at the untar phase. Furthermore we extend the source list with all patches for Amber and AmberTools, and make sure they are downloaded to the appropriate subdirectories AmberXX and AmberToolsYY of the Sources subdirectory to avoid name conflicts. We use extract_cmd to move these to the correct subdirectories of the unpacked sources. As we cannot adapt the installation of the patches without writing or adapting the EasyBlock for Amber, we install the patches with update_amber during the configure phase. Just as in the CSCS EasyConfig, we do 4 builds of Amber in the same directories, in the order serial, OpenMP, hybrid MPI+OpenMP and MPI. Note that the serial build is needed to ensure that the full set of tools is installed, as the other options only seem to install those tools that support either OpenMP or MPI. However, by first doing the serial build, we also ensure that OpenMP versions of the tools will overwrite the serial ones if they exist. We use postinstallcmds to clean up the sources. Some tricks in the EasyConfig to work around problems in Amber: * No host-specific optimizations in CPPTRAJ: The configure script of CPPTRAJ is called without the -tune from AmberTools/src/configure2.* option and therefore does only enable optimizations but no host-specific optimizations. There is a workaround by specifying the compiler option for the host-specific optimizations via the environment variable TUNEFLAGS . NOTE: Amber doesn't always build properly on BeeGFS. It sometimes fails with message such as \"resource busy\". It looks like some locks are not released quickly enough or so. Amber 20 / AmberTools 20 in the 2020a toolchains A straightforward port. We no longer use TUNEFLAGS as the option gets overwritten anyway as these flags are already added via the CFLAGS etc. passed by EasyBuild (and causes warnings on Rome that they get overwritten). In case of problems.. It may be necessary to reset CFLAGS etc when calling configure in the EasyConfig and instead make proper use of TUNEFLAGS (see the 2019b toolchain) to set the optimization options (and be careful with AMD Rome). Some files are now compiled with -O0 which gets overwritten by a -O2 included in CFLAGS etc.","title":"Notes for installing Amber"},{"location":"generated/easyconfigs/a/Amber/#notes-for-installing-amber","text":"Amber web site","title":"Notes for installing Amber"},{"location":"generated/easyconfigs/a/Amber/#general-information","text":"Amber consists of two parts that should be installed together. There is the actual Amber package itself, and AmberTools, with a number of additional tools that can also be used without Amber. AmberTools seems to be updated more frequently than Amber. The software has an EasyBlock that is capable of installing both. Yet even then the software needs a significant amount of patching to make the installation process compatible with the EasyBuild conventions. There are some bad decisions in the configure scripts of Amber, including explicitly looking for X11 libraries in specific directories. Since Amber is semi-commercial software, an account is needed to download the software. Hence Amber and a matching version of AmberTools should be downloaded beforehand to the sources directory as downloading them from EasyBuild will fail, even with the correct URL. The patches for Amber and AmberTools can be downloaded freely though.","title":"General information"},{"location":"generated/easyconfigs/a/Amber/#amber-installation-process","text":"The Amber documentation process is very badly documented. Hence this text with a number of problems we ran into trying to install Amber on the UAntwerp systems. Basically one needs to investigate installation code and the output of the configuration and build processes to figure out all the options and dependencies of the code.","title":"Amber installation process"},{"location":"generated/easyconfigs/a/Amber/#choice-of-build-processes","text":"In Amber 18, there are two build processes that are kind of supported. However, both appear to be very buggy. * A configure -based one that is considered to be the stable build process. Though very similar to traditional autotools-generated configure scripts, it looks like at least part if not all of the configure scripts that are being used are actually hand-written or modified from generated ones. There are definitely hidden parameters that are not shown with --help or --full-help . * A CMake-based process that in version 18 is still considered experimental. It is not guaranteed to work in all environments or for all types of builds, and I indeed could not get it to work. Note that some people have reported that using a parallel make process may fail...","title":"Choice of build processes"},{"location":"generated/easyconfigs/a/Amber/#cpu-build-types","text":"Not counting the CUDA GPU builds, different types of builds are supported. However, it seems that only the executables that are explicitly supported for that build type are being build and rather than do a fallback to a 'lower' level for the other executables. * Serial build. This seems to build the largest set of executables. * MPI build: Using either -mpi or -itelmpi with configure * OpenMP build: Using -openmp with configure * Hybrid MPI/OpenMP build: Using -openmp in combination with either -mpi or -intelmpi .","title":"CPU build types"},{"location":"generated/easyconfigs/a/Amber/#adding-the-amber-and-ambertools-patches","text":"A new version of Amber is released roughly every two years, a new version of AmberTools roughly every year. They come as two .tar.bz2 files. AmberTools can be installed without Amber, but Amber cannot be installed without AmberTools. However, during their life span, both receive numerous patches while the tar archives are not updated. These can be found on: * AmberTools patches page * Amber patches page Amber does come with a tool that will automatically download and apply the patches. update_amber . With this tool, it is possible to specify up to which patchlevel both Amber and AmberTools should be upgraded to get reproducible installations. Moreover, it is possible to download the patches by hand, put them in a specific directory, and let update_amber then apply the patches without re-downloading them again. That may save some time, but using this mechanism we can also save the patches together with the sources files for later re-installations so that Amber can be reinstalled in a reproducible way also should the patches no longer be available. Patches are stored in: * .patches/Amber<version>_Unapplied_Patches for Amber (replace <version> with the version number of Amber) * .patches/AmberTools<version>_Unapplied_Patches for AmberTools (replace <version> with the version number of AmberTools) Running update_amber with the specification of the right patch level will not re-download the patches but only apply them. E.g., ./update_amber --update-to AmberTools/9,Amber/17","title":"Adding the Amber and AmberTools patches"},{"location":"generated/easyconfigs/a/Amber/#amber-and-ambertools-dependencies","text":"Discovering the true dependencies requires a lot of engineering as there is no complete list in the manual. It really requires looking into all output of configure and/or cmake and sometimes even figuring it out from the output of the build process and from the source code. libz bzip2, though the CMake build process fails to recognize the library even though it is provided through a module. It is best to use an optimised BLAS and Lapack implementation. MKL is supported. Arpack, but the code comes with an internal version (in AmberTools) FFTW, but the code comes with an internal version and fails to recognize an external FFTW3 library. The configure script in Amber 18 doesn't even try to find one. netCDF, preferably with PnetCDF as the parallel backend for the old netCDF data formats. The PnetCDF support is used by cpptraj . The code contains an internal version; however, it is not clear if and when it tries to compile the internal PnetCDF. (TODO: Check further, maybe only included automatically in MPI builds which is actually very reasonable?) Boost, though the code comes with an internal version Perl 5, as it does install a number of Perl scripts (optional?) Python, as some of the tools come as Python scripts. It is not clear which other packages are needed... The manual mentions that either Python 2.7 or Python 3.4 or later should be used. It is also not fully clear which other Python packages are needed, though the manual does mention: NumPy SciPy matplotlib cython IPython and notebook? Not useful for job scripts, these are tools for interactive work.","title":"Amber and AmberTools dependencies"},{"location":"generated/easyconfigs/a/Amber/#other-problems-i-ran-into-with-amber-18-and-ambertools-19","text":"The cmake build process clearly does not yet work properly when trying to do a MPI/OpenMP build. Moreover, it fails to find libraries that are clearly present, e.g., libbz2. The configure script is considered to be mature even though there are features that are marked as experimental, in particular --prefix . Note that without --prefix , one needs to do an in-place build and then clean up (likely by hand) files that are not needed for running Amber. In fact, all EasyConfigs I found (see further down) for versions of Amber available in December 2019 used in-place builds. In some configurations we got crashes during the build process that may be due to bugs in the Makefiles in combination with a configure with --prefix . Usual practice with cmake or configure generated makefiles using libtool is that VERBOSE=1 or V=1 show the full command being executed. This does not work with the Amber makefiles. To discover what is really happening, set the variable SHELL in calls of make to sh -x , hence add SHELL='sh-s' to the make command line. When selecting Intel as the compiler, the configure process does seem to know how to set suitable compiler options without using environment variables such as CXX_FLAGS. Some files are compiled with -O0 but this may be deliberate since they are known to sometimes fail when optimization is turned on. However, though most modules are compiled with very reasonable options, in some modules only -O or -O3 is used without -xHost. This implies that these modules are only optimized for whatever Intel considers the default processor for the version of the compiler, and this in turn implies that AVX or younger vector instructions typically are not used. Which means that the performance potential of every recent processor (where recent really is less than 6 years old) remains unexploited. Likely suboptimal compile options were found in: * UCPP: AmberTools/src/ucpp-1.3: No optimization options given * CIFPARSE: AmberTools/src/cifparse: No optimization options given * ANTECHAMBER: AmberTools/src/antechamber: No optimization options given * REDUCE, * AmberTools/src/reduce/toolclasses: Uses only -O , without -xHost . * AmberTools/src/reduce/libpdb: idem * AmberTools/src/reduce/reduce_src: Uses -O3 but no -xHost * LEAP: AmberTools/src/leap/src/leap: No optimization options given * EMIL: AmberTools/src/emil: Uses -O3 but no -xHost * NMRAUX: * AmberTools/src/nmr_aux/prepare_input: No optimization options * CPPTRAJ: AmberTools/src/cpptraj/src and subdirectories: Uses -O3 but no -xHost * AMBPDB: AmberTools/src/ambpdb: Uses -O3 but no -xHost * NAB: AmberTools/src/nab: No optimization options * ETC: AmberTools/src/etc: No optimization options * RISM: AmberTools/src/rism: The build process returns repeatedly to this directory, and somtimes files get compiled with very reasonable optimization options while sometimes these options appear to be completely missing. * SAXS: AmberTools/src/saxs: Uses -O3 but no -xHost * CPHSTATS: AmberTools/src/cphstats: No optimization options * NFE: AmberTools/src/nfe-umbrella-slice: No optimization options Programs that are compiled during the configure phase in a not so optimal way: * Boost is compiled with the GNU C++ compiler instead of the Intel one. -O3 is used but without -march=native . Though that probably does not matter to much for most of what is in Boost.","title":"Other problems I ran into with Amber 18 and AmberTools 19"},{"location":"generated/easyconfigs/a/Amber/#building-amber-with-easybuild","text":"The first version covered by this documentation is the installation of Amber 18 with AmberTools 19 in the 2019b toolchains. There is a EasyBlock for Amber that makes sure that updates are applied, etc. It does seem to require a remarkable number of patches mostly to the configure scripts of Amber to work properly. Places where other EasyConfigs for Amber can be found: EasyBuilder EasyConfig repository on github , with the matching EasyBlock . These EasyConfigs require a lot of patching of the Amber and AmberTools sources. One problem with this is that even just a patch to Amber or AmberTools may invalidate one or more of the EasyBuild-provided patches, making upgrading to a newer version a real pain in the ass. ComputeCanada github repository . ComputeCanada starts from the default EasyBuilder EasyConfigs. IT4I github repository . IT4I does use the EasyBlock for Amber, but doesn't seem to need the pletora of patches used in the EasyBuilder EasyConfigs. CSCS github repository . In their typical way, they avoid using the application-specific EasyBlock and instead push all steps in a traditional MakeCp-based EasyConfig. In their newer versions they build several build types together in one module, which may be a good thing as otherwise some modules would be missing some commands (e.g., the ones that don't support MPI, OpenMP or hybrid mode).","title":"Building Amber with EasyBuild"},{"location":"generated/easyconfigs/a/Amber/#amber-18-ambertools-19-in-the-2019b-and-2020a-toolchains","text":"For the development of the EasyConfig used at UAntwerp, we started from the CSCS one but extended it in several ways. As --prefix still works unreliably in Amber18, we do an in-place build and remove the sources afterwards. As with the regular EasyConfig for Amber, one should set the number of patches for AmberTools and Amber at the start of the EasyConfig file, see the variable patchlevel. To determine the correct number, go to AmberTools patches page Amber patches page We specify the extract command for the tar files so that the amberXX/-prefix is already removed at the untar phase. Furthermore we extend the source list with all patches for Amber and AmberTools, and make sure they are downloaded to the appropriate subdirectories AmberXX and AmberToolsYY of the Sources subdirectory to avoid name conflicts. We use extract_cmd to move these to the correct subdirectories of the unpacked sources. As we cannot adapt the installation of the patches without writing or adapting the EasyBlock for Amber, we install the patches with update_amber during the configure phase. Just as in the CSCS EasyConfig, we do 4 builds of Amber in the same directories, in the order serial, OpenMP, hybrid MPI+OpenMP and MPI. Note that the serial build is needed to ensure that the full set of tools is installed, as the other options only seem to install those tools that support either OpenMP or MPI. However, by first doing the serial build, we also ensure that OpenMP versions of the tools will overwrite the serial ones if they exist. We use postinstallcmds to clean up the sources. Some tricks in the EasyConfig to work around problems in Amber: * No host-specific optimizations in CPPTRAJ: The configure script of CPPTRAJ is called without the -tune from AmberTools/src/configure2.* option and therefore does only enable optimizations but no host-specific optimizations. There is a workaround by specifying the compiler option for the host-specific optimizations via the environment variable TUNEFLAGS . NOTE: Amber doesn't always build properly on BeeGFS. It sometimes fails with message such as \"resource busy\". It looks like some locks are not released quickly enough or so.","title":"Amber 18 / AmberTools 19 in the 2019b and 2020a toolchains"},{"location":"generated/easyconfigs/a/Amber/#amber-20-ambertools-20-in-the-2020a-toolchains","text":"A straightforward port. We no longer use TUNEFLAGS as the option gets overwritten anyway as these flags are already added via the CFLAGS etc. passed by EasyBuild (and causes warnings on Rome that they get overwritten).","title":"Amber 20 / AmberTools 20 in the 2020a toolchains"},{"location":"generated/easyconfigs/a/Amber/#in-case-of-problems","text":"It may be necessary to reset CFLAGS etc when calling configure in the EasyConfig and instead make proper use of TUNEFLAGS (see the 2019b toolchain) to set the optimization options (and be careful with AMD Rome). Some files are now compiled with -O0 which gets overwritten by a -O2 included in CFLAGS etc.","title":"In case of problems.."},{"location":"generated/easyconfigs/a/AutoDock_Vina/","text":"AutoDock_Vina instructions AutoDok-Vina web site EasyBuild AutoDock_Vina in the EasyBuilders repository . Note the different name, maybe because we developed ourselves before it was fully supported in EasyBuild? Version 1.1.2 for 2020a Simply downloaded binaries and a standard EasyConfig. Version 1.2.3 for 2022a Contrary to the EasyBuilders recipe, we still use downloaded binaries. The new download is now itself a single binary, so we needed to write a new EasyConfig.","title":"AutoDock_Vina instructions"},{"location":"generated/easyconfigs/a/AutoDock_Vina/#autodock_vina-instructions","text":"AutoDok-Vina web site","title":"AutoDock_Vina instructions"},{"location":"generated/easyconfigs/a/AutoDock_Vina/#easybuild","text":"AutoDock_Vina in the EasyBuilders repository . Note the different name, maybe because we developed ourselves before it was fully supported in EasyBuild?","title":"EasyBuild"},{"location":"generated/easyconfigs/a/AutoDock_Vina/#version-112-for-2020a","text":"Simply downloaded binaries and a standard EasyConfig.","title":"Version 1.1.2 for 2020a"},{"location":"generated/easyconfigs/a/AutoDock_Vina/#version-123-for-2022a","text":"Contrary to the EasyBuilders recipe, we still use downloaded binaries. The new download is now itself a single binary, so we needed to write a new EasyConfig.","title":"Version 1.2.3 for 2022a"},{"location":"generated/easyconfigs/a/archspec/","text":"archspec installation instructions archspec on GitHub GitHub releases archspec documentation on ReadTheDocs archspec on PyPi General information archspec is developed in the context of Spack and later also used by EasyBuild and EESSI. It is Python code to detect the CPU architecture and features of that architecture and can even suggest the right options for several compilers to optimize for that architecture. Archspec is very picky in the dependencies for a tool that should run at the level it does. Installing with EasyBuild There is support for archspec in the EasyBuilders repository . Archspec 0.1.0 in Intel 2020a We stuck to 0.1.0 even though 0.1.2 was out as it turnes out that later versions needed newer versions of certain packages than we have installed in our Python module.","title":"archspec installation instructions"},{"location":"generated/easyconfigs/a/archspec/#archspec-installation-instructions","text":"archspec on GitHub GitHub releases archspec documentation on ReadTheDocs archspec on PyPi","title":"archspec installation instructions"},{"location":"generated/easyconfigs/a/archspec/#general-information","text":"archspec is developed in the context of Spack and later also used by EasyBuild and EESSI. It is Python code to detect the CPU architecture and features of that architecture and can even suggest the right options for several compilers to optimize for that architecture. Archspec is very picky in the dependencies for a tool that should run at the level it does.","title":"General information"},{"location":"generated/easyconfigs/a/archspec/#installing-with-easybuild","text":"There is support for archspec in the EasyBuilders repository .","title":"Installing with EasyBuild"},{"location":"generated/easyconfigs/a/archspec/#archspec-010-in-intel-2020a","text":"We stuck to 0.1.0 even though 0.1.2 was out as it turnes out that later versions needed newer versions of certain packages than we have installed in our Python module.","title":"Archspec 0.1.0 in Intel 2020a"},{"location":"generated/easyconfigs/a/arpack-ng/","text":"ARPACK-NG installation instructions Compiling 3.7.0 with the Intel 2019 update 4 or the 2020 compilers triggered an error message in the mpiifort wrapper script. Hence we sick to 3.6.3 for the 2019b and 2020a toolchains. To do for future versions * Easybuilders EasyConfig for 3.7.0 adds Eigen to the dependencies","title":"ARPACK-NG installation instructions"},{"location":"generated/easyconfigs/a/arpack-ng/#arpack-ng-installation-instructions","text":"Compiling 3.7.0 with the Intel 2019 update 4 or the 2020 compilers triggered an error message in the mpiifort wrapper script. Hence we sick to 3.6.3 for the 2019b and 2020a toolchains. To do for future versions * Easybuilders EasyConfig for 3.7.0 adds Eigen to the dependencies","title":"ARPACK-NG installation instructions"},{"location":"generated/easyconfigs/b/BCFtools/","text":"BCFtools instructions BCFtools home page BCFtools development on GitHub BCFtools documentation General information BCFtools is part of the SAMtools suite. At some point, SAMtools was split in the current SAMtools, HTSlib and BCFtools. BCFtools dependencies HTSlib zlib GSL: optional, for the polysomy command libperl: optional, to support filters using perl syntax BCFtools consists of a single binary, a number of Perl and Python scripts and a large number of shared libraries in libexec/bcftools (plug-ins). EasyBuild There is support for BCFtools in the EasyBuilders repository which we used as a starting point. 1.10.2 for the 2020a toolchains The EasyConfig was developed to prepare for inclusion in the BioTools bundle. The optional Perl filters are currently disabled as it is not clear what is needed to get them to work.","title":"BCFtools instructions"},{"location":"generated/easyconfigs/b/BCFtools/#bcftools-instructions","text":"BCFtools home page BCFtools development on GitHub BCFtools documentation","title":"BCFtools instructions"},{"location":"generated/easyconfigs/b/BCFtools/#general-information","text":"BCFtools is part of the SAMtools suite. At some point, SAMtools was split in the current SAMtools, HTSlib and BCFtools. BCFtools dependencies HTSlib zlib GSL: optional, for the polysomy command libperl: optional, to support filters using perl syntax BCFtools consists of a single binary, a number of Perl and Python scripts and a large number of shared libraries in libexec/bcftools (plug-ins).","title":"General information"},{"location":"generated/easyconfigs/b/BCFtools/#easybuild","text":"There is support for BCFtools in the EasyBuilders repository which we used as a starting point.","title":"EasyBuild"},{"location":"generated/easyconfigs/b/BCFtools/#1102-for-the-2020a-toolchains","text":"The EasyConfig was developed to prepare for inclusion in the BioTools bundle. The optional Perl filters are currently disabled as it is not clear what is needed to get them to work.","title":"1.10.2 for the 2020a toolchains"},{"location":"generated/easyconfigs/b/BEDTools/","text":"BEDTools installation instructions bedtools 2 web site bedtools development on GitHub General information The build process creates a single executable, bin/bedtools , and a lot of shell scripts in the bin subdirectory, but no libraries or other files. Bedtools (as of version 2.29) is not using a Autotools or CMake script to configure the code. It only contains a complicated Makefile. It does have a `make install prefix=... though. It claims to use HTSLib since version 2.28. It cannot use it as a separate library though but includes the source code. In fact, in version 2.29, this turns out to be a rather old version of HTSlib that still contains a number of routines that are not present anymore in recent versions but are used by the code. In good bio-informatics tradition, compilers and compiler flags are hardcoded in the Makefiles in a way that sometimes makes it tricky to replace them with the correct ones (even though the main Makefile adds options in CXXFLAGS to the end of the compiler flags). Given the number of warnings during the compile that return flags of functions called are not used, I have serious doubts about the robustness of that code. To compile bedtools, one needs to redefine CXX, CC and CFLAGS during the build step and CXX during the install step as it links the library again during that step and needs to use the right compiler. CXX is for the main bedtools code. CC and CFLAGS need to be defined on the make command line for HTSlib.","title":"BEDTools installation instructions"},{"location":"generated/easyconfigs/b/BEDTools/#bedtools-installation-instructions","text":"bedtools 2 web site bedtools development on GitHub","title":"BEDTools installation instructions"},{"location":"generated/easyconfigs/b/BEDTools/#general-information","text":"The build process creates a single executable, bin/bedtools , and a lot of shell scripts in the bin subdirectory, but no libraries or other files. Bedtools (as of version 2.29) is not using a Autotools or CMake script to configure the code. It only contains a complicated Makefile. It does have a `make install prefix=... though. It claims to use HTSLib since version 2.28. It cannot use it as a separate library though but includes the source code. In fact, in version 2.29, this turns out to be a rather old version of HTSlib that still contains a number of routines that are not present anymore in recent versions but are used by the code. In good bio-informatics tradition, compilers and compiler flags are hardcoded in the Makefiles in a way that sometimes makes it tricky to replace them with the correct ones (even though the main Makefile adds options in CXXFLAGS to the end of the compiler flags). Given the number of warnings during the compile that return flags of functions called are not used, I have serious doubts about the robustness of that code. To compile bedtools, one needs to redefine CXX, CC and CFLAGS during the build step and CXX during the install step as it links the library again during that step and needs to use the right compiler. CXX is for the main bedtools code. CC and CFLAGS need to be defined on the make command line for HTSlib.","title":"General information"},{"location":"generated/easyconfigs/b/BLAST%2B/","text":"BLAST+ instructions BLAST+ web site download area latest version General information BLAST+ is one component of a bigger toolkit, the NCBI C++ toolkit , but it is also packaged separately. Since version 2.7, LMDB is a dependency for BLAST+ (but a copy is included in the code) A number of possible dependencies are also included in the code. Check the c++/src/util directory and its subdirectories for those extensions. In version 2.9, this includes: zlib and bzip2 (in c++/src/util/compress ) LMDB (in c++/src/util/lmdb ) PCRE (in c++/src/util/regexp ) There is a load of other dependencies according to the configure script. However, these seem to be the dependencies for the whole NCBI C++ toolkit. Inspection of the shared libraries shows that most are not used in any of the executables generated by the BLAST+ build. EasyBuild information The development of these instructions started with the 2020a toolchains. BLAST+ 2.9.0 For the 2020a toolchains we simply started from the EasyBuilders file as we ran into compile problems when building upon our previous efforts. Checking the executables, the following packages are linked as shared libraries: zlib (libz.so.1) bzip2 (libbz2.so.1.0) LZO (liblzo.so.2) PCRE (libpcre.so.1) LMDB (liblmdb.so) Hence we removed all references to packages that don't seem to be used in the EasyBuild recipe. BLAST+ 2.10.0 Started from the 2.9.0 files for the 2020a toolchain. Patches from the 2.9 version don't seem to be needed. Dependencies seem to be the same as for 2.9.0. The configure script picks up a lot of other libraries from our baselibs module but they don't seem to be used at all. --with-64 causes trouble and is not needed as the compilers only emit 64-bit code, so it has been omitted.","title":"BLAST+ instructions"},{"location":"generated/easyconfigs/b/BLAST%2B/#blast-instructions","text":"BLAST+ web site download area latest version","title":"BLAST+ instructions"},{"location":"generated/easyconfigs/b/BLAST%2B/#general-information","text":"BLAST+ is one component of a bigger toolkit, the NCBI C++ toolkit , but it is also packaged separately. Since version 2.7, LMDB is a dependency for BLAST+ (but a copy is included in the code) A number of possible dependencies are also included in the code. Check the c++/src/util directory and its subdirectories for those extensions. In version 2.9, this includes: zlib and bzip2 (in c++/src/util/compress ) LMDB (in c++/src/util/lmdb ) PCRE (in c++/src/util/regexp ) There is a load of other dependencies according to the configure script. However, these seem to be the dependencies for the whole NCBI C++ toolkit. Inspection of the shared libraries shows that most are not used in any of the executables generated by the BLAST+ build.","title":"General information"},{"location":"generated/easyconfigs/b/BLAST%2B/#easybuild-information","text":"The development of these instructions started with the 2020a toolchains.","title":"EasyBuild information"},{"location":"generated/easyconfigs/b/BLAST%2B/#blast-290","text":"For the 2020a toolchains we simply started from the EasyBuilders file as we ran into compile problems when building upon our previous efforts. Checking the executables, the following packages are linked as shared libraries: zlib (libz.so.1) bzip2 (libbz2.so.1.0) LZO (liblzo.so.2) PCRE (libpcre.so.1) LMDB (liblmdb.so) Hence we removed all references to packages that don't seem to be used in the EasyBuild recipe.","title":"BLAST+ 2.9.0"},{"location":"generated/easyconfigs/b/BLAST%2B/#blast-2100","text":"Started from the 2.9.0 files for the 2020a toolchain. Patches from the 2.9 version don't seem to be needed. Dependencies seem to be the same as for 2.9.0. The configure script picks up a lot of other libraries from our baselibs module but they don't seem to be used at all. --with-64 causes trouble and is not needed as the compilers only emit 64-bit code, so it has been omitted.","title":"BLAST+ 2.10.0"},{"location":"generated/easyconfigs/b/BWA/","text":"BWA EasyBuild recipe EasyBuild has an EasyBlock for BWA. That derives from ConfigureMake, but * with an empty configure step as there is no configure script * The install step is replaced by a manual copy of the files to the correct location. * A suitable sanity check is defined in the EasyBlock. We started from the EasyBuilders recipes for BWA. At the time when we installed BWA, those recipes didn't work as intended though as they did not pick up the compiler and compiler options as specified by EasyBuild. The Makefile of BWA has all variables hard coded into it and hence does not pick up variables from the environment. The solution to this is to define them through buildopts : buildopts = 'CC=\"$CC\" CFLAGS=\"$CFLAGS\"' Furthermore, some additional extensions were made to the recipe: * A sanity check was added: The recipe now checks if the files that should be generated are indeed generated. * Added additional documentation to the EasyConfig. EasyConfigs for both Intel and GCCcore are included in this directory and were tested.","title":"BWA EasyBuild recipe"},{"location":"generated/easyconfigs/b/BWA/#bwa-easybuild-recipe","text":"EasyBuild has an EasyBlock for BWA. That derives from ConfigureMake, but * with an empty configure step as there is no configure script * The install step is replaced by a manual copy of the files to the correct location. * A suitable sanity check is defined in the EasyBlock. We started from the EasyBuilders recipes for BWA. At the time when we installed BWA, those recipes didn't work as intended though as they did not pick up the compiler and compiler options as specified by EasyBuild. The Makefile of BWA has all variables hard coded into it and hence does not pick up variables from the environment. The solution to this is to define them through buildopts : buildopts = 'CC=\"$CC\" CFLAGS=\"$CFLAGS\"' Furthermore, some additional extensions were made to the recipe: * A sanity check was added: The recipe now checks if the files that should be generated are indeed generated. * Added additional documentation to the EasyConfig. EasyConfigs for both Intel and GCCcore are included in this directory and were tested.","title":"BWA EasyBuild recipe"},{"location":"generated/easyconfigs/b/BerkeleyGW/","text":"BerkeleyGW installation notes This file was started with the development of the EasyConfig for version 2.1 in the Intel 2019b toolchain. Earlier EasyConfig files are developed from the default easybuilders files but with additional documentation added to the module. EasyConfig General remarks There is no formula to compute the download link. The link can be recovered from https://berkeleygw.org/download/ . Version 2.1 The download link is different from previous version. The link on the download page takes you to a previewer where you have to click download again. See the EasyConfig for the trick that worked to construct a working download link from the URL copied on the \"Download\" page . The patch `BerkeleyGW-1.2.0_fix_intent.patchBerkeleyGW-1.2.0_fix_intent.patch`` for the wrong intent of certain Fortran declarations in one of the Sigma source files is no longer needed. That file has been corrected. The patch BerkeleyGW-2.0.0_fix_path.patch does not work with version 2.1. The block in the Makefile is now further down, and the file manual.html does no longer exist. Hence the patch has been modified to copy the whole documentation directory (using cp -r ).","title":"BerkeleyGW installation notes"},{"location":"generated/easyconfigs/b/BerkeleyGW/#berkeleygw-installation-notes","text":"This file was started with the development of the EasyConfig for version 2.1 in the Intel 2019b toolchain. Earlier EasyConfig files are developed from the default easybuilders files but with additional documentation added to the module.","title":"BerkeleyGW installation notes"},{"location":"generated/easyconfigs/b/BerkeleyGW/#easyconfig","text":"","title":"EasyConfig"},{"location":"generated/easyconfigs/b/BerkeleyGW/#general-remarks","text":"There is no formula to compute the download link. The link can be recovered from https://berkeleygw.org/download/ .","title":"General remarks"},{"location":"generated/easyconfigs/b/BerkeleyGW/#version-21","text":"The download link is different from previous version. The link on the download page takes you to a previewer where you have to click download again. See the EasyConfig for the trick that worked to construct a working download link from the URL copied on the \"Download\" page . The patch `BerkeleyGW-1.2.0_fix_intent.patchBerkeleyGW-1.2.0_fix_intent.patch`` for the wrong intent of certain Fortran declarations in one of the Sigma source files is no longer needed. That file has been corrected. The patch BerkeleyGW-2.0.0_fix_path.patch does not work with version 2.1. The block in the Makefile is now further down, and the file manual.html does no longer exist. Hence the patch has been modified to copy the whole documentation directory (using cp -r ).","title":"Version 2.1"},{"location":"generated/easyconfigs/b/BioTools/","text":"BioTools instructions BioTools is a bundle of tools popular in the bioinformatics groups at UAntwerpen. It was developed to reduce module clutter on our systems. Packages BCFtools ConfigureMake build process Depends on HTSlib in this Bundle BCFtools consists of a single binary, a number of Perl and Python scripts and a large number of shared libraries in libexec/bcftools (plug-ins). bedtools 2 Development on GitHub Claims to use HTSlib since version 2.28, but it actually includes the necessary code and doesn't seem to have an option yet to include an existing HTSlib library. In fact, the version it includes is rather old and the code relies on features of that old version that are not present in current versions anymore. No configure or CMake, only a Makefile... It does have a `make install prefix=... though. The build process creates a single executable, bin/bedtools , and a lot of shell scripts in the bin subdirectory, but no libraries or other files. CD-HIT Develpment on GitHub MakeCp build process to ensure that we also install some files that are not done by make install . The build process generates a number of binaries and a lot of Perl scripts, but no libraries. ClonalFrameML MakeCp build process. The build process generates a single binary and a R script. fastp Just a make/make install build process, and several variables needed to get the compiler and optimizations right. The build process generates a single executable. HTSlib Development on GitHub ConfigureMake build process. The build process produces three binaries, a shared and a static library. MCL ConfigureMake, but there are two files that are not copied by the make install. The build process generates several binaries, a bash and three Perl scripts, but no libraries. MEGAHIT CMakeMake The build process only produces executables. miniasm MakeCp, and needs some tricks. The build process produces two executables Minimap2 MakeCp, and needs some tricks. The build process produces three binaries and a static library. Ancient: no multicore parallelism, SIMD support only up to SSE 4.1. MUSCLE MakeCp, but does need a patch as the compiler name is hard-coded in the Makefile and not even in a variable that we can overwrite Build process creates a single file, bin/muscle Racon CMakeMake build process. The build process only generates the executable bin racon`. SAMtools ConfigureMake build process, but using an EasyBlock to install additional files that are not installed by make install . Depends on HTSlib in this Bundle SAMtools contains a number of binaries, a static libraries, several Perl scripts but also two LUA scripts. VSEARCH Autotools-based, but needs some preprocessing and configopts. The build process generates a single executable EasyConfigs 2020a bundles HTSlib 1.10.2 BCFtools 1.10.2 CD-HIT 4.8.1 ClonalFrameML 1.12 SAMtools 1.10 bedtools 2.29.2 fastp 0.20.1 MCL 14.137 minimap 2.17 MUSCLE 3.8.31 VSEARCH 2.14.2","title":"BioTools instructions"},{"location":"generated/easyconfigs/b/BioTools/#biotools-instructions","text":"BioTools is a bundle of tools popular in the bioinformatics groups at UAntwerpen. It was developed to reduce module clutter on our systems.","title":"BioTools instructions"},{"location":"generated/easyconfigs/b/BioTools/#packages","text":"BCFtools ConfigureMake build process Depends on HTSlib in this Bundle BCFtools consists of a single binary, a number of Perl and Python scripts and a large number of shared libraries in libexec/bcftools (plug-ins). bedtools 2 Development on GitHub Claims to use HTSlib since version 2.28, but it actually includes the necessary code and doesn't seem to have an option yet to include an existing HTSlib library. In fact, the version it includes is rather old and the code relies on features of that old version that are not present in current versions anymore. No configure or CMake, only a Makefile... It does have a `make install prefix=... though. The build process creates a single executable, bin/bedtools , and a lot of shell scripts in the bin subdirectory, but no libraries or other files. CD-HIT Develpment on GitHub MakeCp build process to ensure that we also install some files that are not done by make install . The build process generates a number of binaries and a lot of Perl scripts, but no libraries. ClonalFrameML MakeCp build process. The build process generates a single binary and a R script. fastp Just a make/make install build process, and several variables needed to get the compiler and optimizations right. The build process generates a single executable. HTSlib Development on GitHub ConfigureMake build process. The build process produces three binaries, a shared and a static library. MCL ConfigureMake, but there are two files that are not copied by the make install. The build process generates several binaries, a bash and three Perl scripts, but no libraries. MEGAHIT CMakeMake The build process only produces executables. miniasm MakeCp, and needs some tricks. The build process produces two executables Minimap2 MakeCp, and needs some tricks. The build process produces three binaries and a static library. Ancient: no multicore parallelism, SIMD support only up to SSE 4.1. MUSCLE MakeCp, but does need a patch as the compiler name is hard-coded in the Makefile and not even in a variable that we can overwrite Build process creates a single file, bin/muscle Racon CMakeMake build process. The build process only generates the executable bin racon`. SAMtools ConfigureMake build process, but using an EasyBlock to install additional files that are not installed by make install . Depends on HTSlib in this Bundle SAMtools contains a number of binaries, a static libraries, several Perl scripts but also two LUA scripts. VSEARCH Autotools-based, but needs some preprocessing and configopts. The build process generates a single executable","title":"Packages"},{"location":"generated/easyconfigs/b/BioTools/#easyconfigs","text":"","title":"EasyConfigs"},{"location":"generated/easyconfigs/b/BioTools/#2020a-bundles","text":"HTSlib 1.10.2 BCFtools 1.10.2 CD-HIT 4.8.1 ClonalFrameML 1.12 SAMtools 1.10 bedtools 2.29.2 fastp 0.20.1 MCL 14.137 minimap 2.17 MUSCLE 3.8.31 VSEARCH 2.14.2","title":"2020a bundles"},{"location":"generated/easyconfigs/b/BioTools-Python/","text":"BioTools-Python instructions BioTools-Python is a bundle with various Python packages for bio-informatics, often wrappers around tools included in the BioTools bundle. Included tools biopython ( PyPi }) Flye Tricky since it can only be downloaded from GitHub with a meaningless filename HTSeq ( Documentation and development on GitHub ) Uses Pysam [pysam]https://pypi.org/project/pysam/) ( homepage on GitHub ) Can use HTSlib from BioTools only if versions match which is rarely the case, so by default we use the internal one. SICER2 ( Homepage/documentation ) Uses BEDTools, though I can't find how to indicate it during installation that BEDTools is available, so it might just use commands from BEDTools at runtime. Packages that should be considered for total removal: * DLCpar * Doesn't seem to be fully Python 3 compliant, I see problems in the install log files. Note that several of these tools use other tools, but rather than being able to use an existing version, include their own. Hence those tools may all behave differently as the dependencies may be compiled with different features enabled. Moreover, we cannot exclude that in the long run conflicts will arise if tools would also include the regular binaries and libraries of those tools they depend on. EasyConfigs The EasyConfig was developed at UAntwerp. One particular problem we had to cope with is Python packages that are not on PyPi but are downloaded from, e.g., GitHub. They may have names such as v%(version)s.zip or even %(version)s.zip. The problem we have then is that in a Bundle or extension list, all files of packages are stored together and the filenames might not be unique. The root of the problem is that EasyBuild has a mechanism to download sources of regular packages or of components of a bundle and rename them on the way to a more meaningful name, but has no such mechanism for extensions. The solution we implemented consists of several components: * We use Bundle as the main EasyBlock rather than PythonBundle, and set exts_defaultclass = 'PythonPackage' instead. * We now also manually need to adjust PYTHONPATH in the EasyConfig. * We still install all packages via the extension list mechanism simply because this assures that they are all equally listed with version number in module spider . However, those of which we want to rename the downloaded file are \"installed\" through the components. This is a fake install though: We use the EasyBlock Tarball and skip the install step so that effectively nothing happens to the install directories.","title":"BioTools-Python instructions"},{"location":"generated/easyconfigs/b/BioTools-Python/#biotools-python-instructions","text":"BioTools-Python is a bundle with various Python packages for bio-informatics, often wrappers around tools included in the BioTools bundle.","title":"BioTools-Python instructions"},{"location":"generated/easyconfigs/b/BioTools-Python/#included-tools","text":"biopython ( PyPi }) Flye Tricky since it can only be downloaded from GitHub with a meaningless filename HTSeq ( Documentation and development on GitHub ) Uses Pysam [pysam]https://pypi.org/project/pysam/) ( homepage on GitHub ) Can use HTSlib from BioTools only if versions match which is rarely the case, so by default we use the internal one. SICER2 ( Homepage/documentation ) Uses BEDTools, though I can't find how to indicate it during installation that BEDTools is available, so it might just use commands from BEDTools at runtime. Packages that should be considered for total removal: * DLCpar * Doesn't seem to be fully Python 3 compliant, I see problems in the install log files. Note that several of these tools use other tools, but rather than being able to use an existing version, include their own. Hence those tools may all behave differently as the dependencies may be compiled with different features enabled. Moreover, we cannot exclude that in the long run conflicts will arise if tools would also include the regular binaries and libraries of those tools they depend on.","title":"Included tools"},{"location":"generated/easyconfigs/b/BioTools-Python/#easyconfigs","text":"The EasyConfig was developed at UAntwerp. One particular problem we had to cope with is Python packages that are not on PyPi but are downloaded from, e.g., GitHub. They may have names such as v%(version)s.zip or even %(version)s.zip. The problem we have then is that in a Bundle or extension list, all files of packages are stored together and the filenames might not be unique. The root of the problem is that EasyBuild has a mechanism to download sources of regular packages or of components of a bundle and rename them on the way to a more meaningful name, but has no such mechanism for extensions. The solution we implemented consists of several components: * We use Bundle as the main EasyBlock rather than PythonBundle, and set exts_defaultclass = 'PythonPackage' instead. * We now also manually need to adjust PYTHONPATH in the EasyConfig. * We still install all packages via the extension list mechanism simply because this assures that they are all equally listed with version number in module spider . However, those of which we want to rename the downloaded file are \"installed\" through the components. This is a fake install though: We use the EasyBlock Tarball and skip the install step so that effectively nothing happens to the install directories.","title":"EasyConfigs"},{"location":"generated/easyconfigs/b/Bowtie2/","text":"Bowtie installation instructions Bowtie GitHub Bowtie on SourceForge EasyConfigs We developed our own Bowtie2 EasyBuild recipes, first based on the MakeCp easyblock, but moving to CMakeMake cince version 2.3.5.1. This is to have more control over the options than when using the Bowtie2 EasyBlock that is used in the EasyBuilders recipes. 2.3.5.1, 2018b and 2019b toolchains CMake for Bowtie2 sets a number of C compiler options in addition to the ones passed to CMake. These are GNU-specific and should be removed as they do cause warnings. Moreover, one of them, ''-msse2'', might even be counterproductive and force optimizing for an older processor. Hence the lines: set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -m64 -g3 -Wall -msse2\") set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -O0\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -O3 -funroll-loops\") should be replaced with set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\") set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG}\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE}\") (Changing the second line is not really needed) 2.4.1, 2020a toolchain The patch for the compiler flags developed for version 2.3.5.1 is still needed in 2.4.1, but the line numbers have changed so we needed to re-build the patch. Rather than using a patch though, we edit CMakeLists through sed in the preconfigopts. Using sed -e 's/ -m64 -g3 -Wall -msse2//' -e 's/ -O3 -funroll-loops//' -i CMakeLists.txt we edit the lines set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -m64 -g3 -Wall -msse2\") set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -O0\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -O3 -funroll-loops\") in CMAkeLists.txt to set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\") set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -O0\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE}\") Note though that in later versions, a different edit might be needed! Bowtie2 works with Intel TBB but it takes some care to get it to work that way. * Not specifying any TBB options: The code includes its own TBB library (2019_U4) but that version does not compile with the Intel 2020 compilers. Note that this is not specific to this version of Bowtie: Trying to recompile older versions (that likely also use an older TBB version) also fails. It looks like the version of TBB included in those codes is not compatible with the Intel 19.1 compoilers in the 2020a toolchain. * Including TBB_LIB_PATH etc.: Though CMake is happy with it, the code fails to link. It is not clear why it fails to find the appropriate libraries. * The trick that does it is to add -tbb to the compiler flags combined with the previous point (the TBB_LIB_PATH etc.) to make CMake happy. * The alternative suggestion (not tried) would be to edit CMakeLists.txt`` and the ``Makefile`` in the root source directory, search for ``2019_U4`` and replace with a newer version of TBB (``2020.1`` at the moment that the recipe was developed). Note that the URL to the GitHub repository of TBB is also outdated as the account\\ and project has changed name for oneAPI. * The non-TBB build succeeds though (setting ``-DNO_TBB=1 ).","title":"Bowtie installation instructions"},{"location":"generated/easyconfigs/b/Bowtie2/#bowtie-installation-instructions","text":"Bowtie GitHub Bowtie on SourceForge","title":"Bowtie installation instructions"},{"location":"generated/easyconfigs/b/Bowtie2/#easyconfigs","text":"We developed our own Bowtie2 EasyBuild recipes, first based on the MakeCp easyblock, but moving to CMakeMake cince version 2.3.5.1. This is to have more control over the options than when using the Bowtie2 EasyBlock that is used in the EasyBuilders recipes.","title":"EasyConfigs"},{"location":"generated/easyconfigs/b/Bowtie2/#2351-2018b-and-2019b-toolchains","text":"CMake for Bowtie2 sets a number of C compiler options in addition to the ones passed to CMake. These are GNU-specific and should be removed as they do cause warnings. Moreover, one of them, ''-msse2'', might even be counterproductive and force optimizing for an older processor. Hence the lines: set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -m64 -g3 -Wall -msse2\") set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -O0\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -O3 -funroll-loops\") should be replaced with set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\") set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG}\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE}\") (Changing the second line is not really needed)","title":"2.3.5.1, 2018b and 2019b toolchains"},{"location":"generated/easyconfigs/b/Bowtie2/#241-2020a-toolchain","text":"The patch for the compiler flags developed for version 2.3.5.1 is still needed in 2.4.1, but the line numbers have changed so we needed to re-build the patch. Rather than using a patch though, we edit CMakeLists through sed in the preconfigopts. Using sed -e 's/ -m64 -g3 -Wall -msse2//' -e 's/ -O3 -funroll-loops//' -i CMakeLists.txt we edit the lines set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -m64 -g3 -Wall -msse2\") set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -O0\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -O3 -funroll-loops\") in CMAkeLists.txt to set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\") set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -O0\") set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE}\") Note though that in later versions, a different edit might be needed! Bowtie2 works with Intel TBB but it takes some care to get it to work that way. * Not specifying any TBB options: The code includes its own TBB library (2019_U4) but that version does not compile with the Intel 2020 compilers. Note that this is not specific to this version of Bowtie: Trying to recompile older versions (that likely also use an older TBB version) also fails. It looks like the version of TBB included in those codes is not compatible with the Intel 19.1 compoilers in the 2020a toolchain. * Including TBB_LIB_PATH etc.: Though CMake is happy with it, the code fails to link. It is not clear why it fails to find the appropriate libraries. * The trick that does it is to add -tbb to the compiler flags combined with the previous point (the TBB_LIB_PATH etc.) to make CMake happy. * The alternative suggestion (not tried) would be to edit CMakeLists.txt`` and the ``Makefile`` in the root source directory, search for ``2019_U4`` and replace with a newer version of TBB (``2020.1`` at the moment that the recipe was developed). Note that the URL to the GitHub repository of TBB is also outdated as the account\\ and project has changed name for oneAPI. * The non-TBB build succeeds though (setting ``-DNO_TBB=1 ).","title":"2.4.1, 2020a toolchain"},{"location":"generated/easyconfigs/b/baselibs/","text":"Baselibs Remarks on individual packages wget wget can now use libidn2 instead of libidn and PCRE2 instead of PCRE. According to the EasyBuilders recipes wget 1.20 needs OpenSSL 1.0.1s or newer or GnuTLS 1.2.11 or newer, with GnuTLS being the default it looks for if no --with-ssl is specified. EasyConfigs 2020a bundle Added lzip to the bundle. Added libdeflate to the bundle. Note that libdeflate contains its own gzip program, but when installing it renames the executables to deflate-gzip and deflate-gunzip to avoid a name clash. Added libidn and the newer libidn2 to the bundle Added wget to the bundle. Not really a library, but it is a tool that gets used by some packages and that requires other components from baseutils, so this is a better place to put it than, e.g., our buildtools module. Added mpdecimal to the bundle, a library that is used by Python in its built-in cdecimal package. Added LZO and Blosc to the bundle. These libraries are used by PyTables (tables on PyPi) which is used by pandas, a package popular with our big data users. Added GDBM, usefull for the dbm package in the Python standard library. Added LMDB, used by BLAST+ Added double-conversion, needed for Qt5. We perform 3 build passes for double-conversion: static, static with position-independent code and shared. Note however that due to our choice of toolchainopts, the pure static in fact already contains position-independent code. We decided to still perform the three passes two have all library names generated (rather than solving it with symbolic links). Added snappy, needed for Qt5 We perform 2 build passes to have both static and shared libraries. Note that it also needs some care to set AVX-specific options. Added libtool also to baselibs even though it is in buildtools already. It turns out that there is software that links to the libtools libraries (in particular NEST does so), and we want to keep buildtools build dependency only as we compile it with the system compilers. Added various sound-related libraries libogg (Ogg container format) libvorbis: needs libogg, implements the OggVorbis codec. FLAC: can use libogg libsndfile: Encapsulates libopus, lib ogg/libvorbis, FLAC. Also added libtheora to complete the line. It can be used in FFmpeg.","title":"Baselibs"},{"location":"generated/easyconfigs/b/baselibs/#baselibs","text":"","title":"Baselibs"},{"location":"generated/easyconfigs/b/baselibs/#remarks-on-individual-packages","text":"","title":"Remarks on individual packages"},{"location":"generated/easyconfigs/b/baselibs/#wget","text":"wget can now use libidn2 instead of libidn and PCRE2 instead of PCRE. According to the EasyBuilders recipes wget 1.20 needs OpenSSL 1.0.1s or newer or GnuTLS 1.2.11 or newer, with GnuTLS being the default it looks for if no --with-ssl is specified.","title":"wget"},{"location":"generated/easyconfigs/b/baselibs/#easyconfigs","text":"","title":"EasyConfigs"},{"location":"generated/easyconfigs/b/baselibs/#2020a-bundle","text":"Added lzip to the bundle. Added libdeflate to the bundle. Note that libdeflate contains its own gzip program, but when installing it renames the executables to deflate-gzip and deflate-gunzip to avoid a name clash. Added libidn and the newer libidn2 to the bundle Added wget to the bundle. Not really a library, but it is a tool that gets used by some packages and that requires other components from baseutils, so this is a better place to put it than, e.g., our buildtools module. Added mpdecimal to the bundle, a library that is used by Python in its built-in cdecimal package. Added LZO and Blosc to the bundle. These libraries are used by PyTables (tables on PyPi) which is used by pandas, a package popular with our big data users. Added GDBM, usefull for the dbm package in the Python standard library. Added LMDB, used by BLAST+ Added double-conversion, needed for Qt5. We perform 3 build passes for double-conversion: static, static with position-independent code and shared. Note however that due to our choice of toolchainopts, the pure static in fact already contains position-independent code. We decided to still perform the three passes two have all library names generated (rather than solving it with symbolic links). Added snappy, needed for Qt5 We perform 2 build passes to have both static and shared libraries. Note that it also needs some care to set AVX-specific options. Added libtool also to baselibs even though it is in buildtools already. It turns out that there is software that links to the libtools libraries (in particular NEST does so), and we want to keep buildtools build dependency only as we compile it with the system compilers. Added various sound-related libraries libogg (Ogg container format) libvorbis: needs libogg, implements the OggVorbis codec. FLAC: can use libogg libsndfile: Encapsulates libopus, lib ogg/libvorbis, FLAC. Also added libtheora to complete the line. It can be used in FFmpeg.","title":"2020a bundle"},{"location":"generated/easyconfigs/b/binutils/","text":"binutils instructions GNU binutils web page GNU binutils download EasyConfigs 2020a toolchains The toolchains are officially based on binutils 2.34 and GCC 9.3. The problem with binutils 2.33.1 and 2.34 is that it keeps trying installing the documentation, even when makeinfo is not present on the system. This causes a failure of the build process. The workaround was to define MAKEINFO=true in buildopts and installopts.","title":"binutils instructions"},{"location":"generated/easyconfigs/b/binutils/#binutils-instructions","text":"GNU binutils web page GNU binutils download","title":"binutils instructions"},{"location":"generated/easyconfigs/b/binutils/#easyconfigs","text":"","title":"EasyConfigs"},{"location":"generated/easyconfigs/b/binutils/#2020a-toolchains","text":"The toolchains are officially based on binutils 2.34 and GCC 9.3. The problem with binutils 2.33.1 and 2.34 is that it keeps trying installing the documentation, even when makeinfo is not present on the system. This causes a failure of the build process. The workaround was to define MAKEINFO=true in buildopts and installopts.","title":"2020a toolchains"},{"location":"generated/easyconfigs/b/buildtools/","text":"buildtools module Buildtools is a collection of various build tools installed in a single module and directory tree. We update it once with every toolchain and give it a version number based on the toolchain. The original setup was to only include executables and not libraries. However, that created a build dependency on sufficiently recent versions of Bison 3.0 and flex, so we decided to include them also even though they provide libraries that we may want to compile with a more recent GCC when used in applications (though I expect that even then those libraries will only be used on a non-performance-critical part of the code, I would expect in I/O. And by specifying other flex and/or Bison modules in the right order when building those applications, we may even totally avoid these problems. Contents The contents of the module evolved over time. It does contain a subset of: * byacc * version check download location * byacc in the EasyBuilders repository * flex * version check GitHub * flex in the EasyBuilders repository * git * version check GitHub * git in the EasyBuilders repository * GNU Autoconf * version check download location * Autoconf in the EasyBuilders repository * GNU Automake * version check download location * Automake in the EasyBuilders repository * GNU Autoconf-archive * version check download location * Autoconf-archive in the EasyBuilders repository * GNU Bison * version check on download location * Bison in the EasyBuilders repository * GNU libtool * version check on the download location * libtool in the EasyBuilders repository * GNU M4 * version check on the download loacation \\ * M4 in the EasyBuilders repository * GNU make * version check on the download location * [make in the EasyBuilders repository] * GNU sed * version check on the download location * No support for sed in the EasyBuilders repository * CMake * version check on the CMake web site * Releases on the GitHub mirror of the development repository * CMake in the EasyBuilders repository * Ninja * version check on the Ninja web site * Releases on the Ninja GitHub * Ninja in the EAsyBuilders repository * NASM * version check on the NASM web site * Releases on the NASM GitHub * Yasm * version check on the Yasm web site * Releases on the Yasm GitHub * Yasm in th EasyBuilders repository * patchelf * version check on the download location * patchelf in the EasyBuilders repository * pkg-config * version check on the download location * pkg-config in the EasyBuilders repository * GNU gperf * version check on the gperf web site * version check on the download location * gperf in the EasyBuilders repository * GNU help2man * version check on the download location * help2man in the EasyBuilders repository * Doxygen * version check on the doxygen web site * version check on the doxugen GitHub * Doxygen on the EasyBuilders web site * re2c * version check on the re2c GitHub * re2c in the EasyBuilders repository * Meson * version check on PyPi * Meson in the EasyBuilders repository * SCons * Version check on PyPi * SCons in the EasyBuilders repository EasyConfigs Notes This documentation was started when developing the 2020a version of the module. CMake still requires an ncurses and OpenSSL library from the system. There are dependencies between those packages, so sometimes the order in the EasyConfig file does matter and is chosen to use the newly installed tools for the installation of some of the other tools in the bundle. 2020a We did notice that some GNU tools are no longer available in the tar.bz2 format and instead in .tar.lz. However, since there lzip was not yet installed on our systems and since there is no easy-to-recognize SOURCE_TAR_LZ or SOURCELOWER_TAR_LZ constant defined in EasyBuild, we do not yet use that format. We did add EBROOT and EBVERSION variables for all components for increased compatibility with standard EasysBuild-generated modules (in case those variables would, e.g., be used in EasyBlocks for certain software packages). Added re2c and SCons to the bundle. Versions used: * Byacc 20191125 * Flex 2.6.4 * re2c 1.3 * git 2.26.0 * GNU Autoconf 2.69 * GNU Automake 1.16.2 * GNU Bison 3.5.3 * GNU libtool 2.4.6 * GNU M4 1.4.18 * GNU make 4.3. Switched back to .tar.gz as there was no .tar.bz2 file anymore * GNU sed 4.8 * CMake 3.17.0 * Ninja 1.10.0 * Meson 0.53.2 * SCons 3.1.2 * NASM 2.14.02 * Yasm 1.3.0 * patchelf 0.10 * pkg-config 0.29.2 * GNU gperf 3.1 * GNU help2man 1.47.13 * Doxygen 1.8.18 2022a Removed those packages that use the system Python and require PYTHONPATH to be set as they don't provide shell script wrappers: Meson and SCons. These will appear in buildtools-systempython and buildtools-python . Versions used: * Byacc 20220128 * Flex 2.6.4 * git 2.36.0 (2022a uses 2.36, but upgraded to 2.36.3) * GNU Autoconf 2.71 * GNU Automake 1.16.2 * GNU Autoconf-archive 2022.09.03 * GNU Bison 3.8.2 * GNU libtool 2.4.6 * GNU M4 1.4.19 * GNU make 4.3. Switched back to .tar.gz as there was no .tar.bz2 file anymore * GNU sed 4.8 * CMake 3.23.4 (2022a still uses 3.23.1 but we went for the latest bugfix release at the time of development) * Ninja 1.10.2 (version used in 2022a, not the newest at time of development) * NASM 2.15.05 * Yasm 1.3.0 * patchelf 0.15.0 * pkg-config 0.29.2 * GNU gperf 3.1 * GNU help2man 1.49.2 * Doxygen 1.9.5 (2022a uses 1.9.4, but we went for the latest bugfix release at the time of development) * re2c 2.2 (Version used in 2022a, not clear why the community did not upgrade to 3.0 which was out before the development of 2022a even started)","title":"buildtools module"},{"location":"generated/easyconfigs/b/buildtools/#buildtools-module","text":"Buildtools is a collection of various build tools installed in a single module and directory tree. We update it once with every toolchain and give it a version number based on the toolchain. The original setup was to only include executables and not libraries. However, that created a build dependency on sufficiently recent versions of Bison 3.0 and flex, so we decided to include them also even though they provide libraries that we may want to compile with a more recent GCC when used in applications (though I expect that even then those libraries will only be used on a non-performance-critical part of the code, I would expect in I/O. And by specifying other flex and/or Bison modules in the right order when building those applications, we may even totally avoid these problems.","title":"buildtools module"},{"location":"generated/easyconfigs/b/buildtools/#contents","text":"The contents of the module evolved over time. It does contain a subset of: * byacc * version check download location * byacc in the EasyBuilders repository * flex * version check GitHub * flex in the EasyBuilders repository * git * version check GitHub * git in the EasyBuilders repository * GNU Autoconf * version check download location * Autoconf in the EasyBuilders repository * GNU Automake * version check download location * Automake in the EasyBuilders repository * GNU Autoconf-archive * version check download location * Autoconf-archive in the EasyBuilders repository * GNU Bison * version check on download location * Bison in the EasyBuilders repository * GNU libtool * version check on the download location * libtool in the EasyBuilders repository * GNU M4 * version check on the download loacation \\ * M4 in the EasyBuilders repository * GNU make * version check on the download location * [make in the EasyBuilders repository] * GNU sed * version check on the download location * No support for sed in the EasyBuilders repository * CMake * version check on the CMake web site * Releases on the GitHub mirror of the development repository * CMake in the EasyBuilders repository * Ninja * version check on the Ninja web site * Releases on the Ninja GitHub * Ninja in the EAsyBuilders repository * NASM * version check on the NASM web site * Releases on the NASM GitHub * Yasm * version check on the Yasm web site * Releases on the Yasm GitHub * Yasm in th EasyBuilders repository * patchelf * version check on the download location * patchelf in the EasyBuilders repository * pkg-config * version check on the download location * pkg-config in the EasyBuilders repository * GNU gperf * version check on the gperf web site * version check on the download location * gperf in the EasyBuilders repository * GNU help2man * version check on the download location * help2man in the EasyBuilders repository * Doxygen * version check on the doxygen web site * version check on the doxugen GitHub * Doxygen on the EasyBuilders web site * re2c * version check on the re2c GitHub * re2c in the EasyBuilders repository * Meson * version check on PyPi * Meson in the EasyBuilders repository * SCons * Version check on PyPi * SCons in the EasyBuilders repository","title":"Contents"},{"location":"generated/easyconfigs/b/buildtools/#easyconfigs","text":"","title":"EasyConfigs"},{"location":"generated/easyconfigs/b/buildtools/#notes","text":"This documentation was started when developing the 2020a version of the module. CMake still requires an ncurses and OpenSSL library from the system. There are dependencies between those packages, so sometimes the order in the EasyConfig file does matter and is chosen to use the newly installed tools for the installation of some of the other tools in the bundle.","title":"Notes"},{"location":"generated/easyconfigs/b/buildtools/#2020a","text":"We did notice that some GNU tools are no longer available in the tar.bz2 format and instead in .tar.lz. However, since there lzip was not yet installed on our systems and since there is no easy-to-recognize SOURCE_TAR_LZ or SOURCELOWER_TAR_LZ constant defined in EasyBuild, we do not yet use that format. We did add EBROOT and EBVERSION variables for all components for increased compatibility with standard EasysBuild-generated modules (in case those variables would, e.g., be used in EasyBlocks for certain software packages). Added re2c and SCons to the bundle. Versions used: * Byacc 20191125 * Flex 2.6.4 * re2c 1.3 * git 2.26.0 * GNU Autoconf 2.69 * GNU Automake 1.16.2 * GNU Bison 3.5.3 * GNU libtool 2.4.6 * GNU M4 1.4.18 * GNU make 4.3. Switched back to .tar.gz as there was no .tar.bz2 file anymore * GNU sed 4.8 * CMake 3.17.0 * Ninja 1.10.0 * Meson 0.53.2 * SCons 3.1.2 * NASM 2.14.02 * Yasm 1.3.0 * patchelf 0.10 * pkg-config 0.29.2 * GNU gperf 3.1 * GNU help2man 1.47.13 * Doxygen 1.8.18","title":"2020a"},{"location":"generated/easyconfigs/b/buildtools/#2022a","text":"Removed those packages that use the system Python and require PYTHONPATH to be set as they don't provide shell script wrappers: Meson and SCons. These will appear in buildtools-systempython and buildtools-python . Versions used: * Byacc 20220128 * Flex 2.6.4 * git 2.36.0 (2022a uses 2.36, but upgraded to 2.36.3) * GNU Autoconf 2.71 * GNU Automake 1.16.2 * GNU Autoconf-archive 2022.09.03 * GNU Bison 3.8.2 * GNU libtool 2.4.6 * GNU M4 1.4.19 * GNU make 4.3. Switched back to .tar.gz as there was no .tar.bz2 file anymore * GNU sed 4.8 * CMake 3.23.4 (2022a still uses 3.23.1 but we went for the latest bugfix release at the time of development) * Ninja 1.10.2 (version used in 2022a, not the newest at time of development) * NASM 2.15.05 * Yasm 1.3.0 * patchelf 0.15.0 * pkg-config 0.29.2 * GNU gperf 3.1 * GNU help2man 1.49.2 * Doxygen 1.9.5 (2022a uses 1.9.4, but we went for the latest bugfix release at the time of development) * re2c 2.2 (Version used in 2022a, not clear why the community did not upgrade to 3.0 which was out before the development of 2022a even started)","title":"2022a"},{"location":"generated/easyconfigs/b/buildtools-systempython/","text":"buildtools-python instructions This module augments buildtools with Python-based buildtools. In this module, they use the system-installed Python 3. Since the packages need PYTHONPATH to be set, they can conflict with software that uses EasyBuild-installed more recent Python versions which is why they have been separated in a separate module. Contents The contents of the module evolved over time. It does contain a subset of: * Meson * version check on PyPi * Meson in the EasyBuilders repository * SCons * Version check on PyPi * SCons in the EasyBuilders repository EasyBuild This module is a UAntwerpen development. Version 2022a New EasyConfig, UAntwerpen-design, derived from the older buildtools versions that contained support for Meson and SCons. Versions used: - Meson: 0.61.5 as that is the last version supporting Python 3.6. - SCons: 4.0.1 as later versions gave problems, likely with the distutils package on the system.","title":"buildtools-python instructions"},{"location":"generated/easyconfigs/b/buildtools-systempython/#buildtools-python-instructions","text":"This module augments buildtools with Python-based buildtools. In this module, they use the system-installed Python 3. Since the packages need PYTHONPATH to be set, they can conflict with software that uses EasyBuild-installed more recent Python versions which is why they have been separated in a separate module.","title":"buildtools-python instructions"},{"location":"generated/easyconfigs/b/buildtools-systempython/#contents","text":"The contents of the module evolved over time. It does contain a subset of: * Meson * version check on PyPi * Meson in the EasyBuilders repository * SCons * Version check on PyPi * SCons in the EasyBuilders repository","title":"Contents"},{"location":"generated/easyconfigs/b/buildtools-systempython/#easybuild","text":"This module is a UAntwerpen development.","title":"EasyBuild"},{"location":"generated/easyconfigs/b/buildtools-systempython/#version-2022a","text":"New EasyConfig, UAntwerpen-design, derived from the older buildtools versions that contained support for Meson and SCons. Versions used: - Meson: 0.61.5 as that is the last version supporting Python 3.6. - SCons: 4.0.1 as later versions gave problems, likely with the distutils package on the system.","title":"Version 2022a"},{"location":"generated/easyconfigs/c/CD-HIT/","text":"CD-HIT instructions CD-HIT home page CD-HTI documentation site CD-HIT on GitHub Releases General information CD-HIT has no configure procedure. There is a make install in more recent versions, but that does not install the documentation. CD-HIT supports OpenMP parallelism but no distributed computing. CD-HIT consists of a number of binaries in the bin subdirectory and a lot of Perl scripts, but no libraries. EasyBuild There is support for CD-HIT in the EasyBuilders repository . We did make a couple of changes though to embed into our BioTools module. CD-HIT 4.8.1 for Intel 2020a BioTools Moved the editing of the path to the Perl binary in the Perl scripts to prebuildopts . This is a hack, but we don't really like messing around with postinstallcmds in a Bundle as would be the case when integrating into BioTools. Moved the content of doc to share/doc/CD-HIT. Put the README file and the license.txt file also in share/doc/CD-HIT. Note that adding 'openmp=yes' to the make command line is not needed as we include the flag to compile for OpenMP in CCFLAGS. We add it by hand as in the Bundle openmp will not be set to True . Even though there is a install target in the Makefile of version 4.8.1, we still use the MakeCp EasyBlock it avoid having to use postinstallcmds or a clumsy setup of installopts to also copy the documentation. Corrected two problems with the sed command to change the shebang. The replace string was simply wrong as it omitted the #! . Some files call perl with the -w flag and somehow that caused problems when using #!/usr/bin/env perl -w . We simply omit the -w flag as it is just to warn for errors in the code and hence more for code developers, and replaced by other mechanisms in modern Perl anyway.","title":"CD-HIT instructions"},{"location":"generated/easyconfigs/c/CD-HIT/#cd-hit-instructions","text":"CD-HIT home page CD-HTI documentation site CD-HIT on GitHub Releases","title":"CD-HIT instructions"},{"location":"generated/easyconfigs/c/CD-HIT/#general-information","text":"CD-HIT has no configure procedure. There is a make install in more recent versions, but that does not install the documentation. CD-HIT supports OpenMP parallelism but no distributed computing. CD-HIT consists of a number of binaries in the bin subdirectory and a lot of Perl scripts, but no libraries.","title":"General information"},{"location":"generated/easyconfigs/c/CD-HIT/#easybuild","text":"There is support for CD-HIT in the EasyBuilders repository . We did make a couple of changes though to embed into our BioTools module.","title":"EasyBuild"},{"location":"generated/easyconfigs/c/CD-HIT/#cd-hit-481-for-intel-2020a-biotools","text":"Moved the editing of the path to the Perl binary in the Perl scripts to prebuildopts . This is a hack, but we don't really like messing around with postinstallcmds in a Bundle as would be the case when integrating into BioTools. Moved the content of doc to share/doc/CD-HIT. Put the README file and the license.txt file also in share/doc/CD-HIT. Note that adding 'openmp=yes' to the make command line is not needed as we include the flag to compile for OpenMP in CCFLAGS. We add it by hand as in the Bundle openmp will not be set to True . Even though there is a install target in the Makefile of version 4.8.1, we still use the MakeCp EasyBlock it avoid having to use postinstallcmds or a clumsy setup of installopts to also copy the documentation. Corrected two problems with the sed command to change the shebang. The replace string was simply wrong as it omitted the #! . Some files call perl with the -w flag and somehow that caused problems when using #!/usr/bin/env perl -w . We simply omit the -w flag as it is just to warn for errors in the code and hence more for code developers, and replaced by other mechanisms in modern Perl anyway.","title":"CD-HIT 4.8.1 for Intel 2020a BioTools"},{"location":"generated/easyconfigs/c/CP2K/","text":"CP2K installation instructions CP2K web site How to compile the CP2K code CP2K GitHub CP2K compiler support: Page on CP2K.org CP2K dashboard CP2K Google forum General installation instructions Part of the procedure is based on contact with one of the CP2K authors. CP2K uses Libint * Versions up to 6.1 only support the currently deprecated Libint 1.1.x interfaces * Versions from 7.1 onwards support Libint 2.x * Libint should be statically linked * max-am (angular momentum) values: libint will generate code to calculate potentials up to a given angular momentum value. You pass the maximum angular momentum values during libint's configure phase, e.g. ./configure --with-libint-max-am=5 --with-libderiv-max-am1=4 ... CP2K cannot detect the values used, you have to set the correct compilation options yourself, using the values from libint's configure, and adding 1, e.g., -D__LIBINT_MAX_AM=6 -D__LIBDERIV_MAX_AM1=5 CP2K comes with a default library of potentials and wavefunction basis sets (data directory in the source archive). The location of this library is compiled into the code using the define -D__DATA_DIR=.../data (by default, the location in the source tree at compile-time is used), and can be overridden at runtime using the environment variable CP2K_DATA_DIR . If no data dir is found, CP2K fails. The configuration of CP2K is based on editing a suitable architecture script that is included by the main Makefile. They are stored in the arch subdirectory and passed to the code through the ARCH variable given when calling make . Be careful calling a version \"hybrid\" as a CP2K option will not think of a hybrid MPI/OpenMP executable but \"hybrid functionals\". So avoid confusion! CP2K contains a script to install a number of its dependencies ( tools/toolchain/install_cp2k_toolchain.sh ). The actual installation is done via scripts for individual packages stored in tools/toolchain/scripts ( link to GitHub master branch ). CP2K EasyConfigs There is an EasyBlock for CP2K. Since we've experienced problems with it, we use the MekeCp generic EasyBlock instead as this allows full control over the installation. Version 7.0 commit 44b70af5 (pre-release of version 7.1) GitHub Scripts for dependencies This commit still uses Libint 1.x The EasyConfig uses the versions of Libint, libxc, PLUMED, libxsmm and ELPA that would be downloaded by the code (the \"toolchain\" script that can download and install those dependencies). The EasyConfig with PLUMED support (version 2.4.1) only includes the popt and psmp executables as PLUMED requires the MPI symbols. On our systems it passed all tests (using update 3 of the Intel 2018 compilers) except tests/QS/regtest-rel/Hg_rel.inp which fails for the psmp executable on Ivybridge CPUs. The EasyConfig without PLUMED support includes the sopt, ssmp, popt and psmp executables. It passes all tests except tests/QMMM/QS/regtest-4/acn-conn-1.inp which fails for the ssmp executable. Suggested dependencies: Package Version suggested Version used ELPA 2017.05.003 2017.05.003 Libint 1.1.6 1.1.6 libxc 4.3.4 4.3.4 libxsmm 1.10.0 1.10.0 PLUMED No suggestion 2.4.1 GSL 2.5 2.5 ### Version 7.0 commit 31ce44db (pre-release of version 7.1) * [GitHub](https://github.com/cp2k/cp2k/tree/44b70af5aa9628cb07451649d52548a7bc5d8de1) * [Scripts for dependencies](https://github.com/cp2k/cp2k/tree/44b70af5aa9628cb07451649d52548a7bc5d8de1/tools/toolchain/scripts) * This commit already uses Libint 2.x. Note that we use a version of Libint specifically packaged and configured for CP2K. * Only a version without PLUMED was tested. * There was a patch needed to hardcode the revision in the Makefile as automatic detection does not work when used with EasyBuild. EasyBuild does not store the ``.git`` directory which is used to recover that information in this commit. Note that this changed again in later revisions. Suggested dependencies: Package Version suggested Version used ELPA 2019.05.001 2019.05.002 Libint 2.6.0 2.6.0 libxc 4.3.4 4.3.4 libxsmm 1.14 1.14 ### Version 7.1 * [GitHub](https://github.com/cp2k/cp2k/tree/support/v7.1) * [Scripts for dependencies](https://github.com/cp2k/cp2k/tree/support/v7.1/tools/toolchain/scripts) * Only trivial changes were needed compared to the EasyConfigs for the prerelease versions except for support of PLUMED 2.5+. * The installation of PLUMED 2.5+ does no longer include the large list of object files for statically linking with PLUMED. We've tried to engineer what the ``install_plumed.sh`` adds to the linker options and made a new patch file. * Note that there are still various options that are not included, e.g., support for PEXSI, QUIP, spglib, SIRIUS and FPGA. It is also not always clear which extensions work with which versions of CP2K. Suggested dependencies: Package Version suggested Version used ELPA 2019.05.001 2019.05.002 Libint 2.6.0 2.6.0 libxc 4.3.4 4.3.4 libxsmm 1.14 1.14 in 2019b, 1.15 in 2020a PLUMED 2.5.2 2.5.4 GSL 2.5 2.5 We tested PLUMED 2.6.0 also (with similar test results as 2.5.4) but sticked to the 2.5 branch which is the one that is officially supported by CP2K version 7.1. We also tested compiling with the older version 2.4.7 of PLUMED, which still worked in the old way with statically linking to a list of object files (different patch included).","title":"CP2K installation instructions"},{"location":"generated/easyconfigs/c/CP2K/#cp2k-installation-instructions","text":"CP2K web site How to compile the CP2K code CP2K GitHub CP2K compiler support: Page on CP2K.org CP2K dashboard CP2K Google forum","title":"CP2K installation instructions"},{"location":"generated/easyconfigs/c/CP2K/#general-installation-instructions","text":"Part of the procedure is based on contact with one of the CP2K authors. CP2K uses Libint * Versions up to 6.1 only support the currently deprecated Libint 1.1.x interfaces * Versions from 7.1 onwards support Libint 2.x * Libint should be statically linked * max-am (angular momentum) values: libint will generate code to calculate potentials up to a given angular momentum value. You pass the maximum angular momentum values during libint's configure phase, e.g. ./configure --with-libint-max-am=5 --with-libderiv-max-am1=4 ... CP2K cannot detect the values used, you have to set the correct compilation options yourself, using the values from libint's configure, and adding 1, e.g., -D__LIBINT_MAX_AM=6 -D__LIBDERIV_MAX_AM1=5 CP2K comes with a default library of potentials and wavefunction basis sets (data directory in the source archive). The location of this library is compiled into the code using the define -D__DATA_DIR=.../data (by default, the location in the source tree at compile-time is used), and can be overridden at runtime using the environment variable CP2K_DATA_DIR . If no data dir is found, CP2K fails. The configuration of CP2K is based on editing a suitable architecture script that is included by the main Makefile. They are stored in the arch subdirectory and passed to the code through the ARCH variable given when calling make . Be careful calling a version \"hybrid\" as a CP2K option will not think of a hybrid MPI/OpenMP executable but \"hybrid functionals\". So avoid confusion! CP2K contains a script to install a number of its dependencies ( tools/toolchain/install_cp2k_toolchain.sh ). The actual installation is done via scripts for individual packages stored in tools/toolchain/scripts ( link to GitHub master branch ).","title":"General installation instructions"},{"location":"generated/easyconfigs/c/CP2K/#cp2k-easyconfigs","text":"There is an EasyBlock for CP2K. Since we've experienced problems with it, we use the MekeCp generic EasyBlock instead as this allows full control over the installation.","title":"CP2K EasyConfigs"},{"location":"generated/easyconfigs/c/CP2K/#version-70-commit-44b70af5-pre-release-of-version-71","text":"GitHub Scripts for dependencies This commit still uses Libint 1.x The EasyConfig uses the versions of Libint, libxc, PLUMED, libxsmm and ELPA that would be downloaded by the code (the \"toolchain\" script that can download and install those dependencies). The EasyConfig with PLUMED support (version 2.4.1) only includes the popt and psmp executables as PLUMED requires the MPI symbols. On our systems it passed all tests (using update 3 of the Intel 2018 compilers) except tests/QS/regtest-rel/Hg_rel.inp which fails for the psmp executable on Ivybridge CPUs. The EasyConfig without PLUMED support includes the sopt, ssmp, popt and psmp executables. It passes all tests except tests/QMMM/QS/regtest-4/acn-conn-1.inp which fails for the ssmp executable. Suggested dependencies: Package Version suggested Version used ELPA 2017.05.003 2017.05.003 Libint 1.1.6 1.1.6 libxc 4.3.4 4.3.4 libxsmm 1.10.0 1.10.0 PLUMED No suggestion 2.4.1 GSL 2.5 2.5 ### Version 7.0 commit 31ce44db (pre-release of version 7.1) * [GitHub](https://github.com/cp2k/cp2k/tree/44b70af5aa9628cb07451649d52548a7bc5d8de1) * [Scripts for dependencies](https://github.com/cp2k/cp2k/tree/44b70af5aa9628cb07451649d52548a7bc5d8de1/tools/toolchain/scripts) * This commit already uses Libint 2.x. Note that we use a version of Libint specifically packaged and configured for CP2K. * Only a version without PLUMED was tested. * There was a patch needed to hardcode the revision in the Makefile as automatic detection does not work when used with EasyBuild. EasyBuild does not store the ``.git`` directory which is used to recover that information in this commit. Note that this changed again in later revisions. Suggested dependencies: Package Version suggested Version used ELPA 2019.05.001 2019.05.002 Libint 2.6.0 2.6.0 libxc 4.3.4 4.3.4 libxsmm 1.14 1.14 ### Version 7.1 * [GitHub](https://github.com/cp2k/cp2k/tree/support/v7.1) * [Scripts for dependencies](https://github.com/cp2k/cp2k/tree/support/v7.1/tools/toolchain/scripts) * Only trivial changes were needed compared to the EasyConfigs for the prerelease versions except for support of PLUMED 2.5+. * The installation of PLUMED 2.5+ does no longer include the large list of object files for statically linking with PLUMED. We've tried to engineer what the ``install_plumed.sh`` adds to the linker options and made a new patch file. * Note that there are still various options that are not included, e.g., support for PEXSI, QUIP, spglib, SIRIUS and FPGA. It is also not always clear which extensions work with which versions of CP2K. Suggested dependencies: Package Version suggested Version used ELPA 2019.05.001 2019.05.002 Libint 2.6.0 2.6.0 libxc 4.3.4 4.3.4 libxsmm 1.14 1.14 in 2019b, 1.15 in 2020a PLUMED 2.5.2 2.5.4 GSL 2.5 2.5 We tested PLUMED 2.6.0 also (with similar test results as 2.5.4) but sticked to the 2.5 branch which is the one that is officially supported by CP2K version 7.1. We also tested compiling with the older version 2.4.7 of PLUMED, which still worked in the old way with statically linking to a list of object files (different patch included).","title":"Version 7.0 commit 44b70af5 (pre-release of version 7.1)"},{"location":"generated/easyconfigs/c/ClonalFrameML/","text":"ClonalFrameML instructions ClonalFrameML on GitHub Releases General information ClonalFrameML consists of a single binary and a R script. No configure process for building the software. Some editing of the Makefile needed instead. EasyBuild There is (likely disappearing) support for ClonalFrameML in the EasyBuilders repository . (At the time of installing the 2020a toolchains the EasyConfigs had not been changed since the 2016b toolchains). Version 1.12 in the 2020a toolchains, preparation for inclusion in BioTools Started from the standard EasyBuilders EasyConfigs. Made our usual changes to the documentation. Changed the location of some installed files. There is an R script which we have put in bin for lack of better options. I guess it should have started with #!/usr/bin/env Rscript to run as an independent program. Put the README file in share/doc/ClonalFrameML","title":"ClonalFrameML instructions"},{"location":"generated/easyconfigs/c/ClonalFrameML/#clonalframeml-instructions","text":"ClonalFrameML on GitHub Releases","title":"ClonalFrameML instructions"},{"location":"generated/easyconfigs/c/ClonalFrameML/#general-information","text":"ClonalFrameML consists of a single binary and a R script. No configure process for building the software. Some editing of the Makefile needed instead.","title":"General information"},{"location":"generated/easyconfigs/c/ClonalFrameML/#easybuild","text":"There is (likely disappearing) support for ClonalFrameML in the EasyBuilders repository . (At the time of installing the 2020a toolchains the EasyConfigs had not been changed since the 2016b toolchains).","title":"EasyBuild"},{"location":"generated/easyconfigs/c/ClonalFrameML/#version-112-in-the-2020a-toolchains-preparation-for-inclusion-in-biotools","text":"Started from the standard EasyBuilders EasyConfigs. Made our usual changes to the documentation. Changed the location of some installed files. There is an R script which we have put in bin for lack of better options. I guess it should have started with #!/usr/bin/env Rscript to run as an independent program. Put the README file in share/doc/ClonalFrameML","title":"Version 1.12 in the 2020a toolchains, preparation for inclusion in BioTools"},{"location":"generated/easyconfigs/c/canu/","text":"Canu installation instructions Canu homepage on readthedocs Canu Git/hub Canu in Bioconda Instructions The instructions are extremely unclear when it comes to the dependencies. According to the Bioconda recipe, one needs * Perl with the Filesys-Df package * GNUplot * OpenJDK (or another Java JDK) * Boost also appears to be necessary. EasyConfigs Canu has support in EasyBuild (package canu). Our EasyConfigs were derived from the standard ones. Canu 1.9 in Intel/2019b Started from EasyConfigs for version 1.8 with Intel 2017b and 1.9 with GCCcore. Canu 2.0 in Intel/2020a Trivial adaptations of the 1.9 EasyConfig","title":"Canu installation instructions"},{"location":"generated/easyconfigs/c/canu/#canu-installation-instructions","text":"Canu homepage on readthedocs Canu Git/hub Canu in Bioconda","title":"Canu installation instructions"},{"location":"generated/easyconfigs/c/canu/#instructions","text":"The instructions are extremely unclear when it comes to the dependencies. According to the Bioconda recipe, one needs * Perl with the Filesys-Df package * GNUplot * OpenJDK (or another Java JDK) * Boost also appears to be necessary.","title":"Instructions"},{"location":"generated/easyconfigs/c/canu/#easyconfigs","text":"Canu has support in EasyBuild (package canu). Our EasyConfigs were derived from the standard ones.","title":"EasyConfigs"},{"location":"generated/easyconfigs/c/canu/#canu-19-in-intel2019b","text":"Started from EasyConfigs for version 1.8 with Intel 2017b and 1.9 with GCCcore.","title":"Canu 1.9 in Intel/2019b"},{"location":"generated/easyconfigs/c/canu/#canu-20-in-intel2020a","text":"Trivial adaptations of the 1.9 EasyConfig","title":"Canu 2.0 in Intel/2020a"},{"location":"generated/easyconfigs/d/DFTB%2B/","text":"DFTB+ installation instructions DFTB+ web site DFTB+ GitHub Dependencies The usual linear algebra stuff: BLAS/LAPACK/ScaLAPACK Python (at least 2.6) with NumPy is needed for the tests. Optional: ELSI library . Testing shows that ELSI should be compiled with support for PEXSI . Optional: ARPACK-ng library for excited state DFTB functionality Optional: DftD3 dispersion library (if you need this dispersion model) There are however a lot more dependencies if you look in the external subdirectory after asking the code to download the dependencies. GPU acceleration is possible General installation instructions The install procedure downloads optional dependencies, but ELSI and ARPACK-ng don't seem to be among them. There is no formal configure step. The code is configured by copying one of the example make.arch files from the sys subdirectory to make.arch in the main source directory and editing it. EasyConfigs The first EasyConfigs covered by this documentation are those for version 19.1. Version 19.1 (for Intel 2020a) Compiled our own ARPACK-ng module to include with DFTB+. This required some additional variables that should be defined when calling make, hence are put in buildopts . Editing of the make.arch file is done with sed in the prebuildopts . There are two EasyConfigs: One without MPI support that contains all subprograms One with MPI support where one subprogram is missing due to lack of MPI support. It also lacks the ELSI support. 2019b: Some tests in the MPI module failed when using Python 3.7.4. Hence we switched back to Python 2.7.16. 2020a: Used Python 3.8.2 for testing as we do no longer support Python 2.7. All tests now worked without problems. TODO Try to implement support for ELSI (introduced in 19.1?). There are a number of difficulties however: There might be a name conflict for the internal symbols. Depending on whether -standard-semantics or or -assume [no]std_mod_proc_name is used, the symbols for module functions are composed from the lowercase module name and function name using either MP or mp as separater between the two. The former is the better behaviour (as it avoids naming conflict with functions that may have mp in their name) while the latter is the default behaviour of the Intel compiler. I got link problems with ELSI als ELSI used the latter while the DFTB+ arch file used options that triggered the first. It actually expects an ELSI with PEXSI support compiled into it If you compile with ELSI support you have to make sure that ELSIINCDIR is defined in make.arch . Otherwise an empty -I argument is generated when compiling dftp+ which leads to problems in the MPI version as the compiler misinterprets the remaining -I arguments and can't find its module files. Similarly ELSIDIR should point to the directory containing the libraries to avoid an empty -L argument. Remaining problem: Difference in symbols in the ELSI library between what is in the library (names containing mp for the MPI versions) and what DFTB+ expects (names with MP ). It is not yet clear what causes this or who is at fault. Note that ELSI will only be fully enabled when compiling with MPI support.","title":"DFTB+ installation instructions"},{"location":"generated/easyconfigs/d/DFTB%2B/#dftb-installation-instructions","text":"DFTB+ web site DFTB+ GitHub Dependencies The usual linear algebra stuff: BLAS/LAPACK/ScaLAPACK Python (at least 2.6) with NumPy is needed for the tests. Optional: ELSI library . Testing shows that ELSI should be compiled with support for PEXSI . Optional: ARPACK-ng library for excited state DFTB functionality Optional: DftD3 dispersion library (if you need this dispersion model) There are however a lot more dependencies if you look in the external subdirectory after asking the code to download the dependencies. GPU acceleration is possible","title":"DFTB+ installation instructions"},{"location":"generated/easyconfigs/d/DFTB%2B/#general-installation-instructions","text":"The install procedure downloads optional dependencies, but ELSI and ARPACK-ng don't seem to be among them. There is no formal configure step. The code is configured by copying one of the example make.arch files from the sys subdirectory to make.arch in the main source directory and editing it.","title":"General installation instructions"},{"location":"generated/easyconfigs/d/DFTB%2B/#easyconfigs","text":"The first EasyConfigs covered by this documentation are those for version 19.1.","title":"EasyConfigs"},{"location":"generated/easyconfigs/d/DFTB%2B/#version-191-for-intel-2020a","text":"Compiled our own ARPACK-ng module to include with DFTB+. This required some additional variables that should be defined when calling make, hence are put in buildopts . Editing of the make.arch file is done with sed in the prebuildopts . There are two EasyConfigs: One without MPI support that contains all subprograms One with MPI support where one subprogram is missing due to lack of MPI support. It also lacks the ELSI support. 2019b: Some tests in the MPI module failed when using Python 3.7.4. Hence we switched back to Python 2.7.16. 2020a: Used Python 3.8.2 for testing as we do no longer support Python 2.7. All tests now worked without problems.","title":"Version 19.1 (for Intel 2020a)"},{"location":"generated/easyconfigs/d/DFTB%2B/#todo","text":"Try to implement support for ELSI (introduced in 19.1?). There are a number of difficulties however: There might be a name conflict for the internal symbols. Depending on whether -standard-semantics or or -assume [no]std_mod_proc_name is used, the symbols for module functions are composed from the lowercase module name and function name using either MP or mp as separater between the two. The former is the better behaviour (as it avoids naming conflict with functions that may have mp in their name) while the latter is the default behaviour of the Intel compiler. I got link problems with ELSI als ELSI used the latter while the DFTB+ arch file used options that triggered the first. It actually expects an ELSI with PEXSI support compiled into it If you compile with ELSI support you have to make sure that ELSIINCDIR is defined in make.arch . Otherwise an empty -I argument is generated when compiling dftp+ which leads to problems in the MPI version as the compiler misinterprets the remaining -I arguments and can't find its module files. Similarly ELSIDIR should point to the directory containing the libraries to avoid an empty -L argument. Remaining problem: Difference in symbols in the ELSI library between what is in the library (names containing mp for the MPI versions) and what DFTB+ expects (names with MP ). It is not yet clear what causes this or who is at fault. Note that ELSI will only be fully enabled when compiling with MPI support.","title":"TODO"},{"location":"generated/easyconfigs/d/DSSP/","text":"DSSP instructions DSSP is a program that can be called from GROMACS but is not integrated in GROMACS in any other way. DSSP web site There is an old download page on a related server at that institution, but those versions are not up-to-date anymore. DSSP GitHub GitHub releases Note that we use the GitHub as source for the releases. General instructions DSSP has moved to a ConfigureMake process. For use with GROMACS, the environment variable DSSP needs to point to the executable (Google for GROMACS 2020 DSSP to find the relevant information in the GROMACS manual). It looks like GROMACS 2020 (the current version when writing this text) still requires the DSSP 2.x branch. Dependencies Boost (optional) Claims to use libz, but no reference found in the code or link line though configure checks for it. Claims to use libbz2, but no reference found in the code or link line though configure checks for it. EasyConfigs There is no support for DSSP in the EasyBuilders repository. Version 2.2.1 for Intel 2017a Old EasyConfig, from before switch to autoconf, with a Makefile patch to make it compatible with EasyBuild. The patch could have been better in retrospect. Version 2.3.0 for Intel 2020a Still installed because it seems that even GORMACS 2020 still needs the version 2.x DSSP syntax. There is now a full configure process which we use in the EasyConfig. The configure script does not correctly install the man page, so we copy it in postinstallcmds. Version 3.1.4 for Intel 2020a Ported from the 2.3.0 EasyConfig. The same problems are present here. In fact, 2.3.0 was released later than 3.1.4 and it is likely the whole configure process was simply backported to that version.","title":"DSSP instructions"},{"location":"generated/easyconfigs/d/DSSP/#dssp-instructions","text":"DSSP is a program that can be called from GROMACS but is not integrated in GROMACS in any other way. DSSP web site There is an old download page on a related server at that institution, but those versions are not up-to-date anymore. DSSP GitHub GitHub releases Note that we use the GitHub as source for the releases.","title":"DSSP instructions"},{"location":"generated/easyconfigs/d/DSSP/#general-instructions","text":"DSSP has moved to a ConfigureMake process. For use with GROMACS, the environment variable DSSP needs to point to the executable (Google for GROMACS 2020 DSSP to find the relevant information in the GROMACS manual). It looks like GROMACS 2020 (the current version when writing this text) still requires the DSSP 2.x branch. Dependencies Boost (optional) Claims to use libz, but no reference found in the code or link line though configure checks for it. Claims to use libbz2, but no reference found in the code or link line though configure checks for it.","title":"General instructions"},{"location":"generated/easyconfigs/d/DSSP/#easyconfigs","text":"There is no support for DSSP in the EasyBuilders repository.","title":"EasyConfigs"},{"location":"generated/easyconfigs/d/DSSP/#version-221-for-intel-2017a","text":"Old EasyConfig, from before switch to autoconf, with a Makefile patch to make it compatible with EasyBuild. The patch could have been better in retrospect.","title":"Version 2.2.1 for Intel 2017a"},{"location":"generated/easyconfigs/d/DSSP/#version-230-for-intel-2020a","text":"Still installed because it seems that even GORMACS 2020 still needs the version 2.x DSSP syntax. There is now a full configure process which we use in the EasyConfig. The configure script does not correctly install the man page, so we copy it in postinstallcmds.","title":"Version 2.3.0 for Intel 2020a"},{"location":"generated/easyconfigs/d/DSSP/#version-314-for-intel-2020a","text":"Ported from the 2.3.0 EasyConfig. The same problems are present here. In fact, 2.3.0 was released later than 3.1.4 and it is likely the whole configure process was simply backported to that version.","title":"Version 3.1.4 for Intel 2020a"},{"location":"generated/easyconfigs/d/double-conversion/","text":"double-conversion instructions double-conversion on GitHub Releases on Git/hub General information Version 3.1: There are three build procedures SCons which is advised according to the documentation in the README.md This build three libraries: * A static library with regular position-dependent code * A static library with position-independent code * A shared library CMake: Can build only one library at a time. Makefile: It simply calls scons according to the documentation. EasyBuild There is EasyBuild support for double-conversion In EasyBuild 4.2, the current version when this documentation was started, it is based on the CMake installation procedure, making three runs to install the three versions of the library that SCons generates. 3.1.5, 2020a toolchains Made a CMake-based EasyConfig using only cosmetic changes to the standard EasyBuilders recipe. Tried to make a SCons-based version. It however does not fully work as the include files are not copied to the install directories. This can be fixed in a postinstallcmds. And there is also a problem with specifying the install directory. The EasyBuild SCons EasyBlock does this by specifying PREFIX but the scons recipe for double-conversion expects prefix instead. This can be fixed through installopts . It turns out that the size of the library files is very different in both cases. So the procedures are still far from equivalent...","title":"double-conversion instructions"},{"location":"generated/easyconfigs/d/double-conversion/#double-conversion-instructions","text":"double-conversion on GitHub Releases on Git/hub","title":"double-conversion instructions"},{"location":"generated/easyconfigs/d/double-conversion/#general-information","text":"Version 3.1: There are three build procedures SCons which is advised according to the documentation in the README.md This build three libraries: * A static library with regular position-dependent code * A static library with position-independent code * A shared library CMake: Can build only one library at a time. Makefile: It simply calls scons according to the documentation.","title":"General information"},{"location":"generated/easyconfigs/d/double-conversion/#easybuild","text":"There is EasyBuild support for double-conversion In EasyBuild 4.2, the current version when this documentation was started, it is based on the CMake installation procedure, making three runs to install the three versions of the library that SCons generates.","title":"EasyBuild"},{"location":"generated/easyconfigs/d/double-conversion/#315-2020a-toolchains","text":"Made a CMake-based EasyConfig using only cosmetic changes to the standard EasyBuilders recipe. Tried to make a SCons-based version. It however does not fully work as the include files are not copied to the install directories. This can be fixed in a postinstallcmds. And there is also a problem with specifying the install directory. The EasyBuild SCons EasyBlock does this by specifying PREFIX but the scons recipe for double-conversion expects prefix instead. This can be fixed through installopts . It turns out that the size of the library files is very different in both cases. So the procedures are still far from equivalent...","title":"3.1.5, 2020a toolchains"},{"location":"generated/easyconfigs/e/ELPA/","text":"ELPA installation instructions ELPA home page ELPA GitLab hosted by MPG Installation instructions on the GitLab General information ELPA stands for Eigenvalue soLvers for Petascale Applications ELPA libraries can be compiled with or without OpenMP support. Both sets of libraries can be installed together as they have different names. ELPA can also be build without MPI support, and those libraries get \"onenode\" in their name. Hence MPI and non-MPI OpenMP and non-OpenMP versions can coexist in a single library directory. ELPA also contains optional GPU support through CUDA. ELPA contains kernels for various CPUs: there is a generic kernel, specialized kernels for SSE, AVX, AVX2, AVX512, and some support for SPARC64, BlueGene P and BlueGene Q. ELPA includes the script manual_cpp that is used during the build process and as of version 2020.05.001 still requires Python 2. Moreover, it calls the Python interpreter using a shebang that calls the python executable. TODOs It is not clear from the documentation which linear algebra libraries should be used for the OpenMP versions of the code: Should we use the multithread version or the singlethread version? EasyConfig First version covered by this documentation: ELPA-2019.11.001-intel-2019b.eb Rather then building an EasyBlock for ELPA that could ensure that all processor-specific kernels that make sense are enabled, we do this in the EasyConfig file based on the value of the VSC_ARCH_LOCAL environment variable. We use a minimal number of kernels. Since the libraries with and without OpenMP and with and without MPI have different names, we build all (by using an array of configopts in EasyBuild). We currently link with matching linear algebra libraries: multithreaded ones for the multithreaded ELPA libraries. A user can always set MKL_NUM_THREADS to change the number of threads MKL uses independent of OMP_NUM_THREADS should it be better to use single threaded linear algebra. Other options that were enabled: Single precision support ( `--enable-single-precision ) Support for skew-symmetric matrices ( --enable-skew-symmetric-support ) There is some symbolic linking in the postinstallcmds to ensure that we also have a version-independent path within the installation directory to the include files and modules. To work with our CentOS 8 systems where we default to Python 3, we have to sed manual_cpp to use the python2 -command.","title":"ELPA installation instructions"},{"location":"generated/easyconfigs/e/ELPA/#elpa-installation-instructions","text":"ELPA home page ELPA GitLab hosted by MPG Installation instructions on the GitLab","title":"ELPA installation instructions"},{"location":"generated/easyconfigs/e/ELPA/#general-information","text":"ELPA stands for Eigenvalue soLvers for Petascale Applications ELPA libraries can be compiled with or without OpenMP support. Both sets of libraries can be installed together as they have different names. ELPA can also be build without MPI support, and those libraries get \"onenode\" in their name. Hence MPI and non-MPI OpenMP and non-OpenMP versions can coexist in a single library directory. ELPA also contains optional GPU support through CUDA. ELPA contains kernels for various CPUs: there is a generic kernel, specialized kernels for SSE, AVX, AVX2, AVX512, and some support for SPARC64, BlueGene P and BlueGene Q. ELPA includes the script manual_cpp that is used during the build process and as of version 2020.05.001 still requires Python 2. Moreover, it calls the Python interpreter using a shebang that calls the python executable.","title":"General information"},{"location":"generated/easyconfigs/e/ELPA/#todos","text":"It is not clear from the documentation which linear algebra libraries should be used for the OpenMP versions of the code: Should we use the multithread version or the singlethread version?","title":"TODOs"},{"location":"generated/easyconfigs/e/ELPA/#easyconfig","text":"First version covered by this documentation: ELPA-2019.11.001-intel-2019b.eb Rather then building an EasyBlock for ELPA that could ensure that all processor-specific kernels that make sense are enabled, we do this in the EasyConfig file based on the value of the VSC_ARCH_LOCAL environment variable. We use a minimal number of kernels. Since the libraries with and without OpenMP and with and without MPI have different names, we build all (by using an array of configopts in EasyBuild). We currently link with matching linear algebra libraries: multithreaded ones for the multithreaded ELPA libraries. A user can always set MKL_NUM_THREADS to change the number of threads MKL uses independent of OMP_NUM_THREADS should it be better to use single threaded linear algebra. Other options that were enabled: Single precision support ( `--enable-single-precision ) Support for skew-symmetric matrices ( --enable-skew-symmetric-support ) There is some symbolic linking in the postinstallcmds to ensure that we also have a version-independent path within the installation directory to the include files and modules. To work with our CentOS 8 systems where we default to Python 3, we have to sed manual_cpp to use the python2 -command.","title":"EasyConfig"},{"location":"generated/easyconfigs/e/ELSI/","text":"ELSI installation instructions ELSI provides and enhances scalable, open-source software library solutions for electronic structure calculations in materials science, condensed matter physics, chemistry, and many other fields. ELSI focuses on methods that solve or circumvent eigenvalue problems in electronic structure theory. The ELSI infrastructure should also be useful for other challenging eigenvalue problems. [Electronic Structure Library website]https://wordpress.elsi-interchange.org/) Electronic Structure Library github with EasyConfigs Dependencies ELSI interfaces to several eigensolvers and linear algebra packages. * ELPA - included with the distribution of ELSI, but can use an external one. * LibOMM - included with the distribution of ELSI, but can use an external one * PEXSI - included with the distribution of ELSI * EigenExa - should be included with the distribution of ELSI? * SLEPc-SIPs , based on PETSc - included with the distribution of ELSE * NTPoly - included with the distribution of ELSI, but can use an external one * BSEPACK - included with the distribution of ELSI, but can use an external one. BSEPACK development seems dead since 2015. * LAPACK * MAGMA linear algebra library for GPUs * SuperLU_dist - included with the distribution of ELSI for PEXSI, but should use an external one if an external PEXI is used * PT-SCOTCH - included with the distribution of ELSI for SuperLU, but should use an external one if an external PEXI is used. EasyConfigs ELSI-2.5.0-intel-2019b Library Version ELPA 2019.11.001 LibOMM Internal PEXSI Internal EigenExa Not included SLEPc-SIPs Not included NTPoly 2.4 BSEPACK Internal BLAS/LAPACK/ScaLAPACK Intel toolchain MAGMA Not included SuperLU_dist Internal PT-SCOTCH Internal Compiling with EigenExa resulted in compilation error messages. Compiling with SIPS support resulted in compilation error messages. Despite some versions of the documentation claiming otherwise, there is no switch USE_EXTERNAL_SUPERLU . Using an external one only makes sense anyway if PEXSI is also external (as this is the part of the code that needs SuperLU). Checking the CMakeLists.txt file learns that if PEXSI is external, SuperLU_DIST and SCOTCH are also assumed to be external. We currently use the internal PEXSI as it takes a lot of time to install an external one. PEXSI uses symPACK which relies on the Berkeley UPC++ 1.0 library . The links mentioned on the PEXSI site are dead, but the above ones are likely correct.","title":"ELSI installation instructions"},{"location":"generated/easyconfigs/e/ELSI/#elsi-installation-instructions","text":"ELSI provides and enhances scalable, open-source software library solutions for electronic structure calculations in materials science, condensed matter physics, chemistry, and many other fields. ELSI focuses on methods that solve or circumvent eigenvalue problems in electronic structure theory. The ELSI infrastructure should also be useful for other challenging eigenvalue problems. [Electronic Structure Library website]https://wordpress.elsi-interchange.org/) Electronic Structure Library github with EasyConfigs","title":"ELSI installation instructions"},{"location":"generated/easyconfigs/e/ELSI/#dependencies","text":"ELSI interfaces to several eigensolvers and linear algebra packages. * ELPA - included with the distribution of ELSI, but can use an external one. * LibOMM - included with the distribution of ELSI, but can use an external one * PEXSI - included with the distribution of ELSI * EigenExa - should be included with the distribution of ELSI? * SLEPc-SIPs , based on PETSc - included with the distribution of ELSE * NTPoly - included with the distribution of ELSI, but can use an external one * BSEPACK - included with the distribution of ELSI, but can use an external one. BSEPACK development seems dead since 2015. * LAPACK * MAGMA linear algebra library for GPUs * SuperLU_dist - included with the distribution of ELSI for PEXSI, but should use an external one if an external PEXI is used * PT-SCOTCH - included with the distribution of ELSI for SuperLU, but should use an external one if an external PEXI is used.","title":"Dependencies"},{"location":"generated/easyconfigs/e/ELSI/#easyconfigs","text":"","title":"EasyConfigs"},{"location":"generated/easyconfigs/e/ELSI/#elsi-250-intel-2019b","text":"Library Version ELPA 2019.11.001 LibOMM Internal PEXSI Internal EigenExa Not included SLEPc-SIPs Not included NTPoly 2.4 BSEPACK Internal BLAS/LAPACK/ScaLAPACK Intel toolchain MAGMA Not included SuperLU_dist Internal PT-SCOTCH Internal Compiling with EigenExa resulted in compilation error messages. Compiling with SIPS support resulted in compilation error messages. Despite some versions of the documentation claiming otherwise, there is no switch USE_EXTERNAL_SUPERLU . Using an external one only makes sense anyway if PEXSI is also external (as this is the part of the code that needs SuperLU). Checking the CMakeLists.txt file learns that if PEXSI is external, SuperLU_DIST and SCOTCH are also assumed to be external. We currently use the internal PEXSI as it takes a lot of time to install an external one. PEXSI uses symPACK which relies on the Berkeley UPC++ 1.0 library . The links mentioned on the PEXSI site are dead, but the above ones are likely correct.","title":"ELSI-2.5.0-intel-2019b"},{"location":"generated/easyconfigs/e/Eigen/","text":"Eigen instructions Eigen weg site Eigen on GitLAb Releases General information Eigen is just a template library for C++. Hence the installation is completelhy compiler-neutral. Eigen moved to GitLab at the end of 2019, so old EasyConfig files will no longer succeed in downloading the files from BitBucket. The move was made because BitBucket dit no longer support mercurial, and if the had to switch to git, they decided they could as well switch to another provider. According to the installation instructions One can simply copy the Eigen subdirectory to any desired location. There is also a CMake installation process. It does install some additional files such as support for pkgconfig and for CMake. That CMake process can produce a lot of warnings, but they are really only relevant to those installing Eigen during the compilation process for an actual application, and not to simply do a source install of Eigen. EasyBuild There is support for Eigen in the EasyBuilders repository . The procedure might be more complicated then needed as a custom EasyBlock is used which given the simplicity of the installation process seems overkill. Version 3.3.7 for the 2020a toolchains. Switched to downloading from GitLab. Switched to installing in the system toolchain as there is nothing compiler-specific about the whole package.","title":"Eigen instructions"},{"location":"generated/easyconfigs/e/Eigen/#eigen-instructions","text":"Eigen weg site Eigen on GitLAb Releases","title":"Eigen instructions"},{"location":"generated/easyconfigs/e/Eigen/#general-information","text":"Eigen is just a template library for C++. Hence the installation is completelhy compiler-neutral. Eigen moved to GitLab at the end of 2019, so old EasyConfig files will no longer succeed in downloading the files from BitBucket. The move was made because BitBucket dit no longer support mercurial, and if the had to switch to git, they decided they could as well switch to another provider. According to the installation instructions One can simply copy the Eigen subdirectory to any desired location. There is also a CMake installation process. It does install some additional files such as support for pkgconfig and for CMake. That CMake process can produce a lot of warnings, but they are really only relevant to those installing Eigen during the compilation process for an actual application, and not to simply do a source install of Eigen.","title":"General information"},{"location":"generated/easyconfigs/e/Eigen/#easybuild","text":"There is support for Eigen in the EasyBuilders repository . The procedure might be more complicated then needed as a custom EasyBlock is used which given the simplicity of the installation process seems overkill.","title":"EasyBuild"},{"location":"generated/easyconfigs/e/Eigen/#version-337-for-the-2020a-toolchains","text":"Switched to downloading from GitLab. Switched to installing in the system toolchain as there is nothing compiler-specific about the whole package.","title":"Version 3.3.7 for the 2020a toolchains."},{"location":"generated/easyconfigs/e/Elk/","text":"Elk installation instructions Elk web site General instructions Elk has a rather primitive build system. There is no configure whatsoever, instead it has a setup script that generates a make.inc file. There is also no install procedure. EasyConfigs First version covered by this documentation: 5.2.14. Version 5.2.14 for Intel 2019b and 2020a The EasyConfig file is derived from some very old ones. It is based on the MakeCp generic EasyBlock There is no config step In the build step we first generate a make.inc file with those options that are not passed through the environment. In the install step we then copy the generated executables. Version 6.3.2 for Intel 2020b There are a number of extra variables that should be initialized in make.inc. Otherwise link errors occur. Stubs for OpenBLAS and BLIS should be included. New in this version (actually one slightly before) is support for Wannier90, which we have also integrated in the recipe.","title":"Elk installation instructions"},{"location":"generated/easyconfigs/e/Elk/#elk-installation-instructions","text":"Elk web site","title":"Elk installation instructions"},{"location":"generated/easyconfigs/e/Elk/#general-instructions","text":"Elk has a rather primitive build system. There is no configure whatsoever, instead it has a setup script that generates a make.inc file. There is also no install procedure.","title":"General instructions"},{"location":"generated/easyconfigs/e/Elk/#easyconfigs","text":"First version covered by this documentation: 5.2.14.","title":"EasyConfigs"},{"location":"generated/easyconfigs/e/Elk/#version-5214-for-intel-2019b-and-2020a","text":"The EasyConfig file is derived from some very old ones. It is based on the MakeCp generic EasyBlock There is no config step In the build step we first generate a make.inc file with those options that are not passed through the environment. In the install step we then copy the generated executables.","title":"Version 5.2.14 for Intel 2019b and 2020a"},{"location":"generated/easyconfigs/e/Elk/#version-632-for-intel-2020b","text":"There are a number of extra variables that should be initialized in make.inc. Otherwise link errors occur. Stubs for OpenBLAS and BLIS should be included. New in this version (actually one slightly before) is support for Wannier90, which we have also integrated in the recipe.","title":"Version 6.3.2 for Intel 2020b"},{"location":"generated/easyconfigs/f/FLAC/","text":"FLAC instructions FLAC web site Release information FLAC on GitHub Releases General instructions Ogg is an optional but highly recommended dependency. XMMS is an optional dependency that we did not include given that it is of little use on a cluster. EasyConfig We developed our EasyConfig with the eye on inclusion in baselibs in the 2020a toolchain, but backported to 2019b as part of libsndfile and librosa for a user. We started our development from an old EasyBuilders EasyConfig that was included with EasyBuild 3.9.4 but not supported anymore in later versions. However, we did make the usual modifications and also some corrections. Added documentation in the module file. Added the dependency on libogg Made it explicit in configopts that we do not include XMMS support, thus also avoiding warnings about it during the run of configure .","title":"FLAC instructions"},{"location":"generated/easyconfigs/f/FLAC/#flac-instructions","text":"FLAC web site Release information FLAC on GitHub Releases","title":"FLAC instructions"},{"location":"generated/easyconfigs/f/FLAC/#general-instructions","text":"Ogg is an optional but highly recommended dependency. XMMS is an optional dependency that we did not include given that it is of little use on a cluster.","title":"General instructions"},{"location":"generated/easyconfigs/f/FLAC/#easyconfig","text":"We developed our EasyConfig with the eye on inclusion in baselibs in the 2020a toolchain, but backported to 2019b as part of libsndfile and librosa for a user. We started our development from an old EasyBuilders EasyConfig that was included with EasyBuild 3.9.4 but not supported anymore in later versions. However, we did make the usual modifications and also some corrections. Added documentation in the module file. Added the dependency on libogg Made it explicit in configopts that we do not include XMMS support, thus also avoiding warnings about it during the run of configure .","title":"EasyConfig"},{"location":"generated/easyconfigs/f/Flye/","text":"Flye installation instructions Flye on GitHub Flye in Bioconda General remarks Flye uses several other packages, including minimap2 and SAMtools. However, rather than using existing libraries on the system, it includes all code and there seems to be no option to build using already existing libraries. The downside is that there is no guarantee that these libraries will always be properly optimized. EasyConfigs Flye has support in EasyBuild so we simply adapt the default EasyBuild recipes for Flye. Version 2.7 As 2.7 was not yet supported in EasyBuild, we made trivial changes to a default 2.6 recipe. The build/install phase does produce an error message about a wheel not being generated yet the result appears to be OK. It certainly passes the import flye test. Version 2.7.1 - 2020a toolchains Moved to the BioTools-Python bundle The test file here uses some very special tricks to prepare to move to the BioTools-Python bundle. We want to install Flye as an extension in that Bundle. The problem is that Flye is downloaded from GitHub and does have a meaningless filename (essentially the version with an extension) so that file name conflicts could occur in the sources repository should one have another such package with the same version number. The solution is to not use PythonBundle, but Bundle as the main EasyBlock, and then Use a component that does nothing to download the sources and use a meaningful name for the file. For this, we use Tarball components and skip the install step. Set the class for extensions to \"PythonPackage\" Install Flye again in the extensions just as we would for any Python package But now we also need to manually adjust PYTHONPATH in the recipe.","title":"Flye installation instructions"},{"location":"generated/easyconfigs/f/Flye/#flye-installation-instructions","text":"Flye on GitHub Flye in Bioconda","title":"Flye installation instructions"},{"location":"generated/easyconfigs/f/Flye/#general-remarks","text":"Flye uses several other packages, including minimap2 and SAMtools. However, rather than using existing libraries on the system, it includes all code and there seems to be no option to build using already existing libraries. The downside is that there is no guarantee that these libraries will always be properly optimized.","title":"General remarks"},{"location":"generated/easyconfigs/f/Flye/#easyconfigs","text":"Flye has support in EasyBuild so we simply adapt the default EasyBuild recipes for Flye.","title":"EasyConfigs"},{"location":"generated/easyconfigs/f/Flye/#version-27","text":"As 2.7 was not yet supported in EasyBuild, we made trivial changes to a default 2.6 recipe. The build/install phase does produce an error message about a wheel not being generated yet the result appears to be OK. It certainly passes the import flye test.","title":"Version 2.7"},{"location":"generated/easyconfigs/f/Flye/#version-271-2020a-toolchains","text":"Moved to the BioTools-Python bundle The test file here uses some very special tricks to prepare to move to the BioTools-Python bundle. We want to install Flye as an extension in that Bundle. The problem is that Flye is downloaded from GitHub and does have a meaningless filename (essentially the version with an extension) so that file name conflicts could occur in the sources repository should one have another such package with the same version number. The solution is to not use PythonBundle, but Bundle as the main EasyBlock, and then Use a component that does nothing to download the sources and use a meaningful name for the file. For this, we use Tarball components and skip the install step. Set the class for extensions to \"PythonPackage\" Install Flye again in the extensions just as we would for any Python package But now we also need to manually adjust PYTHONPATH in the recipe.","title":"Version 2.7.1 - 2020a toolchains"},{"location":"generated/easyconfigs/f/fastp/","text":"fastp installation instructions fastp on GitHub General instructions As of version 0.20.0, fastp has no configure or CMake support. It is build using a simple Makefile with a build and a make install step. Build step: Picks up the compiler from the CXX environment variable Adds the flags specified through CXXFLAGS in the environment to the hard-coded values which luckily also work for Intel. The alternative is to define CXXFLAGS as one of the arguments of the Make command. In version 0.20.0, the default flags are -std=c++11 -g -O3 -I./inc . Install step: Define PREFIX to set the installation directory. It is necessary to also create the installation directory for binaries as it is not done automatically. Depends on zlib The build process produces a single executable and no libraries EasyConfigs There is support for fastp in EasyBuild. We did make a couple of changes though. Version 0.20.0 - Intel 2019b and 0.20.1 - Intel 2020a Added the missing dependency for zlib. In our case, this is the baselibs package. Removed CXX from buildopts as the Makefile picks up the correct value from the EasyBuild environment. Added CXXFLAGS to overwrite the internally set ones. This results in the behaviour which is most in line with what EasyBuild expects. Did specify the C++ standard in toolchainopts since the default compiler options imposed this. Moved to the BioTools bundle for the 2020a toolchains. Note that we added the -std=c++11 option by hand instead of through toolchainopts as in a Bundle we can currently not yet specify individual toolchainopts for a single component.","title":"fastp installation instructions"},{"location":"generated/easyconfigs/f/fastp/#fastp-installation-instructions","text":"fastp on GitHub","title":"fastp installation instructions"},{"location":"generated/easyconfigs/f/fastp/#general-instructions","text":"As of version 0.20.0, fastp has no configure or CMake support. It is build using a simple Makefile with a build and a make install step. Build step: Picks up the compiler from the CXX environment variable Adds the flags specified through CXXFLAGS in the environment to the hard-coded values which luckily also work for Intel. The alternative is to define CXXFLAGS as one of the arguments of the Make command. In version 0.20.0, the default flags are -std=c++11 -g -O3 -I./inc . Install step: Define PREFIX to set the installation directory. It is necessary to also create the installation directory for binaries as it is not done automatically. Depends on zlib The build process produces a single executable and no libraries","title":"General instructions"},{"location":"generated/easyconfigs/f/fastp/#easyconfigs","text":"There is support for fastp in EasyBuild. We did make a couple of changes though.","title":"EasyConfigs"},{"location":"generated/easyconfigs/f/fastp/#version-0200-intel-2019b-and-0201-intel-2020a","text":"Added the missing dependency for zlib. In our case, this is the baselibs package. Removed CXX from buildopts as the Makefile picks up the correct value from the EasyBuild environment. Added CXXFLAGS to overwrite the internally set ones. This results in the behaviour which is most in line with what EasyBuild expects. Did specify the C++ standard in toolchainopts since the default compiler options imposed this. Moved to the BioTools bundle for the 2020a toolchains. Note that we added the -std=c++11 option by hand instead of through toolchainopts as in a Bundle we can currently not yet specify individual toolchainopts for a single component.","title":"Version 0.20.0 - Intel 2019b and 0.20.1 - Intel 2020a"},{"location":"generated/easyconfigs/g/GATK/","text":"GATK 3 installation instructions GATK 3 is no longer supported by the Broad Institute. Information on GATK3 is archived on GitHub in broadgsa/gatk-protected . It also contains the GATK 3 documentation archive . The repository also contains the releases. The files that can be found there are the sources though and have a different content from the files that are used by the EasyConfigs for pre-4.0 versions. It is still possible to download the regular install files from https://software.broadinstitute.org/gatk/download/archive which will redirect to the Google Cloud. It is not clear to which extent the EasyConfig included in this directory is still OK. GATK 4 relies on several Python and R-packages; it is not clear if and to which extent these are also needed for GATK 3. Included EasyConfigs: * GATK-3.7-Java-8.eb: To install from a release file as could be downloaded from the Broad Institute web site before GATK 3 was archived. * GATK-3.8-1-Java-8.eb: To install from the files that can be found in the archive mentioned above. The \"resources\" directory is not present in those archives, only the jar-file is. GATK 4 installation instructions Most if not all EasyConfigs from the EasyBuilders site for GATK 4 are basically junk (recipes up to GATK-4.1.3.0-GCCcore-8.3.0-Java-1.8.eb were checked). They claim to do a full install yet do only a very partial install of GATK. The only Python script that is installed is the wrapper script gatk which will probably even run with most system Python binaries. The included Python packages are never installed. Hence the multi_deps line that is included in the file is basically useless. Its only function is that the module help information will contain a line telling the user to load a Python module. GATK 4.1 has a number of dependencies: * R is used by some tools in GATK for producing graphics. A list of R packages is included in the script instal_R_packages.R in the directory scripts/docker/gatkbase' of the [GATK GitHub repository](https://github.com/broadinstitute/gatk). That script is not included in the distribution file used by the EasyConfig files. It is not clear if that script also includes all dependencies of those packages, so it may still require some puzzling to translate that list into an EasyBuild recipe. * Several Python packages are used by various tools included in GATK, including machine learning packages such as Keras and TensorFlow . A list of packages can be found in the Conda configuration file gatkcondaenv.yml included in the distribution or in [the file gatkcondaenv.yml.template](https://github.com/broadinstitute/gatk/blob/master/scripts/gatkcondaenv.yml.template) in the subdirectory scripts` of the GATK GitHub repository . Minimal installation of GATK In the minimal installation we don't include any of the dependencies mentioned above, not installed in the software directory nor through including them in module dependencies. The EasyConfigs for the minimal installation were developed starting from a then recent recipe in the EasyBuilders repository. We did make a number of changes though: As the code is mostly Java and a Python script that probably runs with any recent system Python, we moved the package to the dummy/SYSTEM toolchain. The user can then still load a more appropriate Python from any toolchain without us having to create a different module for every toolchain. Based on the gatk script and the file Dockerfile in the root directory of the GitHub repository two additional variables were defined: GATK_LOCAL_JAR points to the gatk-package-<version>-local.jar file and ensures the gatk wrapper quickly finds the file The docker file defines GATK_JAR for that function. We cannot find that variable anywhere else, but just to be sure we do define it in the same way as GATK_LOCAL_VAR . As we cannot support Spark on the UAntwerp clusters, the file gatk-package-<version>-spark.jar is removed. The files in gatkPythonPackageArchive.zip are unzipped to the python subdirectory and we add this directory to PYTHONPATH . It is not clear at this time though if this is enough to get these scripts working. One may have to run the included setup scripts. Moreover, contrary to what the GATK web site claims, at least Python 3.4 is required. Older versions of Python may be good enough for the gatk wrapper script but do not work with the newer Python components of GATK 4. Included EasyConfigs: * GATK-4.1.4.1-Java-8-minimal.eb","title":"GATK 3 installation instructions"},{"location":"generated/easyconfigs/g/GATK/#gatk-3-installation-instructions","text":"GATK 3 is no longer supported by the Broad Institute. Information on GATK3 is archived on GitHub in broadgsa/gatk-protected . It also contains the GATK 3 documentation archive . The repository also contains the releases. The files that can be found there are the sources though and have a different content from the files that are used by the EasyConfigs for pre-4.0 versions. It is still possible to download the regular install files from https://software.broadinstitute.org/gatk/download/archive which will redirect to the Google Cloud. It is not clear to which extent the EasyConfig included in this directory is still OK. GATK 4 relies on several Python and R-packages; it is not clear if and to which extent these are also needed for GATK 3. Included EasyConfigs: * GATK-3.7-Java-8.eb: To install from a release file as could be downloaded from the Broad Institute web site before GATK 3 was archived. * GATK-3.8-1-Java-8.eb: To install from the files that can be found in the archive mentioned above. The \"resources\" directory is not present in those archives, only the jar-file is.","title":"GATK 3 installation instructions"},{"location":"generated/easyconfigs/g/GATK/#gatk-4-installation-instructions","text":"Most if not all EasyConfigs from the EasyBuilders site for GATK 4 are basically junk (recipes up to GATK-4.1.3.0-GCCcore-8.3.0-Java-1.8.eb were checked). They claim to do a full install yet do only a very partial install of GATK. The only Python script that is installed is the wrapper script gatk which will probably even run with most system Python binaries. The included Python packages are never installed. Hence the multi_deps line that is included in the file is basically useless. Its only function is that the module help information will contain a line telling the user to load a Python module. GATK 4.1 has a number of dependencies: * R is used by some tools in GATK for producing graphics. A list of R packages is included in the script instal_R_packages.R in the directory scripts/docker/gatkbase' of the [GATK GitHub repository](https://github.com/broadinstitute/gatk). That script is not included in the distribution file used by the EasyConfig files. It is not clear if that script also includes all dependencies of those packages, so it may still require some puzzling to translate that list into an EasyBuild recipe. * Several Python packages are used by various tools included in GATK, including machine learning packages such as Keras and TensorFlow . A list of packages can be found in the Conda configuration file gatkcondaenv.yml included in the distribution or in [the file gatkcondaenv.yml.template](https://github.com/broadinstitute/gatk/blob/master/scripts/gatkcondaenv.yml.template) in the subdirectory scripts` of the GATK GitHub repository .","title":"GATK 4 installation instructions"},{"location":"generated/easyconfigs/g/GATK/#minimal-installation-of-gatk","text":"In the minimal installation we don't include any of the dependencies mentioned above, not installed in the software directory nor through including them in module dependencies. The EasyConfigs for the minimal installation were developed starting from a then recent recipe in the EasyBuilders repository. We did make a number of changes though: As the code is mostly Java and a Python script that probably runs with any recent system Python, we moved the package to the dummy/SYSTEM toolchain. The user can then still load a more appropriate Python from any toolchain without us having to create a different module for every toolchain. Based on the gatk script and the file Dockerfile in the root directory of the GitHub repository two additional variables were defined: GATK_LOCAL_JAR points to the gatk-package-<version>-local.jar file and ensures the gatk wrapper quickly finds the file The docker file defines GATK_JAR for that function. We cannot find that variable anywhere else, but just to be sure we do define it in the same way as GATK_LOCAL_VAR . As we cannot support Spark on the UAntwerp clusters, the file gatk-package-<version>-spark.jar is removed. The files in gatkPythonPackageArchive.zip are unzipped to the python subdirectory and we add this directory to PYTHONPATH . It is not clear at this time though if this is enough to get these scripts working. One may have to run the included setup scripts. Moreover, contrary to what the GATK web site claims, at least Python 3.4 is required. Older versions of Python may be good enough for the gatk wrapper script but do not work with the newer Python components of GATK 4. Included EasyConfigs: * GATK-4.1.4.1-Java-8-minimal.eb","title":"Minimal installation of GATK"},{"location":"generated/easyconfigs/g/GPAW/","text":"GPAW installation notes GPAW is a Quantum MD code. GPAW web site , which also contains the installation instructions. Dependencies GPAW is Python code (3.5 or newer) but it also contains some C code for some performance-critical parts and to interface to a number of libraries on which it depends (for GPAW 19.8.1): BLAS LAPACK MPI BLACS and ScaLAPACK FFTW is highly recommended LibXC 3.X or 4.X. LibXC is a library of exchange-correlation functions for density-functional theory Optional dependency: libvdwxc , a portable C library of density functionals with van der Waals interactions for density functional theory. As of December 2019, this library is still unreleased. It is not clear from the installation documentation whether we need the MPI-aware version of libvdwxc or not, but further on in the documentation it claims that libvdwxc will automatically parallelise which implies the MPI-aware version is needed. Optional dependency: ELPA , which should improve performance for large systems when GPAW is used in LCAO mode GPAW depends on a number of standard Python packages. As of GPAW 19.8.1, these are: NumPY 1.9 or later SciPy 0.14 or later GPAW relies on ASE , a Python package from the same group Check the release notes of GPAW as the releases of ASE and GPAW should match. E.g., when starting this documentation, version 19.8.1 was the most up-to-date release of GPAW with 3.18.0 the matching ASE version. See further below for more instructions on how to determine the right versions for dependencies. ASE has some optional dependencies that are not needed for the benchmarking: Matplotlib (2.0.0 or newer), tkinter and Flask. BLAS, LAPACK, MPI, BLACS and ScaLAPACK are standard components of the Intel toolchain that we use for installing GPAW. See this repository for the FFTW and LibXC EasyConfig files as used at UAntwerp. NumPy and SciPy are standard components of the Python bundles in use at UAntwerp, see the EasyConfig files in this repository. ASE introduces additional dependencies on other Python packages: Matplotlib, tkinter and Flask. These are optional and only needed for some of the functionality. * Matplotlib is a standard component in the Python modules at UAntwerp, see the Python and IntelPython3-Packages EasyConfigs in this repository. * tkinter, the TK interface, is a standard component of recent Python distributions. * Flask needs to be installed. It brings with it a set of other dependencies: * Werkzeug * Jinja2 , and this one needs * MarkupSafe * itsdangerous * click : Included in recent standard Python packages at UAntwerp, but not in some of the older ones. GPAW, when installed automatically, rarely has the best configuration for HPC, so check the section on the internal organization of the compilation process on how to customize to get a proper HPC build. Determining version numbers of dependencies Python versions: On PyPi or towards the bottom of the file setup.py ASE version: In the release notes : preferred version, though other versions may also work. Line install_requires towards the bottom of the file setup.py may give a different restriction from the release notes. In the file doc/install.rst NumPy and SciPy: In the file doc/install.rst GPAW Python ASE NumPy SciPy LibXC 19.8.1 3.4-3.7 3.18.0, \u22653.18.0 \u22651.9 \u22650.14 3.x or 4.x 20.1.0 3.5-3.8 3.19.0, \u22653.18.0 \u22651.9 \u22650.14 3.x or 4.x Internal organization of the compilation process of GPAW GPAW is compiled through Python scripts to integrate in the standard Python package setup process. * Up to and including version 19.8.1: The scripts are fairly outdated though as they still rely on distutils rather than setuptools . * From version 20.1.0 on, setuptools is used instead of distutils easing installation with pip . As a result of this, there are some differences between the way versions up to 19.8.1 and versions from 20.1.0 are configured. There are also differences in the files generated that are not related to this switch. However, togehter with this transition, there was also an important change to how parallel computations are organized, leading to even more differences. In both cases the build process can be customized (and often has to be customized) through a customization script. Version 19.8.1 and likely below GPAW consists of a lot of Python code, a purely sequential shared library _gpaw.cpython-<python version>-<architecture>.so for some operations and for a parallel installation also the specialised parallel Python interpreter gpaw-python . There are two types of installation: 1. If mpicc is not found and no MPI compiler is specified in the customization script, of if the automatic detection of mpicc is undone in the customization script by setting mpicompiler to None , a purely sequential version of GPAW is installed, including only the Python code and the shared library. 1. If mpicc is found or another MPI compiler is specified in the customization script, a distributed memory version of GPAW is build, consisting of the Python code, the sequential shared library and the parallel Python interpreter gpaw-python . Only the latter links to the MPI libraries and to BLACS/ScaLAPACK (if used). Note that the manual of GPAW advises to use OMP_NUM_THREADS=1 which may influence other packages called from GPAW, but the default is to compile with single threaded libraries. The other commands in the bin directory (beside gpaw-python ) are all regular Python scripts, except for gpaw-runscript which is executed by gpaw-python . Note that this script is only an example and has not been adapted to our particular cluster! It is possible and needed to change the default configuration though by adapting customize.py or writing your own variant of this script and then overwriting the default by adding --customize=<name of file> to the argument list of python setup.py build and python setup.py install . At the point where customize.py is called, the NumPy include files have been located, libxc is added to the library list, and the configure process has done an effort to auto-locate the linear algebra libraries (at least BLAS and Lapack, see below). However, we experienced that the choice was often wrong and the resulting code refused to run. The MPI compiler and linker will be set to 'mpicc' if that one is found, or 'None' otherwise. The script config.py tries very hard to auto-determine a suitable BLAS/LAPACK library for various platforms that it recognizes (even ARM). However, it does so not by looking in LIBRARY_PATH or LD_LIBRARY_PATH but only checks default locations for installation of those libraries as part of the OS, except for MKL, where it does use the MKLROOT environment variable. Hence it fails to detect any library in modules that are installed elsewhere in the file system (except then for MKL if the module make correct use of MKLROOT ). It is important that this is corrected in a customization file. Note that on 64-bit Intel architecture if multiple supported BLAS libraries are found, the preferred order is: 1. ACML, in /opt/acml... 1. Intel MKL 1. OpenBLAS , in 'usr/lib', '/usr/local/lib', '/usr/lib64' 1. ATLAS , in 'usr/lib', '/usr/local/lib', '/usr/lib64/atlas' 1. libsatlas: Serial version of Atlas (recent versions of Atlas compile a serial and a threaded version, the latter called libtatlas.) Search is in the same directories as for libatlas. The distutils build_ext method is redefined in setup.py . It first calls the regular distutils build_ext method to build the shared library and prepare some Python scripts for the bin directory, and then calls the build_interpreter routing defined in config.py to build gpaw-python . Only a limited set of C files contain calls to the MPI libraries. The shared library contains no MPI calls. All C files that contain MPI calls include header files that define stub data types/routines/macros so that the sources can be compiled to a serial shared library without the use of a MPI compiler wrapper. When building 'gpaw-python' at a later stage, those files that do contain calls to MPI are recompiled using the MPI compiler wrapper, and all objects are then linked into the gpaw-python binary. For the shared library, compiler flags are used in the distutils way, taking them from the Python configuration and environment. This may lead to very misleading results. You may find customization scripts that specify a different compiler through the compiler variable, but that value is ignored if CC and `LDSHARED are defined in the environment. We consider this a bug, see below, as the code does not do what the comments in setup.py suggests. For linking gpaw-python , one may need to check the argument that is used to detect the correct option for adding runtime library search paths to the command line (if this is used in the customization script). The current code assumes the MPI compiler wrapper is called mpicc and checks if this calls gcc but doesn't recognize other compilers, so the option used may not be the right one. Bugs in 19.8.1 If CC and LDSHARED , the value of compiler customize.py script is ignored during the compilation process and CC respectively LDSHARED are used for the compilation and link steps. The hack to remove this in setup.py does not work. The code is simply incomplete as it does not push the variables back to the environment or fails to feed them in another way to the compilation process. setup.py suggests there is a flag --remove-default-flags which one can use to remove compiler flags imported from various environment variables (and/or Python configuration?), but the flag does not work at all. The hack that should actually remove those (located in the same block of code as the hack for the previous bug) does not work for the same reason that the hack to overrule setting CC or LDSHARED does not work. The code in build_interpreter in config.py to build the argument list for runtime library paths is suspicious. There is no way to set the right options by hand. There are cases where it may use the wrong option, e.g., -R rather than -Wl,-R . Version 20.1.0 and likely above As in earlier versions, GPAW consists of a lot of Python code, a shared library _gpaw.cpython-<python version>-<architecture>.so for some operations and optionally the binary gpaw-python , a specialized parallel Python operator. There is however a major difference for the preferred parallel build of GPAW. There are now three build types, the third one being undocumented in the documentation of version 20.1.0 and hence likely deprecated. 1. Sequential build: If mpicc is not found and no MPI compiler is specified in the customization script, of if the automatic detection of mpicc is undone in the customization script by setting mpicompiler to None , a purely sequential version of GPAW is installed, including only the Python code and the sequential shared library. 1. Prefered parallel build: If mpicc is found or another MPI compiler is specified in the customization script, a distributed memory version of GPAW is build, consisting of the Python code and a parallel shared library. Contrary to version 19.8.1, MPI and the optional BLACS/ScaLAPACK are now linked directly into the shared library. 1. Old-style parallel build: To enable this build type, set mpicompiler (or make sure mpicc is found) and set parallel_python_interpreter to True in the customization file. This build type consists of the the Python sources, the sequential shared library, and the parallel Python interpreter gpaw-python linking to the MPI libaries and optional BLAC/ScaLAPACK. We did not test this build type, the information is derived from analyzing the setup script. In the first two build types, all GPAW commands in the bin directory are Python scripts. Note that the manual of GPAW advises to use OMP_NUM_THREADS=1 which may influence other packages called from GPAW, but the default is to compile with single threaded libraries. It is possible and usually needed to change the default configuration by adapting siteconfig.py or writing your own variant of this script and passing its name (and path if not in the root GPAW directory) to either pip or python setup.py build and python setup.py install through the environment variable GPAW_CONFIG (like GPAW_CONFIG=MySiteconfig.py pip gpaw ). The contents of siteconfig.py is largely the same as for customize.py in prior versions, though the value of certain variables at the start is different. The libxc library is added to the list, but there is no automatic detection of the linear algebra libraries (which often did not work as expected anyway). The NumPy include files are auto-located but only just before compiling the shared library, and are not even shown in the configuration log file. The MPI compiler and linker will be set to 'mpicc' if that one is found, or 'None' otherwise. Bugs in 20.1.0 If CC and LDSHARED , the value of compiler or mpicompiler in the siteconfig.py (the former for a sequential shared library, the latter for a parallel one) is ignored during the compilation process and CC respectively LDSHARED are used for the compilation and link steps. Make sure CC and LDSHARWED are set to the MPI compiler/linker when doing a build with a parallel shared library, or make sure they are not set. setup.py suggests there is a flag --remove-default-flags which one can use to remove compiler flags imported from various environment variables (and/or Python configuration?), but the flag does not work at all. The code in build_interpreter in config.py to build the argument list for runtime library paths is suspicious. There is no way to set the right options by hand. There are cases where it may use the wrong option, e.g., -R rather than -Wl,-R . Note that this code is only used when building gpaw-python , it is not clear if setuptools does the right thing though when processing runtime_library_dirs . Variables for customize.py or siteconfig.py compiler: Default is None which implies that the compiler as would normally result from the standard Python extension configuration process. It is used for compiling and linking the sequential shared library. See the bugs for specific GPAW versions as this variable is often ignored in practice. mpicompiler: Default is mpicc if found and None otherwise. Used for parallel builds and in 20.1.0 and higher for linking the parallel shared library. See the bugs for specific GPAW versions. mpilinker: Default is mpicc if found and None otherwise. Used for linking gpaw-python . See the bugs for specific GPAW versions. libraries: List of libraries to be used when linking non-MPI code. Format of the list entries is 'XX' for libXX.a or libXX.so. mpi_libraries: Additional libraries for linking gpaw-python . library_dirs: Directories to search for the libraries in libraries . mpi_library_dirs: Directories to search for the libraries in mpi_libraries . include_dirs: Include directories needed to compile non-MPI source files. mpi_include_dirs: Additional include directories needed to compile sources for gpaw-python . runtime_library_dirs: list of directories to search for shared (dynamically loaded) libraries at run-time. It is not clear if they are always added to the command line in the right way. mpi_runtinme_library_dirs: Additionsl list of directories to search for shared libraries at run-time for gpaw-python . It is not clear if they are always added to the command line in the right way. extra_compile_args: Extra arguments used when compiling (on top of arguments by the regular Python procedure if --remove-default-flags is not used) extra_link_args: Extra arguments used when linking. define_macros: C preprocessor macros to define when compiling non-MPI code mpi_define_macros: Additionsl C preprocessor macros to define when compiling MPI code. undef_macros: C preprocessor macros to undefine. fftw (boolean, default False ): Use FFTW instead of NumPy FFTW scalapack (boolean, default False ): Use ScaLAPACK libvdwxc (boolean, default False ): Use libvdwxc elpa (boolean, default False ): Use ELPA noblas (20.1.0 and up only, default False ): Compile GPAW without BLAS libraries. parallel_python_interpreter (20.1.0 and up only, default False ): Build the parallel interpreter gpaw-python. Note that in version 19.8.1 (and likely earlier versions) all of the above variables starting with mpi are only used if mpicompiler points to a MPI compiler. They are only used to build the parallel interpreter gpaw-python . In version 20.1.0 (and likely later versions) all of the above variables starting with mpi are only used if parallel_python_interpreter is True , to build the parallel interpreter gpaw-python . This is simply confusing: What has to be specified in the non-mpi variables and what has to be specified in the mpi_ -variables differs depending on whether the new-style build with a parallel shared library is done, or the old-style with a sequential share library and MPI-support provided by gpaw-python . EasyConfig General remarks We based our EasyBuild compile procedure on an EasyConfig for the 19.8.1 version of GPAW included with EasyBuild 4.0.1 and the one for 20.1.0 included in EasyBuild 4.2.0. Both use a customization file to adapt various options for GPAW. This is a very standard procedure for GPAW and is almost always needed. A model customization file is in the GPAW sources as customize.py (version 19.8.1) or siteconfig_example.py (version 20.1.0). Problems encountered: * The patch in the GPAW EasyConfig of EasyBuild 4.0.1 poses several problems * It creates a new file and for some reason this does not work in an extension list. Even when specifying the level at which the patch should be applied, EasyBuild cannot find where to apply the patch. * The customization file is very incomplete and contained several errors * When building the shared library, compiler options are set the distuitls/setuptools way. Options are picked up from the environment ( CFLAGS but also some other variables that we do not use). Options specified in extra_compile_args in the customization script are then added to those. However, the executable gpaw-python is compiled in a different way (specifying the full build instructions in the setup script and config.py rather than organising it through distutils/setuptools) and there the compiler flags specified in the environment are not used. Hence when compiling gpaw-python one has to use extra_compile_args as otherwise no optimization options will be used when recompiling files with the MPI compiler for linking into gpaw-python . This implies no host-specific optimizations with the Intel compiler but implies -O0 should the GNU compilers be used. But setting this variable will also imply that compiler options may be repeated or conflict when compiling the shared library. If extra_compile_args is simply set to whatever is in CFLAGS , options are simply repeated which does no harm while one can still use the EasyBuild mechanisms to influence compiler options. * No support for ELPA, but this is trivial to add. * Not specifically an error, but the Lapack/BLAS libraries were included multiple times. * Setting include_dirs to [] is just plain stupid as it already contains information about the NumPy installation. In some cases (IntelPython for instance) the compilation process now fails to find the NumPy header files. * Implementing the customization script as a patch has its restrictions. It is a cumbersome process when one wants to experiment with options, as one has to edit the true sources, then make a patch for it in a location that EasyBuild can find, and then apply it, which is sometimes tricky for a patch that adds a new file as we must be sure that it is added where we need it (which is tricky in particular with extensions). We've looked at two other mechanisms, and ultimately went for the last after experimenting with the first and running into its limitation (too many files in the EasyConfig directories, easy to edit the wrong file) * Implement the customization file as an additional source file that is stored with the EasyConfig as this directory is also searched for sources by default. There are a number of pitfalls though * It is impossible to specify multiple source files in an extension list. Hence this procedure works well when GPAW is installed as a PythonPackage type EasyConfig, but a trick was needed when installing in an extension list of a 'Bundle' EasyConfig: We specify a dummy component that simply puts the file in the overall source directory of the Bundle and then refer to that one. This is easily done with a \"Tarball\" bundle component and a custom unpack procedure that creates the directory where we want the file to go. * That source file is not automatically copied to the directory where EasyBuild stores the sources for further use. This is in fact a good thing, and we didn't copy manually, as the version in that directory would take precedence to the one stored with the EasyConfig. Hence new edits wouldn't be taken into account. When not using the Tarball bundle component that puts the file automatically in the easybuild subdirectory of the installation directory, it is a good idea to copy the customization script to the %(installdir)s/easybuild directory so that we still have the exact file as it was used to install the program for reproducibility of the installation. * Put the configuation script as a string variable in the EasyConfig for GPAW, and then use prebuildopts to inject it into the GPAW sources using the Bash \"Here Document\" trick. The advantage of this approach is that all configuration information is in a single file which avoids errors when changing the configuration. * The build process could not find the FFTW include files of the Intel FFTW wrappers. It turns out that the include file is not included as fftw/fftw3.h or so, but without the subdirectory. This was solved by adapting our Intel compiler modules and simply including this directory as the problem may occur with other software also. The alternative is to simply compute the directory name from MKLROOT in the customization file and add that one to the list of directories to include in the search path. * The optional libvdwxc needs the FFTW3 MPI-interfaces that are not provided by the Intel wrappers. To make sure any conflict between routine names is impossible, we compile GPAW with the regular FFTW libraries rather than the Intel MKL wrappers when libvdwxc is included. * We also include the Atomic PAW setup files in the installation. They are put in the share/gpaw-setups subdirectory and pointed to by GPAW_SETUP_PATH as required by GPAW. Since this is a comma-separated path rather than the usual colon-separated path we simply set the variable rather than using prepend_path in the module file (so put it in mnodextravars rather than modextrapaths ). * In version 19.8.1, the installation manual advises to set OMP_NUM_THREADS to 1. To avoid user errors, we do this in the module file. 2019b toolchain - GPAW 19.8.1 GPAW was compiled with both a EasyBuild Python 3.7.4 module and the Intel Python 3 distribution, and in both cases both without libvdwxc using the Intel FFTW wrappers for MKL, and with libvdwxc but then using FFTW3 itself (as libvdwxc requires that library anyway). The versions without libvdwxc have the -MKLFFTW suffix. Port of GPAW 19.8.1 to the 2020a toolchains It turns out that there are dangerous omp simd pragmas in c/bmgs/fd.c and likely also in c/bmgs/relax.c and c/symmetry.c'. The one in fd.c caused segmentation violations, likely due to an unaligned memory access. A compiler is allowed to assume that vectorization is 100% safe when omp simd is used and hence to assume that alignment is OK. It looks like the 2020 Intel compilers do make that assumption while the 2019 compilers did not, or that they are the first Intel compilers that turn on OpenMP SIMD support without using the -qopenmp flag (they do it for optimization level -O2` and higher). There are several possible solutions * There is a patch from the authors for 20.1.0 that puts the omp simd in a conditional block. It may be that the only function of that patch is to avoid warnings about unrecognized pragma's with some compilers if OpenMP is not enabled. The pragmas are only included if _OPENMP is defined. We did not check if that solves the problem with the Intel 2020 compilers or if those compilers do define that preprocessor symbol already when they auto-enable OpenMP SIMD support. * Develop a similar patch that simply omits those lines in c/bmgs/fd.c , c/bmgs/relax.c and possibly also in c/symmetry.c (we are not certain that that one never causes problems). The advantage of such a patch is that it is very clear what it does. * Simply add -qno-openmp-simd to the CFLAGS to disable the automatically turned on OpenMP SIMD support also solves the problem. We have checked and there is no performance penalty compared to working with the patch in the second option, so it has no influence on the automatic vectorization capabilities of the compiler. Since no patch is needed, we used the last option. 2020a toolchains - GPAW 20.1.0 We checked our way of working with the EasyConfig files in the EasyBuilders repository as these are developed by people from the institute that also develops GPAW. The customization script is very different from the one used for GPAW 19.8.1, not because the structure has changed, but because it is now developed by different people. There are a number of things that are no longer necessary though, e.g., cleaning libraries and mpi_libraries as there is no linear algebra junk in there. Also some variables for ScaLAPACK do not need to be set anymore and are automatically correctly detected. Also take note of the change in the way the customization script is named and/or specified otherwise due to the switch from distutils to setuptools. The change in architecture has a significant influence on how to build GPAW. In particular, one now has to either set use_mpi in optarch to true to ensure that CC points to the MPI compiler, or unset CC when calling setup.py as it overwrites whatever the setup.py script tries to arrange based on the values of compiler and mpicompiler in the customization file. We use the latter approach and simply unset CC and LDSHARED to ensure that the value specified for mpicompiler is used. Note that besides the changes already mentioned earlier, GPAW is now also packaged as an egg, so `lib/python3.8/site-packages/gpaw' does no longer exist. We took the sanity check tests from the EasyBuilders recipes for GPAW 20.1.0. Note that we did try building with pip as in the EasyBuilders recipe but this did not work as GPAW tried to link to blas by adding -lblas to the link command line which is the wrong BLAS library. The problem with the omp simd pragmas causing segmentation violations is still present. See the remarks for the the port of 19.8.1 to the Intel 2020 compilers for the solution that we implemented. As we already cat the configuration.log file to the regular EasyBuild log, we do no longer store a copy of the file in %(installdir)s/easybuild, so postinstallcmds is no longer needed. Backport of GPAW 20.1.0 to the 2019b toolchains Due to problems with the 2020a compiler that got solved later in the process, we also generated a version of GPAW 20.1.0 for the 2019b toolchains as we know that 19.8.1 worked properly with those compilers. This is a trivial backport. Even though it seems that -qno-openmp-simd is not needed, we stuck to it as it does no harm either. Checking the build result Search in the EasyBuild log file for python setup to see if the compilation did not produce errors. Some packages have Python fallback code if the compilation fails, so the standard EasyBuild sanity check will not detect these problems, but the result will be a much slower Python package. The smallest of the PRACE UEABS benchmarks runs fine on a single node (finished in about 4 minutes on a 28-core broadwell node). Hence this is also an easy test. We did not that the benchmark does not work anymore with GPAW 20.1.0, which diverges. This may be due to a change in algorithms and not to a problem with the compilation of GPAW.","title":"GPAW installation notes"},{"location":"generated/easyconfigs/g/GPAW/#gpaw-installation-notes","text":"GPAW is a Quantum MD code. GPAW web site , which also contains the installation instructions.","title":"GPAW installation notes"},{"location":"generated/easyconfigs/g/GPAW/#dependencies","text":"GPAW is Python code (3.5 or newer) but it also contains some C code for some performance-critical parts and to interface to a number of libraries on which it depends (for GPAW 19.8.1): BLAS LAPACK MPI BLACS and ScaLAPACK FFTW is highly recommended LibXC 3.X or 4.X. LibXC is a library of exchange-correlation functions for density-functional theory Optional dependency: libvdwxc , a portable C library of density functionals with van der Waals interactions for density functional theory. As of December 2019, this library is still unreleased. It is not clear from the installation documentation whether we need the MPI-aware version of libvdwxc or not, but further on in the documentation it claims that libvdwxc will automatically parallelise which implies the MPI-aware version is needed. Optional dependency: ELPA , which should improve performance for large systems when GPAW is used in LCAO mode GPAW depends on a number of standard Python packages. As of GPAW 19.8.1, these are: NumPY 1.9 or later SciPy 0.14 or later GPAW relies on ASE , a Python package from the same group Check the release notes of GPAW as the releases of ASE and GPAW should match. E.g., when starting this documentation, version 19.8.1 was the most up-to-date release of GPAW with 3.18.0 the matching ASE version. See further below for more instructions on how to determine the right versions for dependencies. ASE has some optional dependencies that are not needed for the benchmarking: Matplotlib (2.0.0 or newer), tkinter and Flask. BLAS, LAPACK, MPI, BLACS and ScaLAPACK are standard components of the Intel toolchain that we use for installing GPAW. See this repository for the FFTW and LibXC EasyConfig files as used at UAntwerp. NumPy and SciPy are standard components of the Python bundles in use at UAntwerp, see the EasyConfig files in this repository. ASE introduces additional dependencies on other Python packages: Matplotlib, tkinter and Flask. These are optional and only needed for some of the functionality. * Matplotlib is a standard component in the Python modules at UAntwerp, see the Python and IntelPython3-Packages EasyConfigs in this repository. * tkinter, the TK interface, is a standard component of recent Python distributions. * Flask needs to be installed. It brings with it a set of other dependencies: * Werkzeug * Jinja2 , and this one needs * MarkupSafe * itsdangerous * click : Included in recent standard Python packages at UAntwerp, but not in some of the older ones. GPAW, when installed automatically, rarely has the best configuration for HPC, so check the section on the internal organization of the compilation process on how to customize to get a proper HPC build.","title":"Dependencies"},{"location":"generated/easyconfigs/g/GPAW/#determining-version-numbers-of-dependencies","text":"Python versions: On PyPi or towards the bottom of the file setup.py ASE version: In the release notes : preferred version, though other versions may also work. Line install_requires towards the bottom of the file setup.py may give a different restriction from the release notes. In the file doc/install.rst NumPy and SciPy: In the file doc/install.rst GPAW Python ASE NumPy SciPy LibXC 19.8.1 3.4-3.7 3.18.0, \u22653.18.0 \u22651.9 \u22650.14 3.x or 4.x 20.1.0 3.5-3.8 3.19.0, \u22653.18.0 \u22651.9 \u22650.14 3.x or 4.x","title":"Determining version numbers of dependencies"},{"location":"generated/easyconfigs/g/GPAW/#internal-organization-of-the-compilation-process-of-gpaw","text":"GPAW is compiled through Python scripts to integrate in the standard Python package setup process. * Up to and including version 19.8.1: The scripts are fairly outdated though as they still rely on distutils rather than setuptools . * From version 20.1.0 on, setuptools is used instead of distutils easing installation with pip . As a result of this, there are some differences between the way versions up to 19.8.1 and versions from 20.1.0 are configured. There are also differences in the files generated that are not related to this switch. However, togehter with this transition, there was also an important change to how parallel computations are organized, leading to even more differences. In both cases the build process can be customized (and often has to be customized) through a customization script.","title":"Internal organization of the compilation process of GPAW"},{"location":"generated/easyconfigs/g/GPAW/#version-1981-and-likely-below","text":"GPAW consists of a lot of Python code, a purely sequential shared library _gpaw.cpython-<python version>-<architecture>.so for some operations and for a parallel installation also the specialised parallel Python interpreter gpaw-python . There are two types of installation: 1. If mpicc is not found and no MPI compiler is specified in the customization script, of if the automatic detection of mpicc is undone in the customization script by setting mpicompiler to None , a purely sequential version of GPAW is installed, including only the Python code and the shared library. 1. If mpicc is found or another MPI compiler is specified in the customization script, a distributed memory version of GPAW is build, consisting of the Python code, the sequential shared library and the parallel Python interpreter gpaw-python . Only the latter links to the MPI libraries and to BLACS/ScaLAPACK (if used). Note that the manual of GPAW advises to use OMP_NUM_THREADS=1 which may influence other packages called from GPAW, but the default is to compile with single threaded libraries. The other commands in the bin directory (beside gpaw-python ) are all regular Python scripts, except for gpaw-runscript which is executed by gpaw-python . Note that this script is only an example and has not been adapted to our particular cluster! It is possible and needed to change the default configuration though by adapting customize.py or writing your own variant of this script and then overwriting the default by adding --customize=<name of file> to the argument list of python setup.py build and python setup.py install . At the point where customize.py is called, the NumPy include files have been located, libxc is added to the library list, and the configure process has done an effort to auto-locate the linear algebra libraries (at least BLAS and Lapack, see below). However, we experienced that the choice was often wrong and the resulting code refused to run. The MPI compiler and linker will be set to 'mpicc' if that one is found, or 'None' otherwise. The script config.py tries very hard to auto-determine a suitable BLAS/LAPACK library for various platforms that it recognizes (even ARM). However, it does so not by looking in LIBRARY_PATH or LD_LIBRARY_PATH but only checks default locations for installation of those libraries as part of the OS, except for MKL, where it does use the MKLROOT environment variable. Hence it fails to detect any library in modules that are installed elsewhere in the file system (except then for MKL if the module make correct use of MKLROOT ). It is important that this is corrected in a customization file. Note that on 64-bit Intel architecture if multiple supported BLAS libraries are found, the preferred order is: 1. ACML, in /opt/acml... 1. Intel MKL 1. OpenBLAS , in 'usr/lib', '/usr/local/lib', '/usr/lib64' 1. ATLAS , in 'usr/lib', '/usr/local/lib', '/usr/lib64/atlas' 1. libsatlas: Serial version of Atlas (recent versions of Atlas compile a serial and a threaded version, the latter called libtatlas.) Search is in the same directories as for libatlas. The distutils build_ext method is redefined in setup.py . It first calls the regular distutils build_ext method to build the shared library and prepare some Python scripts for the bin directory, and then calls the build_interpreter routing defined in config.py to build gpaw-python . Only a limited set of C files contain calls to the MPI libraries. The shared library contains no MPI calls. All C files that contain MPI calls include header files that define stub data types/routines/macros so that the sources can be compiled to a serial shared library without the use of a MPI compiler wrapper. When building 'gpaw-python' at a later stage, those files that do contain calls to MPI are recompiled using the MPI compiler wrapper, and all objects are then linked into the gpaw-python binary. For the shared library, compiler flags are used in the distutils way, taking them from the Python configuration and environment. This may lead to very misleading results. You may find customization scripts that specify a different compiler through the compiler variable, but that value is ignored if CC and `LDSHARED are defined in the environment. We consider this a bug, see below, as the code does not do what the comments in setup.py suggests. For linking gpaw-python , one may need to check the argument that is used to detect the correct option for adding runtime library search paths to the command line (if this is used in the customization script). The current code assumes the MPI compiler wrapper is called mpicc and checks if this calls gcc but doesn't recognize other compilers, so the option used may not be the right one.","title":"Version 19.8.1 and likely below"},{"location":"generated/easyconfigs/g/GPAW/#bugs-in-1981","text":"If CC and LDSHARED , the value of compiler customize.py script is ignored during the compilation process and CC respectively LDSHARED are used for the compilation and link steps. The hack to remove this in setup.py does not work. The code is simply incomplete as it does not push the variables back to the environment or fails to feed them in another way to the compilation process. setup.py suggests there is a flag --remove-default-flags which one can use to remove compiler flags imported from various environment variables (and/or Python configuration?), but the flag does not work at all. The hack that should actually remove those (located in the same block of code as the hack for the previous bug) does not work for the same reason that the hack to overrule setting CC or LDSHARED does not work. The code in build_interpreter in config.py to build the argument list for runtime library paths is suspicious. There is no way to set the right options by hand. There are cases where it may use the wrong option, e.g., -R rather than -Wl,-R .","title":"Bugs in 19.8.1"},{"location":"generated/easyconfigs/g/GPAW/#version-2010-and-likely-above","text":"As in earlier versions, GPAW consists of a lot of Python code, a shared library _gpaw.cpython-<python version>-<architecture>.so for some operations and optionally the binary gpaw-python , a specialized parallel Python operator. There is however a major difference for the preferred parallel build of GPAW. There are now three build types, the third one being undocumented in the documentation of version 20.1.0 and hence likely deprecated. 1. Sequential build: If mpicc is not found and no MPI compiler is specified in the customization script, of if the automatic detection of mpicc is undone in the customization script by setting mpicompiler to None , a purely sequential version of GPAW is installed, including only the Python code and the sequential shared library. 1. Prefered parallel build: If mpicc is found or another MPI compiler is specified in the customization script, a distributed memory version of GPAW is build, consisting of the Python code and a parallel shared library. Contrary to version 19.8.1, MPI and the optional BLACS/ScaLAPACK are now linked directly into the shared library. 1. Old-style parallel build: To enable this build type, set mpicompiler (or make sure mpicc is found) and set parallel_python_interpreter to True in the customization file. This build type consists of the the Python sources, the sequential shared library, and the parallel Python interpreter gpaw-python linking to the MPI libaries and optional BLAC/ScaLAPACK. We did not test this build type, the information is derived from analyzing the setup script. In the first two build types, all GPAW commands in the bin directory are Python scripts. Note that the manual of GPAW advises to use OMP_NUM_THREADS=1 which may influence other packages called from GPAW, but the default is to compile with single threaded libraries. It is possible and usually needed to change the default configuration by adapting siteconfig.py or writing your own variant of this script and passing its name (and path if not in the root GPAW directory) to either pip or python setup.py build and python setup.py install through the environment variable GPAW_CONFIG (like GPAW_CONFIG=MySiteconfig.py pip gpaw ). The contents of siteconfig.py is largely the same as for customize.py in prior versions, though the value of certain variables at the start is different. The libxc library is added to the list, but there is no automatic detection of the linear algebra libraries (which often did not work as expected anyway). The NumPy include files are auto-located but only just before compiling the shared library, and are not even shown in the configuration log file. The MPI compiler and linker will be set to 'mpicc' if that one is found, or 'None' otherwise.","title":"Version 20.1.0 and likely above"},{"location":"generated/easyconfigs/g/GPAW/#bugs-in-2010","text":"If CC and LDSHARED , the value of compiler or mpicompiler in the siteconfig.py (the former for a sequential shared library, the latter for a parallel one) is ignored during the compilation process and CC respectively LDSHARED are used for the compilation and link steps. Make sure CC and LDSHARWED are set to the MPI compiler/linker when doing a build with a parallel shared library, or make sure they are not set. setup.py suggests there is a flag --remove-default-flags which one can use to remove compiler flags imported from various environment variables (and/or Python configuration?), but the flag does not work at all. The code in build_interpreter in config.py to build the argument list for runtime library paths is suspicious. There is no way to set the right options by hand. There are cases where it may use the wrong option, e.g., -R rather than -Wl,-R . Note that this code is only used when building gpaw-python , it is not clear if setuptools does the right thing though when processing runtime_library_dirs .","title":"Bugs in 20.1.0"},{"location":"generated/easyconfigs/g/GPAW/#variables-for-customizepy-or-siteconfigpy","text":"compiler: Default is None which implies that the compiler as would normally result from the standard Python extension configuration process. It is used for compiling and linking the sequential shared library. See the bugs for specific GPAW versions as this variable is often ignored in practice. mpicompiler: Default is mpicc if found and None otherwise. Used for parallel builds and in 20.1.0 and higher for linking the parallel shared library. See the bugs for specific GPAW versions. mpilinker: Default is mpicc if found and None otherwise. Used for linking gpaw-python . See the bugs for specific GPAW versions. libraries: List of libraries to be used when linking non-MPI code. Format of the list entries is 'XX' for libXX.a or libXX.so. mpi_libraries: Additional libraries for linking gpaw-python . library_dirs: Directories to search for the libraries in libraries . mpi_library_dirs: Directories to search for the libraries in mpi_libraries . include_dirs: Include directories needed to compile non-MPI source files. mpi_include_dirs: Additional include directories needed to compile sources for gpaw-python . runtime_library_dirs: list of directories to search for shared (dynamically loaded) libraries at run-time. It is not clear if they are always added to the command line in the right way. mpi_runtinme_library_dirs: Additionsl list of directories to search for shared libraries at run-time for gpaw-python . It is not clear if they are always added to the command line in the right way. extra_compile_args: Extra arguments used when compiling (on top of arguments by the regular Python procedure if --remove-default-flags is not used) extra_link_args: Extra arguments used when linking. define_macros: C preprocessor macros to define when compiling non-MPI code mpi_define_macros: Additionsl C preprocessor macros to define when compiling MPI code. undef_macros: C preprocessor macros to undefine. fftw (boolean, default False ): Use FFTW instead of NumPy FFTW scalapack (boolean, default False ): Use ScaLAPACK libvdwxc (boolean, default False ): Use libvdwxc elpa (boolean, default False ): Use ELPA noblas (20.1.0 and up only, default False ): Compile GPAW without BLAS libraries. parallel_python_interpreter (20.1.0 and up only, default False ): Build the parallel interpreter gpaw-python. Note that in version 19.8.1 (and likely earlier versions) all of the above variables starting with mpi are only used if mpicompiler points to a MPI compiler. They are only used to build the parallel interpreter gpaw-python . In version 20.1.0 (and likely later versions) all of the above variables starting with mpi are only used if parallel_python_interpreter is True , to build the parallel interpreter gpaw-python . This is simply confusing: What has to be specified in the non-mpi variables and what has to be specified in the mpi_ -variables differs depending on whether the new-style build with a parallel shared library is done, or the old-style with a sequential share library and MPI-support provided by gpaw-python .","title":"Variables for customize.py or siteconfig.py"},{"location":"generated/easyconfigs/g/GPAW/#easyconfig","text":"","title":"EasyConfig"},{"location":"generated/easyconfigs/g/GPAW/#general-remarks","text":"We based our EasyBuild compile procedure on an EasyConfig for the 19.8.1 version of GPAW included with EasyBuild 4.0.1 and the one for 20.1.0 included in EasyBuild 4.2.0. Both use a customization file to adapt various options for GPAW. This is a very standard procedure for GPAW and is almost always needed. A model customization file is in the GPAW sources as customize.py (version 19.8.1) or siteconfig_example.py (version 20.1.0). Problems encountered: * The patch in the GPAW EasyConfig of EasyBuild 4.0.1 poses several problems * It creates a new file and for some reason this does not work in an extension list. Even when specifying the level at which the patch should be applied, EasyBuild cannot find where to apply the patch. * The customization file is very incomplete and contained several errors * When building the shared library, compiler options are set the distuitls/setuptools way. Options are picked up from the environment ( CFLAGS but also some other variables that we do not use). Options specified in extra_compile_args in the customization script are then added to those. However, the executable gpaw-python is compiled in a different way (specifying the full build instructions in the setup script and config.py rather than organising it through distutils/setuptools) and there the compiler flags specified in the environment are not used. Hence when compiling gpaw-python one has to use extra_compile_args as otherwise no optimization options will be used when recompiling files with the MPI compiler for linking into gpaw-python . This implies no host-specific optimizations with the Intel compiler but implies -O0 should the GNU compilers be used. But setting this variable will also imply that compiler options may be repeated or conflict when compiling the shared library. If extra_compile_args is simply set to whatever is in CFLAGS , options are simply repeated which does no harm while one can still use the EasyBuild mechanisms to influence compiler options. * No support for ELPA, but this is trivial to add. * Not specifically an error, but the Lapack/BLAS libraries were included multiple times. * Setting include_dirs to [] is just plain stupid as it already contains information about the NumPy installation. In some cases (IntelPython for instance) the compilation process now fails to find the NumPy header files. * Implementing the customization script as a patch has its restrictions. It is a cumbersome process when one wants to experiment with options, as one has to edit the true sources, then make a patch for it in a location that EasyBuild can find, and then apply it, which is sometimes tricky for a patch that adds a new file as we must be sure that it is added where we need it (which is tricky in particular with extensions). We've looked at two other mechanisms, and ultimately went for the last after experimenting with the first and running into its limitation (too many files in the EasyConfig directories, easy to edit the wrong file) * Implement the customization file as an additional source file that is stored with the EasyConfig as this directory is also searched for sources by default. There are a number of pitfalls though * It is impossible to specify multiple source files in an extension list. Hence this procedure works well when GPAW is installed as a PythonPackage type EasyConfig, but a trick was needed when installing in an extension list of a 'Bundle' EasyConfig: We specify a dummy component that simply puts the file in the overall source directory of the Bundle and then refer to that one. This is easily done with a \"Tarball\" bundle component and a custom unpack procedure that creates the directory where we want the file to go. * That source file is not automatically copied to the directory where EasyBuild stores the sources for further use. This is in fact a good thing, and we didn't copy manually, as the version in that directory would take precedence to the one stored with the EasyConfig. Hence new edits wouldn't be taken into account. When not using the Tarball bundle component that puts the file automatically in the easybuild subdirectory of the installation directory, it is a good idea to copy the customization script to the %(installdir)s/easybuild directory so that we still have the exact file as it was used to install the program for reproducibility of the installation. * Put the configuation script as a string variable in the EasyConfig for GPAW, and then use prebuildopts to inject it into the GPAW sources using the Bash \"Here Document\" trick. The advantage of this approach is that all configuration information is in a single file which avoids errors when changing the configuration. * The build process could not find the FFTW include files of the Intel FFTW wrappers. It turns out that the include file is not included as fftw/fftw3.h or so, but without the subdirectory. This was solved by adapting our Intel compiler modules and simply including this directory as the problem may occur with other software also. The alternative is to simply compute the directory name from MKLROOT in the customization file and add that one to the list of directories to include in the search path. * The optional libvdwxc needs the FFTW3 MPI-interfaces that are not provided by the Intel wrappers. To make sure any conflict between routine names is impossible, we compile GPAW with the regular FFTW libraries rather than the Intel MKL wrappers when libvdwxc is included. * We also include the Atomic PAW setup files in the installation. They are put in the share/gpaw-setups subdirectory and pointed to by GPAW_SETUP_PATH as required by GPAW. Since this is a comma-separated path rather than the usual colon-separated path we simply set the variable rather than using prepend_path in the module file (so put it in mnodextravars rather than modextrapaths ). * In version 19.8.1, the installation manual advises to set OMP_NUM_THREADS to 1. To avoid user errors, we do this in the module file.","title":"General remarks"},{"location":"generated/easyconfigs/g/GPAW/#2019b-toolchain-gpaw-1981","text":"GPAW was compiled with both a EasyBuild Python 3.7.4 module and the Intel Python 3 distribution, and in both cases both without libvdwxc using the Intel FFTW wrappers for MKL, and with libvdwxc but then using FFTW3 itself (as libvdwxc requires that library anyway). The versions without libvdwxc have the -MKLFFTW suffix.","title":"2019b toolchain - GPAW 19.8.1"},{"location":"generated/easyconfigs/g/GPAW/#port-of-gpaw-1981-to-the-2020a-toolchains","text":"It turns out that there are dangerous omp simd pragmas in c/bmgs/fd.c and likely also in c/bmgs/relax.c and c/symmetry.c'. The one in fd.c caused segmentation violations, likely due to an unaligned memory access. A compiler is allowed to assume that vectorization is 100% safe when omp simd is used and hence to assume that alignment is OK. It looks like the 2020 Intel compilers do make that assumption while the 2019 compilers did not, or that they are the first Intel compilers that turn on OpenMP SIMD support without using the -qopenmp flag (they do it for optimization level -O2` and higher). There are several possible solutions * There is a patch from the authors for 20.1.0 that puts the omp simd in a conditional block. It may be that the only function of that patch is to avoid warnings about unrecognized pragma's with some compilers if OpenMP is not enabled. The pragmas are only included if _OPENMP is defined. We did not check if that solves the problem with the Intel 2020 compilers or if those compilers do define that preprocessor symbol already when they auto-enable OpenMP SIMD support. * Develop a similar patch that simply omits those lines in c/bmgs/fd.c , c/bmgs/relax.c and possibly also in c/symmetry.c (we are not certain that that one never causes problems). The advantage of such a patch is that it is very clear what it does. * Simply add -qno-openmp-simd to the CFLAGS to disable the automatically turned on OpenMP SIMD support also solves the problem. We have checked and there is no performance penalty compared to working with the patch in the second option, so it has no influence on the automatic vectorization capabilities of the compiler. Since no patch is needed, we used the last option.","title":"Port of GPAW 19.8.1 to the 2020a toolchains"},{"location":"generated/easyconfigs/g/GPAW/#2020a-toolchains-gpaw-2010","text":"We checked our way of working with the EasyConfig files in the EasyBuilders repository as these are developed by people from the institute that also develops GPAW. The customization script is very different from the one used for GPAW 19.8.1, not because the structure has changed, but because it is now developed by different people. There are a number of things that are no longer necessary though, e.g., cleaning libraries and mpi_libraries as there is no linear algebra junk in there. Also some variables for ScaLAPACK do not need to be set anymore and are automatically correctly detected. Also take note of the change in the way the customization script is named and/or specified otherwise due to the switch from distutils to setuptools. The change in architecture has a significant influence on how to build GPAW. In particular, one now has to either set use_mpi in optarch to true to ensure that CC points to the MPI compiler, or unset CC when calling setup.py as it overwrites whatever the setup.py script tries to arrange based on the values of compiler and mpicompiler in the customization file. We use the latter approach and simply unset CC and LDSHARED to ensure that the value specified for mpicompiler is used. Note that besides the changes already mentioned earlier, GPAW is now also packaged as an egg, so `lib/python3.8/site-packages/gpaw' does no longer exist. We took the sanity check tests from the EasyBuilders recipes for GPAW 20.1.0. Note that we did try building with pip as in the EasyBuilders recipe but this did not work as GPAW tried to link to blas by adding -lblas to the link command line which is the wrong BLAS library. The problem with the omp simd pragmas causing segmentation violations is still present. See the remarks for the the port of 19.8.1 to the Intel 2020 compilers for the solution that we implemented. As we already cat the configuration.log file to the regular EasyBuild log, we do no longer store a copy of the file in %(installdir)s/easybuild, so postinstallcmds is no longer needed.","title":"2020a toolchains - GPAW 20.1.0"},{"location":"generated/easyconfigs/g/GPAW/#backport-of-gpaw-2010-to-the-2019b-toolchains","text":"Due to problems with the 2020a compiler that got solved later in the process, we also generated a version of GPAW 20.1.0 for the 2019b toolchains as we know that 19.8.1 worked properly with those compilers. This is a trivial backport. Even though it seems that -qno-openmp-simd is not needed, we stuck to it as it does no harm either.","title":"Backport of GPAW 20.1.0 to the 2019b toolchains"},{"location":"generated/easyconfigs/g/GPAW/#checking-the-build-result","text":"Search in the EasyBuild log file for python setup to see if the compilation did not produce errors. Some packages have Python fallback code if the compilation fails, so the standard EasyBuild sanity check will not detect these problems, but the result will be a much slower Python package. The smallest of the PRACE UEABS benchmarks runs fine on a single node (finished in about 4 minutes on a 28-core broadwell node). Hence this is also an easy test. We did not that the benchmark does not work anymore with GPAW 20.1.0, which diverges. This may be due to a change in algorithms and not to a problem with the compilation of GPAW.","title":"Checking the build result"},{"location":"generated/easyconfigs/g/GROMACS/","text":"GROMACS installation instructions General information GROMACS uses CMake for configuration. A number of important variables are: Variable Default What? GMX_DOUBLE OFF Use double precision GMX_OPENMP ON Use OpenMP GMX_MPI OFF Use MPI GMX_THREAD_MPI ON Build a thread-MPI-based multithreaded version of GROMACS (not compatible with MPI) GMX_SIMD AUTO AUTO seems to do the job of selecting the right SIMD code well enough GMX_GPU OFF GMX_USE_OPENCL OFF Enable OpenCL acceleration GMX_X11 OFF GMX_FFT_LIBRARY fftw3 Indicate which FFT library should be used (e.g., mkl) GMX_EXTERNAL_BLAS O GMX_EXTERNAL_LAPACK O GMX_USE_TNG ON Use the TNG library for trajectory I/O GMX_EXTERNAL_TNG OFF GMX_EXTERNAL_ZLIB OFF GMX_PREFER_STATIC_LIBS O BUILD_SHARED_LIBS ON Try OFF in case of segmentation violations. EasyConfigs We switched from using the EasyBuilders recipes that are based on a custom EasyBlock for GROMACS to our in-house-developed recipes that use the CMakeMake EasyBlock. The reason is that we wanted to make clearer which CMake options were used to be able to better play with those options and work around problems. We use a Bundle setup to generate multiple executables with and without MPI and in single or double precision. Installation hints Search for \"Test project\" to locate the tests in the EasyBuild log file. GROMACS 2018.2, Intel 2018b toolchain GROMACS recognizes the Intel compilers and choses options that it deems appropriate. They are added to the default options that EasyBuild tries to impose. Since we assume that the options were in fact carefully chosen (as they are nontrivial) we prevent EasyBuild from overwriting these. Benchmarking of GROMACS 2018.2 on our systems for a problem from one of our users also showed better performance if we did not try to impose the EasyBuild compiler options which was an additional motivation for us to leave behind the GROMACS EasyBlock. Yet on machines with AVX and no AVX2 it turns out that some floating point tests fail in the underflow range. Adding the floating point option -no-ftz cures this problem. Since this may have a negative influence on performance we only use this option on our Ivy Bridge cluster. GROMACS 2019.4, Intel toolchain The problem with test failing on Ivy Bridge still occurs with GROMACS 2019.4 and the Intel 2019 compilers. Due to the immaturity of the Intel 2019 MPI implementation, we also generated executables with the Intel 2018 compilers ()2018b toolchain). The Intel 2019 update 5 compilers crash when compiling GROMACS 2019.4. Update 4 works. PLUMED 2.6.0 integration PLUMED 2.6.0 contains a patch to integrate with GROMACS 2019.4. This can be installed through the plumed patch -p -e gromacs-2019.4 command, before configuration with CMake according to the GROMACS and PLUMED manuals. Single precision executables fail with segmentation violations during a number of tests when using the Intel 2019 update 4 compilers for PLUMED and GROMACS. GROMACS 2020.2, Intel toolchain Did a straightforwared port from the 2019.4 EasyConfig. GROMACS 2021.1, Intel toolchain Replaced -DGMX_SIMD=AUTO by -DGMX_SIMD=AVX2_128 for Rome/Zen2 only, following https://manual.gromacs.org/documentation/2021/install-guide/index.html#simd-support Needs 2020.4 compilers (problems with 2020.0), so slight workaround for toolchain on Rome/Zen2 Added -DGMXAPI=OFF Removed \"check\" from make in CUDA version","title":"GROMACS installation instructions"},{"location":"generated/easyconfigs/g/GROMACS/#gromacs-installation-instructions","text":"","title":"GROMACS installation instructions"},{"location":"generated/easyconfigs/g/GROMACS/#general-information","text":"GROMACS uses CMake for configuration. A number of important variables are: Variable Default What? GMX_DOUBLE OFF Use double precision GMX_OPENMP ON Use OpenMP GMX_MPI OFF Use MPI GMX_THREAD_MPI ON Build a thread-MPI-based multithreaded version of GROMACS (not compatible with MPI) GMX_SIMD AUTO AUTO seems to do the job of selecting the right SIMD code well enough GMX_GPU OFF GMX_USE_OPENCL OFF Enable OpenCL acceleration GMX_X11 OFF GMX_FFT_LIBRARY fftw3 Indicate which FFT library should be used (e.g., mkl) GMX_EXTERNAL_BLAS O GMX_EXTERNAL_LAPACK O GMX_USE_TNG ON Use the TNG library for trajectory I/O GMX_EXTERNAL_TNG OFF GMX_EXTERNAL_ZLIB OFF GMX_PREFER_STATIC_LIBS O BUILD_SHARED_LIBS ON Try OFF in case of segmentation violations.","title":"General information"},{"location":"generated/easyconfigs/g/GROMACS/#easyconfigs","text":"We switched from using the EasyBuilders recipes that are based on a custom EasyBlock for GROMACS to our in-house-developed recipes that use the CMakeMake EasyBlock. The reason is that we wanted to make clearer which CMake options were used to be able to better play with those options and work around problems. We use a Bundle setup to generate multiple executables with and without MPI and in single or double precision.","title":"EasyConfigs"},{"location":"generated/easyconfigs/g/GROMACS/#installation-hints","text":"Search for \"Test project\" to locate the tests in the EasyBuild log file.","title":"Installation hints"},{"location":"generated/easyconfigs/g/GROMACS/#gromacs-20182-intel-2018b-toolchain","text":"GROMACS recognizes the Intel compilers and choses options that it deems appropriate. They are added to the default options that EasyBuild tries to impose. Since we assume that the options were in fact carefully chosen (as they are nontrivial) we prevent EasyBuild from overwriting these. Benchmarking of GROMACS 2018.2 on our systems for a problem from one of our users also showed better performance if we did not try to impose the EasyBuild compiler options which was an additional motivation for us to leave behind the GROMACS EasyBlock. Yet on machines with AVX and no AVX2 it turns out that some floating point tests fail in the underflow range. Adding the floating point option -no-ftz cures this problem. Since this may have a negative influence on performance we only use this option on our Ivy Bridge cluster.","title":"GROMACS 2018.2, Intel 2018b toolchain"},{"location":"generated/easyconfigs/g/GROMACS/#gromacs-20194-intel-toolchain","text":"The problem with test failing on Ivy Bridge still occurs with GROMACS 2019.4 and the Intel 2019 compilers. Due to the immaturity of the Intel 2019 MPI implementation, we also generated executables with the Intel 2018 compilers ()2018b toolchain). The Intel 2019 update 5 compilers crash when compiling GROMACS 2019.4. Update 4 works.","title":"GROMACS 2019.4, Intel toolchain"},{"location":"generated/easyconfigs/g/GROMACS/#plumed-260-integration","text":"PLUMED 2.6.0 contains a patch to integrate with GROMACS 2019.4. This can be installed through the plumed patch -p -e gromacs-2019.4 command, before configuration with CMake according to the GROMACS and PLUMED manuals. Single precision executables fail with segmentation violations during a number of tests when using the Intel 2019 update 4 compilers for PLUMED and GROMACS.","title":"PLUMED 2.6.0 integration"},{"location":"generated/easyconfigs/g/GROMACS/#gromacs-20202-intel-toolchain","text":"Did a straightforwared port from the 2019.4 EasyConfig.","title":"GROMACS 2020.2, Intel toolchain"},{"location":"generated/easyconfigs/g/GROMACS/#gromacs-20211-intel-toolchain","text":"Replaced -DGMX_SIMD=AUTO by -DGMX_SIMD=AVX2_128 for Rome/Zen2 only, following https://manual.gromacs.org/documentation/2021/install-guide/index.html#simd-support Needs 2020.4 compilers (problems with 2020.0), so slight workaround for toolchain on Rome/Zen2 Added -DGMXAPI=OFF Removed \"check\" from make in CUDA version","title":"GROMACS 2021.1, Intel toolchain"},{"location":"generated/easyconfigs/g/GenomeTools/","text":"GenomeTools installation instructions General information GenomeTools wasn't updated much between 2017 and early 2020. Problems started to arrise with new versions of Pango and cairo. GenomeTools includes most other dependencies. See the direcotry src/external Some of those libraries are really ancient and one can hence expect that newer versions may not work. Though it is possible to use already installed versions instead of the included onces by specifying useshared=yes on the make command line, it is not advised since some of the versions are so old we might expect new versions may not be compatible with the code. This is particulary the case for the included samtools. EasyConfigs Our own EasyConfig is derived from the EasyBuilders EasyConfigs for GenomeTools.","title":"GenomeTools installation instructions"},{"location":"generated/easyconfigs/g/GenomeTools/#genometools-installation-instructions","text":"","title":"GenomeTools installation instructions"},{"location":"generated/easyconfigs/g/GenomeTools/#general-information","text":"GenomeTools wasn't updated much between 2017 and early 2020. Problems started to arrise with new versions of Pango and cairo. GenomeTools includes most other dependencies. See the direcotry src/external Some of those libraries are really ancient and one can hence expect that newer versions may not work. Though it is possible to use already installed versions instead of the included onces by specifying useshared=yes on the make command line, it is not advised since some of the versions are so old we might expect new versions may not be compatible with the code. This is particulary the case for the included samtools.","title":"General information"},{"location":"generated/easyconfigs/g/GenomeTools/#easyconfigs","text":"Our own EasyConfig is derived from the EasyBuilders EasyConfigs for GenomeTools.","title":"EasyConfigs"},{"location":"generated/easyconfigs/g/gnuplot/","text":"GNUplot installation instruction GNUplot home page General remarks GNUplot 5.2.x is incompatible with libgd 2.3.x as it uses the libgd-config script which is dropped from the 2.3.x releases of libgd (after being deprecated for a while). The problem here is that 2.3.x does solve security problems present in the 2.2.5 release so reverting to an older version of libgd isn't really a solution either... EasyConfigs The structure of our EasyConfig files is a bit different from those of the EasyBuilders repositories as we have split up the dependencies to make it easier to enable or disable certain options in GNUplot. 5.4.0 for 2020a We had to use preconfigopts to change the LIBS variable to avoid the error message about libiconv_open.","title":"GNUplot installation instruction"},{"location":"generated/easyconfigs/g/gnuplot/#gnuplot-installation-instruction","text":"GNUplot home page","title":"GNUplot installation instruction"},{"location":"generated/easyconfigs/g/gnuplot/#general-remarks","text":"GNUplot 5.2.x is incompatible with libgd 2.3.x as it uses the libgd-config script which is dropped from the 2.3.x releases of libgd (after being deprecated for a while). The problem here is that 2.3.x does solve security problems present in the 2.2.5 release so reverting to an older version of libgd isn't really a solution either...","title":"General remarks"},{"location":"generated/easyconfigs/g/gnuplot/#easyconfigs","text":"The structure of our EasyConfig files is a bit different from those of the EasyBuilders repositories as we have split up the dependencies to make it easier to enable or disable certain options in GNUplot.","title":"EasyConfigs"},{"location":"generated/easyconfigs/g/gnuplot/#540-for-2020a","text":"We had to use preconfigopts to change the LIBS variable to avoid the error message about libiconv_open.","title":"5.4.0 for 2020a"},{"location":"generated/easyconfigs/h/HTSeq/","text":"HTSeq instructions HTSeq on PyPi HTSeq documentation on readthedocs HTSeq development on GitHub General information Recent versions do support Python 3 Needs: NumPy Pysam matplotlib Development needs Cython SWIG. SWIG does not seem to be needed though to compile the PyPi sources.","title":"HTSeq instructions"},{"location":"generated/easyconfigs/h/HTSeq/#htseq-instructions","text":"HTSeq on PyPi HTSeq documentation on readthedocs HTSeq development on GitHub","title":"HTSeq instructions"},{"location":"generated/easyconfigs/h/HTSeq/#general-information","text":"Recent versions do support Python 3 Needs: NumPy Pysam matplotlib Development needs Cython SWIG. SWIG does not seem to be needed though to compile the PyPi sources.","title":"General information"},{"location":"generated/easyconfigs/h/HTSlib/","text":"HTSlib instructions HTSlib home page Development on GitHub General information HTSlib is the underlying library of SAMtools The build process creates three binaries, a shared and a static library. HTSlib dependencies: zlib libbz2 liblzma libcurl: Optional but recommended for network access. libcrypto: We always take this one from the OS to ensure that security patches are applied. EasyBuild There is support for HTSlib in the EasyBuilders repository which we used as a starting point for our EasyConfig files. Version 1.10.2 in the 2020a toolchain EasyConfig developed to prepare for inclusion in the BioTools bundle. We added some configopts to make clear which configuration we build.","title":"HTSlib instructions"},{"location":"generated/easyconfigs/h/HTSlib/#htslib-instructions","text":"HTSlib home page Development on GitHub","title":"HTSlib instructions"},{"location":"generated/easyconfigs/h/HTSlib/#general-information","text":"HTSlib is the underlying library of SAMtools The build process creates three binaries, a shared and a static library. HTSlib dependencies: zlib libbz2 liblzma libcurl: Optional but recommended for network access. libcrypto: We always take this one from the OS to ensure that security patches are applied.","title":"General information"},{"location":"generated/easyconfigs/h/HTSlib/#easybuild","text":"There is support for HTSlib in the EasyBuilders repository which we used as a starting point for our EasyConfig files.","title":"EasyBuild"},{"location":"generated/easyconfigs/h/HTSlib/#version-1102-in-the-2020a-toolchain","text":"EasyConfig developed to prepare for inclusion in the BioTools bundle. We added some configopts to make clear which configuration we build.","title":"Version 1.10.2 in the 2020a toolchain"},{"location":"generated/easyconfigs/i/IOzone/","text":"IOzone EasyConfig This EasyConfig is derived from the standard EasyBuilders EasyConfig. As it really spends time doing I/O, it is simply compiled with the system compiler. Notes IOzone uses by default rsh to start processes on other nodes, which typically will not work on clusters. This is solved by setting the environment variable RSH to ssh which we do in the module file to avoid user error.","title":"IOzone EasyConfig"},{"location":"generated/easyconfigs/i/IOzone/#iozone-easyconfig","text":"This EasyConfig is derived from the standard EasyBuilders EasyConfig. As it really spends time doing I/O, it is simply compiled with the system compiler.","title":"IOzone EasyConfig"},{"location":"generated/easyconfigs/i/IOzone/#notes","text":"IOzone uses by default rsh to start processes on other nodes, which typically will not work on clusters. This is solved by setting the environment variable RSH to ssh which we do in the module file to avoid user error.","title":"Notes"},{"location":"generated/easyconfigs/i/IntelPython3/","text":"IntelPython3 instructions At UAntwerp, we do not install the Intel compilers using EasyBuild, but use the Intel procedure. Hence the EasyConfig files in this directory are just dummies used to create the module files. Note that we only use the .0x.eb files to install module files, and we put them in a directory of architecture-neutral files. The -yyyy[a|b].eb files are only used within EasyBuild. For the modules we simply use symbolic links to the appropriate .0x module file.","title":"IntelPython3 instructions"},{"location":"generated/easyconfigs/i/IntelPython3/#intelpython3-instructions","text":"At UAntwerp, we do not install the Intel compilers using EasyBuild, but use the Intel procedure. Hence the EasyConfig files in this directory are just dummies used to create the module files. Note that we only use the .0x.eb files to install module files, and we put them in a directory of architecture-neutral files. The -yyyy[a|b].eb files are only used within EasyBuild. For the modules we simply use symbolic links to the appropriate .0x module file.","title":"IntelPython3 instructions"},{"location":"generated/easyconfigs/i/IntelPython3-Packages/","text":"IntelPython3-Packages These EasyBuild recipes install the packages that are included in the regular Python 3 packages in this repositoriy but not in the releases of Intel Python in those toolchains. So after loading IntelPython3-Packages, you should get Intel Python 3 and a set of packages that is a superset of those in the regular Python module(s) of that toolchain, though some versions of packages may be older as they are taken from the Intel Distribution for Python. 2020a modules Used the Intel compiler rather than GCC. This did solve problems with tables. Needed to use HDF5 1.10 instead of HDF5 1.12, even though the 1.12 version worked in our regular Python module...","title":"IntelPython3-Packages"},{"location":"generated/easyconfigs/i/IntelPython3-Packages/#intelpython3-packages","text":"These EasyBuild recipes install the packages that are included in the regular Python 3 packages in this repositoriy but not in the releases of Intel Python in those toolchains. So after loading IntelPython3-Packages, you should get Intel Python 3 and a set of packages that is a superset of those in the regular Python module(s) of that toolchain, though some versions of packages may be older as they are taken from the Intel Distribution for Python.","title":"IntelPython3-Packages"},{"location":"generated/easyconfigs/i/IntelPython3-Packages/#2020a-modules","text":"Used the Intel compiler rather than GCC. This did solve problems with tables. Needed to use HDF5 1.10 instead of HDF5 1.12, even though the 1.12 version worked in our regular Python module...","title":"2020a modules"},{"location":"generated/easyconfigs/i/i-PI/","text":"i-PI instructions i-PI is pure Python code, but it does have a number of Python dependencies that are not part of our standard Python modules. It is meant to be used with various computational chemistry packages, such as CP2K. i-PI web site Manual of the latest version i-PI on GitHub GitHub releases i-PI and CP2K EasyConfigs No support for i-PI was found in the EasyBuild distribution (version 4.3.2 at time of first install). Version 2.3.0 in the 2020a toolchain Installed for Python 3.8.3. The dependencies were first checked by checking the requirements.txt file and installing them manually via pip , to also find the dependencies of dependencies and to ensure that all are installed properly.","title":"i-PI instructions"},{"location":"generated/easyconfigs/i/i-PI/#i-pi-instructions","text":"i-PI is pure Python code, but it does have a number of Python dependencies that are not part of our standard Python modules. It is meant to be used with various computational chemistry packages, such as CP2K. i-PI web site Manual of the latest version i-PI on GitHub GitHub releases i-PI and CP2K","title":"i-PI instructions"},{"location":"generated/easyconfigs/i/i-PI/#easyconfigs","text":"No support for i-PI was found in the EasyBuild distribution (version 4.3.2 at time of first install).","title":"EasyConfigs"},{"location":"generated/easyconfigs/i/i-PI/#version-230-in-the-2020a-toolchain","text":"Installed for Python 3.8.3. The dependencies were first checked by checking the requirements.txt file and installing them manually via pip , to also find the dependencies of dependencies and to ensure that all are installed properly.","title":"Version 2.3.0 in the 2020a toolchain"},{"location":"generated/easyconfigs/j/Java/","text":"Java instructions Java home page OpenJDK home page Adoptium project The current Java modules at UAntwerpen are taken from the OpenJDK distribution Adoptium managed by the Eclipse community. The Adoptium Temurin builds are recommended by whichjdk.com . We use the standard EasyBuild recipes with enhanced documentation. The purpose of this setup is to make it easy to install new subversions that fix security problems without having to change any of the EasyConfigs of packages that use Java, yet have a fully automated installation procedure. So EasyConfigs should refer to, e.g., Java/11 as a dependency and not, e.g., Java/11.0.16, and users should also simply use Java/11 to load whatever Jave 11 version is considered to be the safer one by system staff. Note that Java distinguishes between long tem support versions and feature versions. We prefer to only offer long term support versions in the central software stack, but users are free to install other versions via the EasyBuild-user module. As of October 2022, the LTS versions are: 8: Official unpaid support ended, though some distributions still provide new patches. Not recommended anymore. 11: Launched in September 2018 17: Launched in September 2021 EasyBuild Java support in the EasyBuilders repository Java/8 (currently 8.345) for 2020a Standard EasyConfigs with some improvements to the documentation. Java/11 (currently 11.0.16) for 2020a and 2022a Standard EasyConfigs with some improvements to the documentation. Java/17 (currently 17.0.4) for 2022a Standard EasyConfigs with some improvements to the documentation.","title":"Java instructions"},{"location":"generated/easyconfigs/j/Java/#java-instructions","text":"Java home page OpenJDK home page Adoptium project The current Java modules at UAntwerpen are taken from the OpenJDK distribution Adoptium managed by the Eclipse community. The Adoptium Temurin builds are recommended by whichjdk.com . We use the standard EasyBuild recipes with enhanced documentation. The purpose of this setup is to make it easy to install new subversions that fix security problems without having to change any of the EasyConfigs of packages that use Java, yet have a fully automated installation procedure. So EasyConfigs should refer to, e.g., Java/11 as a dependency and not, e.g., Java/11.0.16, and users should also simply use Java/11 to load whatever Jave 11 version is considered to be the safer one by system staff. Note that Java distinguishes between long tem support versions and feature versions. We prefer to only offer long term support versions in the central software stack, but users are free to install other versions via the EasyBuild-user module. As of October 2022, the LTS versions are: 8: Official unpaid support ended, though some distributions still provide new patches. Not recommended anymore. 11: Launched in September 2018 17: Launched in September 2021","title":"Java instructions"},{"location":"generated/easyconfigs/j/Java/#easybuild","text":"Java support in the EasyBuilders repository","title":"EasyBuild"},{"location":"generated/easyconfigs/j/Java/#java8-currently-8345-for-2020a","text":"Standard EasyConfigs with some improvements to the documentation.","title":"Java/8 (currently 8.345) for 2020a"},{"location":"generated/easyconfigs/j/Java/#java11-currently-11016-for-2020a-and-2022a","text":"Standard EasyConfigs with some improvements to the documentation.","title":"Java/11 (currently 11.0.16) for 2020a and 2022a"},{"location":"generated/easyconfigs/j/Java/#java17-currently-1704-for-2022a","text":"Standard EasyConfigs with some improvements to the documentation.","title":"Java/17 (currently 17.0.4) for 2022a"},{"location":"generated/easyconfigs/j/Jmol/README.Jmol/","text":"Jmol instructions Links: * Jmol web site (and this page can be accessed as www.jmol.org also) * Jmol documentation is integrated on the web site. At this time (December 2019) the link to the Wiki []wiki.jmol.org](http://wiki.jmol.org/index.php/Literature) does not work. * Actual repository on SourceForge * Somewhere in the SourceForge pages it is said that development is moving to GitHub . From the summary page on SourceForge: Jmol/JSmol is a molecular viewer for 3D chemical structures that runs in four independent modes: an HTML5-only web application utilizing jQuery, a Java applet, a stand-alone Java program (Jmol.jar), and a \"headless\" server-side component (JmolData.jar). Jmol can read many file types, including PDB, CIF, SDF, MOL, PyMOL PSE files, and Spartan files, as well as output from Gaussian, GAMESS, MOPAC, VASP, CRYSTAL, CASTEP, QuantumEspresso, VMD, and many other quantum chemistry programs. Files can be transferred directly from several databases, including RCSB, EDS, NCI, PubChem, and MaterialsProject. Multiple files can be loaded and compared. A rich scripting language and a well-developed web API allow easy customization of the user interface. Features include interactive animation and linear morphing. Jmol interfaces well with JSpecView for spectroscopy, JSME for 2D->3D conversion, POV-Ray for images, and CAD programs for 3D printing (VRML export). However, since the login and visualisation nodes of our cluster are not meant to offer web services, the EasyConfig files in this repository only install the stand-alone Java program and \"headless\" component JmolData.jar. The shell scripts to start Jmol are also included.","title":"Jmol instructions"},{"location":"generated/easyconfigs/j/Jmol/README.Jmol/#jmol-instructions","text":"Links: * Jmol web site (and this page can be accessed as www.jmol.org also) * Jmol documentation is integrated on the web site. At this time (December 2019) the link to the Wiki []wiki.jmol.org](http://wiki.jmol.org/index.php/Literature) does not work. * Actual repository on SourceForge * Somewhere in the SourceForge pages it is said that development is moving to GitHub . From the summary page on SourceForge: Jmol/JSmol is a molecular viewer for 3D chemical structures that runs in four independent modes: an HTML5-only web application utilizing jQuery, a Java applet, a stand-alone Java program (Jmol.jar), and a \"headless\" server-side component (JmolData.jar). Jmol can read many file types, including PDB, CIF, SDF, MOL, PyMOL PSE files, and Spartan files, as well as output from Gaussian, GAMESS, MOPAC, VASP, CRYSTAL, CASTEP, QuantumEspresso, VMD, and many other quantum chemistry programs. Files can be transferred directly from several databases, including RCSB, EDS, NCI, PubChem, and MaterialsProject. Multiple files can be loaded and compared. A rich scripting language and a well-developed web API allow easy customization of the user interface. Features include interactive animation and linear morphing. Jmol interfaces well with JSpecView for spectroscopy, JSME for 2D->3D conversion, POV-Ray for images, and CAD programs for 3D printing (VRML export). However, since the login and visualisation nodes of our cluster are not meant to offer web services, the EasyConfig files in this repository only install the stand-alone Java program and \"headless\" component JmolData.jar. The shell scripts to start Jmol are also included.","title":"Jmol instructions"},{"location":"generated/easyconfigs/k/Kraken2/","text":"Kraken2 instructions Kraken2 web site Kraken2 on GitHub Releases via GitHub General information None. EasyBuild Developement of this documentation started for the 2020a toolchains. Kraken2 was in beta at that time and at version 2.0.9. There is EasyBuild support for Kraken2 . 2.0.9-beta EasyConfig development started from the EasyBuilders file for version 2.008-beta Switched to the Intel toolchain (from gompi) Needed to edit the sed-command as the Makefile moved to a different directory.","title":"Kraken2 instructions"},{"location":"generated/easyconfigs/k/Kraken2/#kraken2-instructions","text":"Kraken2 web site Kraken2 on GitHub Releases via GitHub","title":"Kraken2 instructions"},{"location":"generated/easyconfigs/k/Kraken2/#general-information","text":"None.","title":"General information"},{"location":"generated/easyconfigs/k/Kraken2/#easybuild","text":"Developement of this documentation started for the 2020a toolchains. Kraken2 was in beta at that time and at version 2.0.9. There is EasyBuild support for Kraken2 .","title":"EasyBuild"},{"location":"generated/easyconfigs/k/Kraken2/#209-beta","text":"EasyConfig development started from the EasyBuilders file for version 2.008-beta Switched to the Intel toolchain (from gompi) Needed to edit the sed-command as the Makefile moved to a different directory.","title":"2.0.9-beta"},{"location":"generated/easyconfigs/k/kim-api/","text":"kim-api installation instructions. The KIM API is the API to use the OpenKIM models database from within applications. OpenKIM web site API download on that site kim-api on GitHub GitHub releases General instructions Installing the full OpenKIM collection requires two packages: * kim-api which provide the API * openkim-models: If you want to provide a system installation of the default models database. The information in the EasyBuild kim-api recipes suggest that CMake is needed for the model installation, hence likely used by the kim-api-collections-management command. EasyBuild There is support for kom-api in the EasyBuilders repository . We did modify the help text to make it relevant to the users at UAntwerpen. kim-api 2.2.1 in intel 2020a We kept the download link used by the default EasyBuild recipes. It is also possible to download straight from GitHub. Instead of CMake we add our buildtools module to the list of dependencies. CMake is needed to install models.","title":"kim-api installation instructions."},{"location":"generated/easyconfigs/k/kim-api/#kim-api-installation-instructions","text":"The KIM API is the API to use the OpenKIM models database from within applications. OpenKIM web site API download on that site kim-api on GitHub GitHub releases","title":"kim-api installation instructions."},{"location":"generated/easyconfigs/k/kim-api/#general-instructions","text":"Installing the full OpenKIM collection requires two packages: * kim-api which provide the API * openkim-models: If you want to provide a system installation of the default models database. The information in the EasyBuild kim-api recipes suggest that CMake is needed for the model installation, hence likely used by the kim-api-collections-management command.","title":"General instructions"},{"location":"generated/easyconfigs/k/kim-api/#easybuild","text":"There is support for kom-api in the EasyBuilders repository . We did modify the help text to make it relevant to the users at UAntwerpen.","title":"EasyBuild"},{"location":"generated/easyconfigs/k/kim-api/#kim-api-221-in-intel-2020a","text":"We kept the download link used by the default EasyBuild recipes. It is also possible to download straight from GitHub. Instead of CMake we add our buildtools module to the list of dependencies. CMake is needed to install models.","title":"kim-api 2.2.1 in intel 2020a"},{"location":"generated/easyconfigs/l/LAMMPS/","text":"LAMMPS instructions LAMMPS web site at SANDIA National Labs LAMMPS development on GitHub GitHub releases General information LAMMPS is configured by first doing make with a number of yes- and/or no- commands. There are also a number of generics that enable/disable all packages simultaneously. So the order is important! The LAMMPS sources consist of two big parts: lib/* subdirectories: Various optional libraries for LAMMPS that need to be build AFTER the configuration of LAMMPS (see below) and BEFORE LAAMMPS is actually build src subdirectory: The main LAMMPS sources where the whole build process of the executables (or libraries, as LAMMPS can also be build as library) take place LAMMPS packages also have corresponding subdirectories However, LAMMPS can also be extended by simply adding C++ source files and corresponding header files, a feature used by UAntwerp users to add 4 sets of 2 source files. PLUMED has a command to copy the necessary files to define a PLUMED package to the LAMMPS sources. We did notice that the PLUMED integration doesn't always work and my require pairing certain specific version of PLUMED with specific versions of LAMMPS. So not all of our modules include PLUMED support. LAMMPS packages: There is a lot of useful information in the Include packages in build page of the manual COMPRESS: Needs libz. KIM: Needs kim-api version 2 and libcurl. MESSAGE: ZeroMQ library needed (also known as zmq) NOTE: Not yet included as a dependency in the EasyConfigsfor 3Mar2020, so likely taken from the system. PYTHON: Speaks for itself, needs a Python installation with working shared libraries. The documentation of LAMMPS 10Feb2021 still claims that this should be Python 2??? VORONOI: Requires the Voro++ library USER-MOLFILE: May need VMD molfile plugin header files ( VMD is a visualisation package). LAMMPS ships with defaults but they may work or not work with the version of VMD installed on a system. USER-NETCDF needs netCDF installed but should auto-detect the libraries. EasyBuild We started the development of this information from our internal documentation when installing with the 2020a toolchains. There is support for LAMMPS in the standard EasyBuild repository . It uses a specific EasyBlock for LAMMPS . We did not always rely on that EasyBlock and instead often did a more manual install directly steering the build process from the EasyConfig file. Version 11Aug2017 for 2020a This is a straight port of the module that we compiled to the 2020a toolchains. We used the patch mechanism to inject a number of files specific to the UAntwerp installation. The remainder of the work was done with a MakeCp -based EasyConfig that first executes a sequence of make yes/no-... - commands to configure LAMMPS before doing the actual build. The build in this EasyConfig is done as a single build, but it actually has three phases: Build a parallel version Build a sequential version Build a number of extra tools. We do rely on the OS-provided Python for this build. Version 3Mar2020 for Intel 2020a NOTE : Compile problems on CentOS 7, so only installed on CentOS 8 machines. Started from the EasyBuilders file for this version, but changed the dependencies. So it now uses the LAMMPS EasyBlock rather than the MakeCp EasyBlock that we used before. Added some options to configopts to also build some tools that might be needed. The LAMMPS EasyBlock will build the documentation by default which will fail as we do not have sphinx-build installed in the 2020a toolchains (after all, we don't run a web server to give users access to the web documentation). Therefore we had to explicitly add -DBUILD_DOC=no to configopts as otherwise the EasyBlock will enable the documentation build. Problem: The CMake USER-INTEL.cmake file forces the use of -xHost . We removed that from that file using sed in preconfigopts . Problem: Kokkos build did not work. Kokkos produced error messages about header files that Intel takes from GCC, so we disabled kokkos. Problems with USER-packages: PLUMED: The code fails to add the necessary libraries to the linker command line and as a result produces error messages that plumed_create , plumed_finalize and plumed_cmd_nothrow are not defined. These symbols are defined in the PLUMED-library libplumedWrapper.a . The solution was to change the PLUMED mode to shared by adding -DPLUMED_MODE=shared to the CMake command line.","title":"LAMMPS instructions"},{"location":"generated/easyconfigs/l/LAMMPS/#lammps-instructions","text":"LAMMPS web site at SANDIA National Labs LAMMPS development on GitHub GitHub releases","title":"LAMMPS instructions"},{"location":"generated/easyconfigs/l/LAMMPS/#general-information","text":"LAMMPS is configured by first doing make with a number of yes- and/or no- commands. There are also a number of generics that enable/disable all packages simultaneously. So the order is important! The LAMMPS sources consist of two big parts: lib/* subdirectories: Various optional libraries for LAMMPS that need to be build AFTER the configuration of LAMMPS (see below) and BEFORE LAAMMPS is actually build src subdirectory: The main LAMMPS sources where the whole build process of the executables (or libraries, as LAMMPS can also be build as library) take place LAMMPS packages also have corresponding subdirectories However, LAMMPS can also be extended by simply adding C++ source files and corresponding header files, a feature used by UAntwerp users to add 4 sets of 2 source files. PLUMED has a command to copy the necessary files to define a PLUMED package to the LAMMPS sources. We did notice that the PLUMED integration doesn't always work and my require pairing certain specific version of PLUMED with specific versions of LAMMPS. So not all of our modules include PLUMED support. LAMMPS packages: There is a lot of useful information in the Include packages in build page of the manual COMPRESS: Needs libz. KIM: Needs kim-api version 2 and libcurl. MESSAGE: ZeroMQ library needed (also known as zmq) NOTE: Not yet included as a dependency in the EasyConfigsfor 3Mar2020, so likely taken from the system. PYTHON: Speaks for itself, needs a Python installation with working shared libraries. The documentation of LAMMPS 10Feb2021 still claims that this should be Python 2??? VORONOI: Requires the Voro++ library USER-MOLFILE: May need VMD molfile plugin header files ( VMD is a visualisation package). LAMMPS ships with defaults but they may work or not work with the version of VMD installed on a system. USER-NETCDF needs netCDF installed but should auto-detect the libraries.","title":"General information"},{"location":"generated/easyconfigs/l/LAMMPS/#easybuild","text":"We started the development of this information from our internal documentation when installing with the 2020a toolchains. There is support for LAMMPS in the standard EasyBuild repository . It uses a specific EasyBlock for LAMMPS . We did not always rely on that EasyBlock and instead often did a more manual install directly steering the build process from the EasyConfig file.","title":"EasyBuild"},{"location":"generated/easyconfigs/l/LAMMPS/#version-11aug2017-for-2020a","text":"This is a straight port of the module that we compiled to the 2020a toolchains. We used the patch mechanism to inject a number of files specific to the UAntwerp installation. The remainder of the work was done with a MakeCp -based EasyConfig that first executes a sequence of make yes/no-... - commands to configure LAMMPS before doing the actual build. The build in this EasyConfig is done as a single build, but it actually has three phases: Build a parallel version Build a sequential version Build a number of extra tools. We do rely on the OS-provided Python for this build.","title":"Version 11Aug2017 for 2020a"},{"location":"generated/easyconfigs/l/LAMMPS/#version-3mar2020-for-intel-2020a","text":"NOTE : Compile problems on CentOS 7, so only installed on CentOS 8 machines. Started from the EasyBuilders file for this version, but changed the dependencies. So it now uses the LAMMPS EasyBlock rather than the MakeCp EasyBlock that we used before. Added some options to configopts to also build some tools that might be needed. The LAMMPS EasyBlock will build the documentation by default which will fail as we do not have sphinx-build installed in the 2020a toolchains (after all, we don't run a web server to give users access to the web documentation). Therefore we had to explicitly add -DBUILD_DOC=no to configopts as otherwise the EasyBlock will enable the documentation build. Problem: The CMake USER-INTEL.cmake file forces the use of -xHost . We removed that from that file using sed in preconfigopts . Problem: Kokkos build did not work. Kokkos produced error messages about header files that Intel takes from GCC, so we disabled kokkos. Problems with USER-packages: PLUMED: The code fails to add the necessary libraries to the linker command line and as a result produces error messages that plumed_create , plumed_finalize and plumed_cmd_nothrow are not defined. These symbols are defined in the PLUMED-library libplumedWrapper.a . The solution was to change the PLUMED mode to shared by adding -DPLUMED_MODE=shared to the CMake command line.","title":"Version 3Mar2020 for Intel 2020a"},{"location":"generated/easyconfigs/l/LMDB/","text":"LMDB instructions LMDB web site GitHub mirror of the code General information LMDB comes without a configure script, only a Makefile The Makefile does support make install but of course prefix needs to be redefined. The Makefile doesn't honour compiler-related environment flags. Hence the need to redefine CC and OPT when calling make to build the code. Re-defining CFLAGS may be dangerous as the Makefile doesn't only use the optimization options but adds various options that are necessary. EasyBuild information This information starts with our 0.9.24 EasyConfigs There is support for LMDB in the EasyBuilders repositories. However, that support uses the MakeCp generic EasyBlock rather than the install target in the Makefile. It also copies midl.h which on inspection is only used internally in LMDB and not copies by the install Makefile target. 0.9.22 Our EasyConfigs were based on those from the EasyBuilders repositories 0.9.24 in the 2020a toolchains. Moved to baselibs to reduce module clutter Switched to a ConfigureMake generic EasyBlock to use the install target from the Makefile. Required skipping the configure step Required adding the definition of prefix when calling make install . Required a correction of the sanity_check from the EasyBuilders recipes as midl.h is not installed.","title":"LMDB instructions"},{"location":"generated/easyconfigs/l/LMDB/#lmdb-instructions","text":"LMDB web site GitHub mirror of the code","title":"LMDB instructions"},{"location":"generated/easyconfigs/l/LMDB/#general-information","text":"LMDB comes without a configure script, only a Makefile The Makefile does support make install but of course prefix needs to be redefined. The Makefile doesn't honour compiler-related environment flags. Hence the need to redefine CC and OPT when calling make to build the code. Re-defining CFLAGS may be dangerous as the Makefile doesn't only use the optimization options but adds various options that are necessary.","title":"General information"},{"location":"generated/easyconfigs/l/LMDB/#easybuild-information","text":"This information starts with our 0.9.24 EasyConfigs There is support for LMDB in the EasyBuilders repositories. However, that support uses the MakeCp generic EasyBlock rather than the install target in the Makefile. It also copies midl.h which on inspection is only used internally in LMDB and not copies by the install Makefile target.","title":"EasyBuild information"},{"location":"generated/easyconfigs/l/LMDB/#0922","text":"Our EasyConfigs were based on those from the EasyBuilders repositories","title":"0.9.22"},{"location":"generated/easyconfigs/l/LMDB/#0924-in-the-2020a-toolchains","text":"Moved to baselibs to reduce module clutter Switched to a ConfigureMake generic EasyBlock to use the install target from the Makefile. Required skipping the configure step Required adding the definition of prefix when calling make install . Required a correction of the sanity_check from the EasyBuilders recipes as midl.h is not installed.","title":"0.9.24 in the 2020a toolchains."},{"location":"generated/easyconfigs/l/libgd/","text":"libgd installation instructions libgd web site libgd on GitHub EasyConfigs 2.3.0 for Intel 2020a This version was just out when we installed it. TODO: It looks like libgd goes searching for some libraries that we do not yet include as a dependency or have installed on our system. Check if they can be useful. The consequence is likely that it will move further down in the build chain.","title":"libgd installation instructions"},{"location":"generated/easyconfigs/l/libgd/#libgd-installation-instructions","text":"libgd web site libgd on GitHub","title":"libgd installation instructions"},{"location":"generated/easyconfigs/l/libgd/#easyconfigs","text":"","title":"EasyConfigs"},{"location":"generated/easyconfigs/l/libgd/#230-for-intel-2020a","text":"This version was just out when we installed it. TODO: It looks like libgd goes searching for some libraries that we do not yet include as a dependency or have installed on our system. Check if they can be useful. The consequence is likely that it will move further down in the build chain.","title":"2.3.0 for Intel 2020a"},{"location":"generated/easyconfigs/l/libogg/","text":"libogg instructions libogg web site downloads via the web site libogg on GitHub libogg releases on GitHub General information libogg has a configure - make - make install build process. EasyConfig No support was found in EasyBuild at the moment of the first installation, so we developed our own EasyConfig. Some compiler options are coded in the configure script. There is one which we expected might cause warnings even with gcc and removed that one (-O20) using sed, but we did not touch the other flags. The ones specified by EasyBuild are added to them at the end. Integrated into the baselibs module from the 2020a toolchains on, but we did use it in the libsnd bundle in the 2019b toolchains for a particular user.","title":"libogg instructions"},{"location":"generated/easyconfigs/l/libogg/#libogg-instructions","text":"libogg web site downloads via the web site libogg on GitHub libogg releases on GitHub","title":"libogg instructions"},{"location":"generated/easyconfigs/l/libogg/#general-information","text":"libogg has a configure - make - make install build process.","title":"General information"},{"location":"generated/easyconfigs/l/libogg/#easyconfig","text":"No support was found in EasyBuild at the moment of the first installation, so we developed our own EasyConfig. Some compiler options are coded in the configure script. There is one which we expected might cause warnings even with gcc and removed that one (-O20) using sed, but we did not touch the other flags. The ones specified by EasyBuild are added to them at the end. Integrated into the baselibs module from the 2020a toolchains on, but we did use it in the libsnd bundle in the 2019b toolchains for a particular user.","title":"EasyConfig"},{"location":"generated/easyconfigs/l/librosa/","text":"librosa instructions librosa on PyPi librosa documentation librosa on GitHub Releases on GitHub General information Dependencies Python dependencies: SoundFile audioread resampy numba and its dependencies. librosa depends on libsndfile which by itself depends on several sound libraries. librosa depends on FFmpeg via audioread (or GStreamer which we do not have installed) Note that librosa is very picky on its dependencies. It tends to be lagging a bit. E.g., in June 2020, Python 3.8 was not yet supported and the then current version (0.7.2) also did not work with the then current numba (0.49.1). EasyConfig We started from scratch. 2019b toolchain We did not try to bundle libsndfile with the Python packages and instead created a separate module for it. We decided to make a separate module for numba also as this package has a much wider applicability. Moreover, we integrated LLVM into that package. 2020a toolchain - partly postponed We moved libsndfile and the libraries on which it depends into the baselibs package. We also stuck to a separate numba package. Awaiting a new version that supports Python 3.8 and numba 0.49.1 as we do not want to compromise on the software experience of other users or support multiple versions in a toolchain.","title":"librosa instructions"},{"location":"generated/easyconfigs/l/librosa/#librosa-instructions","text":"librosa on PyPi librosa documentation librosa on GitHub Releases on GitHub","title":"librosa instructions"},{"location":"generated/easyconfigs/l/librosa/#general-information","text":"","title":"General information"},{"location":"generated/easyconfigs/l/librosa/#dependencies","text":"Python dependencies: SoundFile audioread resampy numba and its dependencies. librosa depends on libsndfile which by itself depends on several sound libraries. librosa depends on FFmpeg via audioread (or GStreamer which we do not have installed) Note that librosa is very picky on its dependencies. It tends to be lagging a bit. E.g., in June 2020, Python 3.8 was not yet supported and the then current version (0.7.2) also did not work with the then current numba (0.49.1).","title":"Dependencies"},{"location":"generated/easyconfigs/l/librosa/#easyconfig","text":"We started from scratch.","title":"EasyConfig"},{"location":"generated/easyconfigs/l/librosa/#2019b-toolchain","text":"We did not try to bundle libsndfile with the Python packages and instead created a separate module for it. We decided to make a separate module for numba also as this package has a much wider applicability. Moreover, we integrated LLVM into that package.","title":"2019b toolchain"},{"location":"generated/easyconfigs/l/librosa/#2020a-toolchain-partly-postponed","text":"We moved libsndfile and the libraries on which it depends into the baselibs package. We also stuck to a separate numba package. Awaiting a new version that supports Python 3.8 and numba 0.49.1 as we do not want to compromise on the software experience of other users or support multiple versions in a toolchain.","title":"2020a toolchain - partly postponed"},{"location":"generated/easyconfigs/l/libsndfile/","text":"libsnd instructions libsnd web site libsnd on GitHub Releases General information Dependencies libopus FLAC libogg and libvorbis speex is a potential dependency but has not been included since it is marked as obsolete on its web site. ALSA support is also not included since that is not useful on a cluster. SQLite3 is an optional dependency. We did not include it. Building libsnd uses a configure - make - make install build process. There are traces of CMake support in the code, but in version 1.0.28 this does not work properly, or in fact tries to call the autoconf tools internally. One needs to run autogen.sh to generate the configure script. EasyConfig There was no support in the standard EasyBuilders repository at the time of the first install at UAntwerp. 1.0.28 in the 2020a toolchains EasyConfig developed solely with the intention to include in baselibs. 1.0.28 in the 2019b toolchain Bundle together with the dependencies except for libopus which was already included in the baselibs bundle in the 2019b toolchain. Note that we include libsnd twice in the bundle, once to generate the static libraries and once to generate the shared libraries.","title":"libsnd instructions"},{"location":"generated/easyconfigs/l/libsndfile/#libsnd-instructions","text":"libsnd web site libsnd on GitHub Releases","title":"libsnd instructions"},{"location":"generated/easyconfigs/l/libsndfile/#general-information","text":"","title":"General information"},{"location":"generated/easyconfigs/l/libsndfile/#dependencies","text":"libopus FLAC libogg and libvorbis speex is a potential dependency but has not been included since it is marked as obsolete on its web site. ALSA support is also not included since that is not useful on a cluster. SQLite3 is an optional dependency. We did not include it.","title":"Dependencies"},{"location":"generated/easyconfigs/l/libsndfile/#building","text":"libsnd uses a configure - make - make install build process. There are traces of CMake support in the code, but in version 1.0.28 this does not work properly, or in fact tries to call the autoconf tools internally. One needs to run autogen.sh to generate the configure script.","title":"Building"},{"location":"generated/easyconfigs/l/libsndfile/#easyconfig","text":"There was no support in the standard EasyBuilders repository at the time of the first install at UAntwerp.","title":"EasyConfig"},{"location":"generated/easyconfigs/l/libsndfile/#1028-in-the-2020a-toolchains","text":"EasyConfig developed solely with the intention to include in baselibs.","title":"1.0.28 in the 2020a toolchains"},{"location":"generated/easyconfigs/l/libsndfile/#1028-in-the-2019b-toolchain","text":"Bundle together with the dependencies except for libopus which was already included in the baselibs bundle in the 2019b toolchain. Note that we include libsnd twice in the bundle, once to generate the static libraries and once to generate the shared libraries.","title":"1.0.28 in the 2019b toolchain"},{"location":"generated/easyconfigs/l/libtheora/","text":"libtheora instructions libtheora web site downloads via the web site libvorbis on GitHub libvorbis releases on GitHub General information libtheora needs libogg. The optional sample encoder also needs libvorbis. LibSDL is an optional dependency used for the player and not useful for the cluster. libtheora has a configure - make - make install build process. EasyConfig No support was found in EasyBuild at the moment of the first installation, so we developed our own EasyConfig. The autogen.sh script does start configure at the end which is unfortunate in the EasyBuild context. We remove the last line of autogen.sh . We do explicitly disable the search for LibSDL to avoid the associated warning and to make it very clear in the EasyConfig that we did not overlook this dependency. (Well, the option doesn't work in version 1.1.1...) Integrated into the baselibs module from the 2020a toolchains on. Version 1.1.1, 2020a toolchains It turns out that libtheora has problems with our version of libpng, so we stopped the effort to get it to install. The problem occurs while compiling one of the example files, 'png2theora.c'. The bug has been submitted and corrected in February 2014 but has not yet made it into a released version. Hence we decided to disable building the examples until a new version of libtheora becomes available which should fix this problem as it is already corrected in the source code of 'examples/png2theora.c' ( relevant GitHub commit ).","title":"libtheora instructions"},{"location":"generated/easyconfigs/l/libtheora/#libtheora-instructions","text":"libtheora web site downloads via the web site libvorbis on GitHub libvorbis releases on GitHub","title":"libtheora instructions"},{"location":"generated/easyconfigs/l/libtheora/#general-information","text":"libtheora needs libogg. The optional sample encoder also needs libvorbis. LibSDL is an optional dependency used for the player and not useful for the cluster. libtheora has a configure - make - make install build process.","title":"General information"},{"location":"generated/easyconfigs/l/libtheora/#easyconfig","text":"No support was found in EasyBuild at the moment of the first installation, so we developed our own EasyConfig. The autogen.sh script does start configure at the end which is unfortunate in the EasyBuild context. We remove the last line of autogen.sh . We do explicitly disable the search for LibSDL to avoid the associated warning and to make it very clear in the EasyConfig that we did not overlook this dependency. (Well, the option doesn't work in version 1.1.1...) Integrated into the baselibs module from the 2020a toolchains on.","title":"EasyConfig"},{"location":"generated/easyconfigs/l/libtheora/#version-111-2020a-toolchains","text":"It turns out that libtheora has problems with our version of libpng, so we stopped the effort to get it to install. The problem occurs while compiling one of the example files, 'png2theora.c'. The bug has been submitted and corrected in February 2014 but has not yet made it into a released version. Hence we decided to disable building the examples until a new version of libtheora becomes available which should fix this problem as it is already corrected in the source code of 'examples/png2theora.c' ( relevant GitHub commit ).","title":"Version 1.1.1, 2020a toolchains"},{"location":"generated/easyconfigs/l/libvdwxc/","text":"EasyBuild libvdwxc installation notes Home page with documentation GitLab repository Libvdwxc can be downloaded from two different places * Using the tags tab in the GitLab repository * These files do not yet include configure , it can be generated by running ./autogen.sh . * From launchpad.net * Sources are only avaiable as .tar.gz files, not as the often more compact .atr.bz2 or .zip files, but they do already contain the configure script. Dependencies Based on version 0.4.0 MPI support is optional FFTW3 is mandatory It is possible to compile libvdwxc without MPI using MKL and the FFTW3 wrappers from Intel. The MPI version of libvdwxc requires FFTW3 with MPI support which the Intel wrappers do not provide. PFFT is optional and only for MPI builds. The documentation warns that it mostly makes sense for very large problems as in other cases the MPI-version of FFTW3 may very well be faster. Libvdwxc produces only one set of libraries with the same names for the MPI-aware and sequential version. The API is also not split up in MPI and regular versions of the functions, but the MPI version adds 3 functions to initialize the library for MPI. Hence we suspect that using the MPI-aware libvdwxc library may pose problems in sequential codes. EasyConfig development We have chosen to download the sources from GitLab rather than from Launchpad.net as this is also where the source code is maintained. We download the .tar.bz2 version as this is often the most compact file. Specific remarks libvdwxc-0.4.0-intel-2019b-sequential.eb We need to unset MPICC and MPIFC , otherwise the configure process will automatically configure for the MPI-aware version. Just to be sure we also pass --without-mpi to configure , but in fact this generates an error message if MPICC and MPIFC are set. We need to point configure to the Intel FFTW wrapper header files and libraries, which we do through the variables FFTW3_INCLUDES and FFTW3_LIBS . We do also need to include the suitable MKL libraries of course (we use the single threaded ones). libvdwxc-0.4.0-intel-2019b-MPI.eb For this variant of the library, a MPI-aware version of FFTW is needed. Hence we use FFTW itself for this variant rather than the Intel FFTW wrappers for MKL.","title":"EasyBuild libvdwxc installation notes"},{"location":"generated/easyconfigs/l/libvdwxc/#easybuild-libvdwxc-installation-notes","text":"Home page with documentation GitLab repository Libvdwxc can be downloaded from two different places * Using the tags tab in the GitLab repository * These files do not yet include configure , it can be generated by running ./autogen.sh . * From launchpad.net * Sources are only avaiable as .tar.gz files, not as the often more compact .atr.bz2 or .zip files, but they do already contain the configure script.","title":"EasyBuild libvdwxc installation notes"},{"location":"generated/easyconfigs/l/libvdwxc/#dependencies","text":"Based on version 0.4.0 MPI support is optional FFTW3 is mandatory It is possible to compile libvdwxc without MPI using MKL and the FFTW3 wrappers from Intel. The MPI version of libvdwxc requires FFTW3 with MPI support which the Intel wrappers do not provide. PFFT is optional and only for MPI builds. The documentation warns that it mostly makes sense for very large problems as in other cases the MPI-version of FFTW3 may very well be faster. Libvdwxc produces only one set of libraries with the same names for the MPI-aware and sequential version. The API is also not split up in MPI and regular versions of the functions, but the MPI version adds 3 functions to initialize the library for MPI. Hence we suspect that using the MPI-aware libvdwxc library may pose problems in sequential codes.","title":"Dependencies"},{"location":"generated/easyconfigs/l/libvdwxc/#easyconfig-development","text":"We have chosen to download the sources from GitLab rather than from Launchpad.net as this is also where the source code is maintained. We download the .tar.bz2 version as this is often the most compact file.","title":"EasyConfig development"},{"location":"generated/easyconfigs/l/libvdwxc/#specific-remarks","text":"libvdwxc-0.4.0-intel-2019b-sequential.eb We need to unset MPICC and MPIFC , otherwise the configure process will automatically configure for the MPI-aware version. Just to be sure we also pass --without-mpi to configure , but in fact this generates an error message if MPICC and MPIFC are set. We need to point configure to the Intel FFTW wrapper header files and libraries, which we do through the variables FFTW3_INCLUDES and FFTW3_LIBS . We do also need to include the suitable MKL libraries of course (we use the single threaded ones). libvdwxc-0.4.0-intel-2019b-MPI.eb For this variant of the library, a MPI-aware version of FFTW is needed. Hence we use FFTW itself for this variant rather than the Intel FFTW wrappers for MKL.","title":"Specific remarks"},{"location":"generated/easyconfigs/l/libvorbis/","text":"libvorbis instructions libvorbis web site downloads via the web site libvorbis on GitHub libvorbis releases on GitHub General information libvorbis needs libogg. libvorbis has a configure - make - make install build process. EasyConfig No support was found in EasyBuild at the moment of the first installation, so we developed our own EasyConfig. Some compiler options are coded in the configure script. As we use GCC anyway which is what the configure script assumes in its selection of options, we did no effort to remove them using a patch or sed. The compiler flags specified by EasyBuild are added at the end by the configure procedure. Integrated into the baselibs module from the 2020a toolchains on, but we did use it in the libsnd bundle in the 2019b toolchains for a particular user.","title":"libvorbis instructions"},{"location":"generated/easyconfigs/l/libvorbis/#libvorbis-instructions","text":"libvorbis web site downloads via the web site libvorbis on GitHub libvorbis releases on GitHub","title":"libvorbis instructions"},{"location":"generated/easyconfigs/l/libvorbis/#general-information","text":"libvorbis needs libogg. libvorbis has a configure - make - make install build process.","title":"General information"},{"location":"generated/easyconfigs/l/libvorbis/#easyconfig","text":"No support was found in EasyBuild at the moment of the first installation, so we developed our own EasyConfig. Some compiler options are coded in the configure script. As we use GCC anyway which is what the configure script assumes in its selection of options, we did no effort to remove them using a patch or sed. The compiler flags specified by EasyBuild are added at the end by the configure procedure. Integrated into the baselibs module from the 2020a toolchains on, but we did use it in the libsnd bundle in the 2019b toolchains for a particular user.","title":"EasyConfig"},{"location":"generated/easyconfigs/m/MATLAB/","text":"MATLAB instructions MATLAB web site EasyBuild 2020a, 2021a, 2022a These EasyConfigs don't use EasyBuild to install Matlab but only generate a module that enables an installation made elsewhere.","title":"MATLAB instructions"},{"location":"generated/easyconfigs/m/MATLAB/#matlab-instructions","text":"MATLAB web site","title":"MATLAB instructions"},{"location":"generated/easyconfigs/m/MATLAB/#easybuild","text":"","title":"EasyBuild"},{"location":"generated/easyconfigs/m/MATLAB/#2020a-2021a-2022a","text":"These EasyConfigs don't use EasyBuild to install Matlab but only generate a module that enables an installation made elsewhere.","title":"2020a, 2021a, 2022a"},{"location":"generated/easyconfigs/m/MEGAHIT/","text":"MEGAHIT installation instructions MEGAHIT on GitHub MEGAHIT on the Metagenomics wiki General information MEGAHIT is mostly C/C++ code, but the script that starts the code is written in Python. It does seem to work with the standard system Python though. MEGAHIT builds using CMake and Make. The build process only produces a number of executables. MEGAHIT has a built-in test: after loading the module, run megahit --test . Building MEGAHIT fails in EasyBuild when the 'pic'-toolchainopt is True. EasyConfigs There is support for MEGAHIT in the EasyBuilders tree. 1.2.9 for intel 2019b and 2020a We started from a standard EasyBuild recipe for GCCcore but compiled with the Intel compiler. The standard script does contain a multideps to load the Python module. Since the code does not install any packages, its only function is to load a default Python version and to print information about compatible Python modules. Since on our system the megahit script works fine with the system Python, we outcommented that line and instead give some information in our the usage field. Tested by running megahit --test which runs MEGAHIT on a toy dataset. 2020a: Move to the BioTools Bundle.","title":"MEGAHIT installation instructions"},{"location":"generated/easyconfigs/m/MEGAHIT/#megahit-installation-instructions","text":"MEGAHIT on GitHub MEGAHIT on the Metagenomics wiki","title":"MEGAHIT installation instructions"},{"location":"generated/easyconfigs/m/MEGAHIT/#general-information","text":"MEGAHIT is mostly C/C++ code, but the script that starts the code is written in Python. It does seem to work with the standard system Python though. MEGAHIT builds using CMake and Make. The build process only produces a number of executables. MEGAHIT has a built-in test: after loading the module, run megahit --test . Building MEGAHIT fails in EasyBuild when the 'pic'-toolchainopt is True.","title":"General information"},{"location":"generated/easyconfigs/m/MEGAHIT/#easyconfigs","text":"There is support for MEGAHIT in the EasyBuilders tree.","title":"EasyConfigs"},{"location":"generated/easyconfigs/m/MEGAHIT/#129-for-intel-2019b-and-2020a","text":"We started from a standard EasyBuild recipe for GCCcore but compiled with the Intel compiler. The standard script does contain a multideps to load the Python module. Since the code does not install any packages, its only function is to load a default Python version and to print information about compatible Python modules. Since on our system the megahit script works fine with the system Python, we outcommented that line and instead give some information in our the usage field. Tested by running megahit --test which runs MEGAHIT on a toy dataset. 2020a: Move to the BioTools Bundle.","title":"1.2.9 for intel 2019b and 2020a"},{"location":"generated/easyconfigs/m/Maple/","text":"Maple instructions Maple web site EasyBuild 2020.0 These EasyConfigs don't use EasyBuild to install Maple but only generate a module that enables an installation made elsewhere.","title":"Maple instructions"},{"location":"generated/easyconfigs/m/Maple/#maple-instructions","text":"Maple web site","title":"Maple instructions"},{"location":"generated/easyconfigs/m/Maple/#easybuild","text":"","title":"EasyBuild"},{"location":"generated/easyconfigs/m/Maple/#20200","text":"These EasyConfigs don't use EasyBuild to install Maple but only generate a module that enables an installation made elsewhere.","title":"2020.0"},{"location":"generated/easyconfigs/m/Mathematica/","text":"Maple instructions Mathematica web site EasyBuild 12.0 These EasyConfigs don't use EasyBuild to install Mathematica but only generate a module that enables an installation made elsewhere.","title":"Maple instructions"},{"location":"generated/easyconfigs/m/Mathematica/#maple-instructions","text":"Mathematica web site","title":"Maple instructions"},{"location":"generated/easyconfigs/m/Mathematica/#easybuild","text":"","title":"EasyBuild"},{"location":"generated/easyconfigs/m/Mathematica/#120","text":"These EasyConfigs don't use EasyBuild to install Mathematica but only generate a module that enables an installation made elsewhere.","title":"12.0"},{"location":"generated/easyconfigs/m/miniasm/","text":"miniasm instructions miniasm development on GitHub Releases General information Build process: Only a Makefile that needs editing or redefining variables on the make command line, similar to minimap2 which is developed in the same GitHub account. The build process generates two executables, miniasm and minidot . Often used together with minimap2. Development is extremely slow. At this time (June 2020) the most recent version is only 0.3 which was released in July 2018, with the most recent commit in October 2019. There is documentation included in the source files, but only as TeX documents so LaTeX would be needed to build the documentation which makes little sense on a cluster as there is no fast way to open these documents without a GUI. EasyBuild At this time (June 2020) there is no EasyBuilders support for miniasm. Version 0.3 for 2020a EasyConfig developed for inclusion in BioTools and derived from the one for mimimap2 as it uses a similar methodology.","title":"miniasm instructions"},{"location":"generated/easyconfigs/m/miniasm/#miniasm-instructions","text":"miniasm development on GitHub Releases","title":"miniasm instructions"},{"location":"generated/easyconfigs/m/miniasm/#general-information","text":"Build process: Only a Makefile that needs editing or redefining variables on the make command line, similar to minimap2 which is developed in the same GitHub account. The build process generates two executables, miniasm and minidot . Often used together with minimap2. Development is extremely slow. At this time (June 2020) the most recent version is only 0.3 which was released in July 2018, with the most recent commit in October 2019. There is documentation included in the source files, but only as TeX documents so LaTeX would be needed to build the documentation which makes little sense on a cluster as there is no fast way to open these documents without a GUI.","title":"General information"},{"location":"generated/easyconfigs/m/miniasm/#easybuild","text":"At this time (June 2020) there is no EasyBuilders support for miniasm.","title":"EasyBuild"},{"location":"generated/easyconfigs/m/miniasm/#version-03-for-2020a","text":"EasyConfig developed for inclusion in BioTools and derived from the one for mimimap2 as it uses a similar methodology.","title":"Version 0.3 for 2020a"},{"location":"generated/easyconfigs/m/minimap2/","text":"minimap2 instructions Minimap2 on GitHub Check latest release General information As of May 2019, minimap2 is still further developed with frequent commits. Yet the base of the code is ancient and not up to modern standards anymore. There seems to be no form of multi-core parallelism, and though there are SIMD-specific routines in the code, they only go up to SSE4.1 with zero support for the newer AVX2 or AVX512 instruction set extensions. There is no configure process and no make install support in the code. CFLAGS is also hard-coded for the GNU compiler in a way that cannot be overwritten from the environment, only from the make command line. The build process produces three binaries and a static library. EasyBuild support There is support for minimap2 in the EasyBuilders repository . We started from those files, but made a few enhancements, see below. The package is integrated in BioTools at UAntwerp. Version 2.17 in the 2020a toolchains Started from the EasyBuilders recipe for GCCcore but made several modifications Switched to the Intel toolchain Added buildopts to make sure CFLAGS from the EasyBuild environment is used rather than the hard-coded GCC flags that don't optimize for recent architectures. Edit the Makefile via prebuildopts to remove some flags that may in fact turn of certain optimizations for modern CPUs. Build the extra rather than the default target all so that the optional executables minimap2-lite and sdust are also generated.","title":"minimap2 instructions"},{"location":"generated/easyconfigs/m/minimap2/#minimap2-instructions","text":"Minimap2 on GitHub Check latest release","title":"minimap2 instructions"},{"location":"generated/easyconfigs/m/minimap2/#general-information","text":"As of May 2019, minimap2 is still further developed with frequent commits. Yet the base of the code is ancient and not up to modern standards anymore. There seems to be no form of multi-core parallelism, and though there are SIMD-specific routines in the code, they only go up to SSE4.1 with zero support for the newer AVX2 or AVX512 instruction set extensions. There is no configure process and no make install support in the code. CFLAGS is also hard-coded for the GNU compiler in a way that cannot be overwritten from the environment, only from the make command line. The build process produces three binaries and a static library.","title":"General information"},{"location":"generated/easyconfigs/m/minimap2/#easybuild-support","text":"There is support for minimap2 in the EasyBuilders repository . We started from those files, but made a few enhancements, see below. The package is integrated in BioTools at UAntwerp.","title":"EasyBuild support"},{"location":"generated/easyconfigs/m/minimap2/#version-217-in-the-2020a-toolchains","text":"Started from the EasyBuilders recipe for GCCcore but made several modifications Switched to the Intel toolchain Added buildopts to make sure CFLAGS from the EasyBuild environment is used rather than the hard-coded GCC flags that don't optimize for recent architectures. Edit the Makefile via prebuildopts to remove some flags that may in fact turn of certain optimizations for modern CPUs. Build the extra rather than the default target all so that the optional executables minimap2-lite and sdust are also generated.","title":"Version 2.17 in the 2020a toolchains"},{"location":"generated/easyconfigs/m/molmod/","text":"molmod instructions molmod home page at UGent molmod on GitHub Releases molmod documentation General information molmod is a Python package. EasyBuild There is support for molmod in the EasyBuilders repository . Our EasyConfigs are build from the standard EasyBuilder ones 1.4.7 for the 2020a toolchains Build for both our own-compiled Python 3 module and the Intel Python 3 distribution. Updated our own EasyConfig with parameters specified in the EasyBuilder ones. Updated the documentation in the module files. Checked the patch used in the EasyBuilders repository for versions 1.4.5 and 1.4.6. It is not relevant for version 1.4.7.","title":"molmod instructions"},{"location":"generated/easyconfigs/m/molmod/#molmod-instructions","text":"molmod home page at UGent molmod on GitHub Releases molmod documentation","title":"molmod instructions"},{"location":"generated/easyconfigs/m/molmod/#general-information","text":"molmod is a Python package.","title":"General information"},{"location":"generated/easyconfigs/m/molmod/#easybuild","text":"There is support for molmod in the EasyBuilders repository . Our EasyConfigs are build from the standard EasyBuilder ones","title":"EasyBuild"},{"location":"generated/easyconfigs/m/molmod/#147-for-the-2020a-toolchains","text":"Build for both our own-compiled Python 3 module and the Intel Python 3 distribution. Updated our own EasyConfig with parameters specified in the EasyBuilder ones. Updated the documentation in the module files. Checked the patch used in the EasyBuilders repository for versions 1.4.5 and 1.4.6. It is not relevant for version 1.4.7.","title":"1.4.7 for the 2020a toolchains"},{"location":"generated/easyconfigs/m/monitor/","text":"monitor instructions monitor on GitHub General information The monitor package is really just a single Perl-script. The download does contain C-code but that is only for testing and never compiled or so during the installation process. EasyBuild The EasyConfig files were developed in Antwerp.","title":"monitor instructions"},{"location":"generated/easyconfigs/m/monitor/#monitor-instructions","text":"monitor on GitHub","title":"monitor instructions"},{"location":"generated/easyconfigs/m/monitor/#general-information","text":"The monitor package is really just a single Perl-script. The download does contain C-code but that is only for testing and never compiled or so during the installation process.","title":"General information"},{"location":"generated/easyconfigs/m/monitor/#easybuild","text":"The EasyConfig files were developed in Antwerp.","title":"EasyBuild"},{"location":"generated/easyconfigs/n/NAMD/","text":"NAMD instructions NAMD web site Building NAMD can be tricky as it first builds Charm++ and then compiles NAMD with it. EasyBuild NAMD in the EasyBuilders repository Versions 2.12 and 2.14 from binaries UAntwerpen EasyConfig where for now we simply download from binaries after problems trying to build NAMD ourselves. When we tried 2.12 with the version of EasyBuild from 2017, it turned out that it was slower than the downloaded binaries and than our manual builds.","title":"NAMD instructions"},{"location":"generated/easyconfigs/n/NAMD/#namd-instructions","text":"NAMD web site Building NAMD can be tricky as it first builds Charm++ and then compiles NAMD with it.","title":"NAMD instructions"},{"location":"generated/easyconfigs/n/NAMD/#easybuild","text":"NAMD in the EasyBuilders repository","title":"EasyBuild"},{"location":"generated/easyconfigs/n/NAMD/#versions-212-and-214-from-binaries","text":"UAntwerpen EasyConfig where for now we simply download from binaries after problems trying to build NAMD ourselves. When we tried 2.12 with the version of EasyBuild from 2017, it turned out that it was slower than the downloaded binaries and than our manual builds.","title":"Versions 2.12 and 2.14 from binaries"},{"location":"generated/easyconfigs/n/NEST/","text":"NEST installation instructions NEST web site NEST on GitHub (nest-simulator) Install instructions on GitHub Releases NEST documentation on ReadTheDocs NEST installation instructions ReadTheDocs version General instructions NEST supports both OpenMP and MPI parallelism. NEST is build with CMake. There are quite a lot of settings, in some cases a user may have to compile his/her own version because of that. EasyBuild There used to be EasyBuild support for NEST but that was no longer maintained and NEST was removed in version 4. We started from an older EasyConfig for our own development. NEST 2.20.0 for the 2020a toolchains (and 2019b backport) Started from an EasyConfig in the EasyBuild 3.9.4 distribution. We switched to the Intel compilers. We did a check of the options . libneurosim was deliberatly not included as there is no official release of that package at the moment and development seems to have essentially stopped in 2018. MUSIC was not included as we don't even know what this software is or where to find it. Made an OpenMP version and an OpenMP + MPI-version. Note that there is no need to do that via the respective flags in optarch , it can simply by defining certain variables on the CMake command line. Added documentation to the module file. As Boost only seems to be needed for the C++ test suite, we left it disabled (which is the default anyway). Checked bin/nest_vars.sh and added the necessary lines to the EasyConfig to set those variables. Compiler optimization flags CMake does pick up the options from EasyBuild However, the CMake recipe also sets the OpenMP flag itself so we removed that from the EasyBuild options ( openmp set to False in toolchainopts) When specifying the with-optimize flag, the code chose -O2 as the optimization level so that is what we also do with EasyBuild. Since the code itself selects -fp-model=strict we set strict to True in toolchainopts to get the same effect also from EasyBuild. We do remove the flag that is set by CMake using with-intel-compiler-flags We do redefine CMAKE_C_FLAGS_RELEASE to remove the -O3 from it. After all, -O3 with Intel doesn't always improve performance, and it is better to give the full control to EasyBuild. For the 2019b backport, we used a Bundle EasyConfig to also include libtool There is a problem with the CMake recipe: For some reason, when searching for the libraries, it takes the ones from buildtools, even though specified differently and the include files are selected from the right directory. The solution was to manually correct CMAKE_LIBRARY_PATH in the preconfigopts.","title":"NEST installation instructions"},{"location":"generated/easyconfigs/n/NEST/#nest-installation-instructions","text":"NEST web site NEST on GitHub (nest-simulator) Install instructions on GitHub Releases NEST documentation on ReadTheDocs NEST installation instructions ReadTheDocs version","title":"NEST installation instructions"},{"location":"generated/easyconfigs/n/NEST/#general-instructions","text":"NEST supports both OpenMP and MPI parallelism. NEST is build with CMake. There are quite a lot of settings, in some cases a user may have to compile his/her own version because of that.","title":"General instructions"},{"location":"generated/easyconfigs/n/NEST/#easybuild","text":"There used to be EasyBuild support for NEST but that was no longer maintained and NEST was removed in version 4. We started from an older EasyConfig for our own development.","title":"EasyBuild"},{"location":"generated/easyconfigs/n/NEST/#nest-2200-for-the-2020a-toolchains-and-2019b-backport","text":"Started from an EasyConfig in the EasyBuild 3.9.4 distribution. We switched to the Intel compilers. We did a check of the options . libneurosim was deliberatly not included as there is no official release of that package at the moment and development seems to have essentially stopped in 2018. MUSIC was not included as we don't even know what this software is or where to find it. Made an OpenMP version and an OpenMP + MPI-version. Note that there is no need to do that via the respective flags in optarch , it can simply by defining certain variables on the CMake command line. Added documentation to the module file. As Boost only seems to be needed for the C++ test suite, we left it disabled (which is the default anyway). Checked bin/nest_vars.sh and added the necessary lines to the EasyConfig to set those variables. Compiler optimization flags CMake does pick up the options from EasyBuild However, the CMake recipe also sets the OpenMP flag itself so we removed that from the EasyBuild options ( openmp set to False in toolchainopts) When specifying the with-optimize flag, the code chose -O2 as the optimization level so that is what we also do with EasyBuild. Since the code itself selects -fp-model=strict we set strict to True in toolchainopts to get the same effect also from EasyBuild. We do remove the flag that is set by CMake using with-intel-compiler-flags We do redefine CMAKE_C_FLAGS_RELEASE to remove the -O3 from it. After all, -O3 with Intel doesn't always improve performance, and it is better to give the full control to EasyBuild. For the 2019b backport, we used a Bundle EasyConfig to also include libtool There is a problem with the CMake recipe: For some reason, when searching for the libraries, it takes the ones from buildtools, even though specified differently and the include files are selected from the right directory. The solution was to manually correct CMAKE_LIBRARY_PATH in the preconfigopts.","title":"NEST 2.20.0 for the 2020a toolchains (and 2019b backport)"},{"location":"generated/easyconfigs/n/NTPoly/","text":"NTPoly installation instructions Homepage: William-Dawsun github.io Electronic Structure Library github with EasyConfigs EasyConfigs The EasyConfigs are derived from those on the Electronic Structure Library github We did make a change: By doing two iterations we build both static and shared libraries. Though NTPoly does use MPI, it does not need the 'usempi': True in the toolchainopts.","title":"NTPoly installation instructions"},{"location":"generated/easyconfigs/n/NTPoly/#ntpoly-installation-instructions","text":"Homepage: William-Dawsun github.io Electronic Structure Library github with EasyConfigs","title":"NTPoly installation instructions"},{"location":"generated/easyconfigs/n/NTPoly/#easyconfigs","text":"The EasyConfigs are derived from those on the Electronic Structure Library github We did make a change: By doing two iterations we build both static and shared libraries. Though NTPoly does use MPI, it does not need the 'usempi': True in the toolchainopts.","title":"EasyConfigs"},{"location":"generated/easyconfigs/n/NetPyNE/","text":"NetPyNE instructions NetPyNE home page NetPyNE on PyPi NetPyNE development on GitHub General information NetPyNE is a Python package as its name suggests. It is pure Python code; there is no compilation of compiled languages taking place during installation. A GUI is under development in a separate package (NetPyNE-UI) that also needs NEURON. It is based on Jupyter. EasyBuild There is no EasyBuilders support for NetPyNE. 0.9.6 for Intel 2020a Build on top of our Python 3.8.3 distribution and IntelPython3. The installation on our Python 3.8.3 module succeeds but it is not clear if the module will actually work as the setup.py script claims it only supports up to Python 3.7.","title":"NetPyNE instructions"},{"location":"generated/easyconfigs/n/NetPyNE/#netpyne-instructions","text":"NetPyNE home page NetPyNE on PyPi NetPyNE development on GitHub","title":"NetPyNE instructions"},{"location":"generated/easyconfigs/n/NetPyNE/#general-information","text":"NetPyNE is a Python package as its name suggests. It is pure Python code; there is no compilation of compiled languages taking place during installation. A GUI is under development in a separate package (NetPyNE-UI) that also needs NEURON. It is based on Jupyter.","title":"General information"},{"location":"generated/easyconfigs/n/NetPyNE/#easybuild","text":"There is no EasyBuilders support for NetPyNE.","title":"EasyBuild"},{"location":"generated/easyconfigs/n/NetPyNE/#096-for-intel-2020a","text":"Build on top of our Python 3.8.3 distribution and IntelPython3. The installation on our Python 3.8.3 module succeeds but it is not clear if the module will actually work as the setup.py script claims it only supports up to Python 3.7.","title":"0.9.6 for Intel 2020a"},{"location":"generated/easyconfigs/n/netCDF/","text":"netCDF @ UAntwerp The netCDF libraries are distributed in several packages: * netCDF-C is the core library and contains the C API. ( Project on GitHub )] * netCDF-Fortran: Fortran wrappers ( Project on GitHub ) * netCDF-4 C++: C++ wrappers, version 4 (the current C++ API) ( Project on gitHub ) * netCDF-3 C++: The legacy C++-API, included for as long as it still works ( Download site ) Moreover, several back-ends can be added to the netCDF package * HDF5 is the back-end for the newest netCDF file format. Parallel access will be enabled if the HDF5 library supports this. * PnetCDF is a back-end for parallel access when using some of the older netCDF file formats. EasyConfigs The installation at UAntwerp differs from the default EasyBuild way of installing netCDF. All necessary components except HDF5 are combined in a bundle as the libraries are meant to be installed together in a single directory. Our vision at UAntwerp is that it is worse to have longer search paths for binaries, libraries and include files than it is to have a few library files in the search path that a package may not need. Shorter module lists are also a benefit to users. For this, we've been using bundles since the 2016b toolchains. However, initially they only included the C, Fortran and C++ interfaces but not the PnetCDF back-end. 2019b toolchains - netCDF 4.7.0 From the 2019b toolchains on we do a MPI and a non-MPI build. The MPI build includes the PnetCDF back-end and uses a HDF5 module explicitly compiled with MPI (and MPI-IO) support. The non-MPI version does not include the PnetCDF back-end and uses a HDF5 module compiled without support for MPI. This one is safer to use in non-MPI applications as with some MPI implementations a program compiled with libraries that use MPI may hang when started without mpiexec/mpirun depending on how the environment is set up. 2020a toolchains - netCDF 4.7.3 Since it looks like older versions can no longer be downloaded from the UniData site, we switched to downloading from the GitHub for netCDF-C, netCDF-Fortran and netCDF4-CXX. This makes it easier to test with older versions if the newest version doesn't configure or compile correctly. Probably already present before: I noticed that LD_LIBRARY_PATH etc are not set in the proper way in the bundle, refering to a non-existen lib64 directory rather than to lib. Hence we set LD_LIVRARY_PATH, LIBRARY_PATH and to be sure CFILES also in the preconfigopts, prebuildopts and preinstallopts. We made the switch to HDF5 1.10 but skipped 1.12 for now. netCDF-Fortran 4.5.2 ran into configure problems so we stuck to 4.4.5, the last release in the 4.4.x series. It was necessary to set parallel to 1 for building the Fortran interfaces as we observed compile errors that seemed random.","title":"netCDF @ UAntwerp"},{"location":"generated/easyconfigs/n/netCDF/#netcdf-uantwerp","text":"The netCDF libraries are distributed in several packages: * netCDF-C is the core library and contains the C API. ( Project on GitHub )] * netCDF-Fortran: Fortran wrappers ( Project on GitHub ) * netCDF-4 C++: C++ wrappers, version 4 (the current C++ API) ( Project on gitHub ) * netCDF-3 C++: The legacy C++-API, included for as long as it still works ( Download site ) Moreover, several back-ends can be added to the netCDF package * HDF5 is the back-end for the newest netCDF file format. Parallel access will be enabled if the HDF5 library supports this. * PnetCDF is a back-end for parallel access when using some of the older netCDF file formats.","title":"netCDF @ UAntwerp"},{"location":"generated/easyconfigs/n/netCDF/#easyconfigs","text":"The installation at UAntwerp differs from the default EasyBuild way of installing netCDF. All necessary components except HDF5 are combined in a bundle as the libraries are meant to be installed together in a single directory. Our vision at UAntwerp is that it is worse to have longer search paths for binaries, libraries and include files than it is to have a few library files in the search path that a package may not need. Shorter module lists are also a benefit to users. For this, we've been using bundles since the 2016b toolchains. However, initially they only included the C, Fortran and C++ interfaces but not the PnetCDF back-end.","title":"EasyConfigs"},{"location":"generated/easyconfigs/n/netCDF/#2019b-toolchains-netcdf-470","text":"From the 2019b toolchains on we do a MPI and a non-MPI build. The MPI build includes the PnetCDF back-end and uses a HDF5 module explicitly compiled with MPI (and MPI-IO) support. The non-MPI version does not include the PnetCDF back-end and uses a HDF5 module compiled without support for MPI. This one is safer to use in non-MPI applications as with some MPI implementations a program compiled with libraries that use MPI may hang when started without mpiexec/mpirun depending on how the environment is set up.","title":"2019b toolchains - netCDF 4.7.0"},{"location":"generated/easyconfigs/n/netCDF/#2020a-toolchains-netcdf-473","text":"Since it looks like older versions can no longer be downloaded from the UniData site, we switched to downloading from the GitHub for netCDF-C, netCDF-Fortran and netCDF4-CXX. This makes it easier to test with older versions if the newest version doesn't configure or compile correctly. Probably already present before: I noticed that LD_LIBRARY_PATH etc are not set in the proper way in the bundle, refering to a non-existen lib64 directory rather than to lib. Hence we set LD_LIVRARY_PATH, LIBRARY_PATH and to be sure CFILES also in the preconfigopts, prebuildopts and preinstallopts. We made the switch to HDF5 1.10 but skipped 1.12 for now. netCDF-Fortran 4.5.2 ran into configure problems so we stuck to 4.4.5, the last release in the 4.4.x series. It was necessary to set parallel to 1 for building the Fortran interfaces as we observed compile errors that seemed random.","title":"2020a toolchains - netCDF 4.7.3"},{"location":"generated/easyconfigs/n/numba/","text":"numba instructions numba home page numba on PyPi numba on GitHub Note that there are other interesting repositories with examples, documentation, benchmarking code, ... in the numba GitHub account . numba dependencies Numba relies on the LLVM backend. It accesses that backend through the llvmlite package ( code on GitHub ). Each version of llvmlite supports only specific versions of LLVM, so match them based on the information in the LLVM README on GitUb . EasyConfig We developed our own EasyConfig, partially starting from the one for LLVM. Our recipe contains 3 packages: * First a suitable version of LLVM is installed. Supported versions depend on what llvmlite supports at that moment, and those versions are often considerably older than the version that we would want for a Clang/Flang compiler toolchain. * Next the Python llvmlite package is installed. * Finally the Python numba package itself is installed. The options that we use for compiling LLVM are very different from those used in the EasyBuilders recipe for LLVM. They were taken from the scripts of llvmlite to install in a Conda environment . Note that some of the Conda recipes do advise to use certain patches. The patches can be found in the conda-recipes subdirectory of the llvmlite GitHub . Compiling LLVM with those options failed if 'pic' was not set to True (since we didn't have the shared library function turned on, this option was not set automatically). 2020a to0lchains: Numba 0.49.1, llvmlite 0.32.1 and LLVM 8.0.1 (and 2019b backport) It is still impossible to compile LLVM with the Intel compilers so we stuck to GCCcore. However, we've played it dirty and claim that the module belongs in the Intel toolchain to not confuse our users who have to load a combination of GCCcore and Intel modules. Backport to 2019 based on numba 0.48.0 since a program using it turned out to be incompatible with Numba 0.49.1 We did not apply any of the LLVM patches in the conda-recipes subdirectory of the llvmlite GitHub as so far we did not run into compile or link problems or observe problems when using numba. They may be needed though in rare or in future cases.","title":"numba instructions"},{"location":"generated/easyconfigs/n/numba/#numba-instructions","text":"numba home page numba on PyPi numba on GitHub Note that there are other interesting repositories with examples, documentation, benchmarking code, ... in the numba GitHub account .","title":"numba instructions"},{"location":"generated/easyconfigs/n/numba/#numba-dependencies","text":"Numba relies on the LLVM backend. It accesses that backend through the llvmlite package ( code on GitHub ). Each version of llvmlite supports only specific versions of LLVM, so match them based on the information in the LLVM README on GitUb .","title":"numba dependencies"},{"location":"generated/easyconfigs/n/numba/#easyconfig","text":"We developed our own EasyConfig, partially starting from the one for LLVM. Our recipe contains 3 packages: * First a suitable version of LLVM is installed. Supported versions depend on what llvmlite supports at that moment, and those versions are often considerably older than the version that we would want for a Clang/Flang compiler toolchain. * Next the Python llvmlite package is installed. * Finally the Python numba package itself is installed. The options that we use for compiling LLVM are very different from those used in the EasyBuilders recipe for LLVM. They were taken from the scripts of llvmlite to install in a Conda environment . Note that some of the Conda recipes do advise to use certain patches. The patches can be found in the conda-recipes subdirectory of the llvmlite GitHub . Compiling LLVM with those options failed if 'pic' was not set to True (since we didn't have the shared library function turned on, this option was not set automatically).","title":"EasyConfig"},{"location":"generated/easyconfigs/n/numba/#2020a-to0lchains-numba-0491-llvmlite-0321-and-llvm-801-and-2019b-backport","text":"It is still impossible to compile LLVM with the Intel compilers so we stuck to GCCcore. However, we've played it dirty and claim that the module belongs in the Intel toolchain to not confuse our users who have to load a combination of GCCcore and Intel modules. Backport to 2019 based on numba 0.48.0 since a program using it turned out to be incompatible with Numba 0.49.1 We did not apply any of the LLVM patches in the conda-recipes subdirectory of the llvmlite GitHub as so far we did not run into compile or link problems or observe problems when using numba. They may be needed though in rare or in future cases.","title":"2020a to0lchains: Numba 0.49.1, llvmlite 0.32.1 and LLVM 8.0.1 (and 2019b backport)"},{"location":"generated/easyconfigs/o/OpenFOAM/","text":"OpenFOAM installation instructions The organisation behind OpenFOAM is a bit of a mess. One thing is for sure: there is the OpenFOAM Foundation which is a vehicle created by the original developers to promote and manage the free, open source distribution of OpenFOAM when that company (OpenCFD) was sold to Silicon Graphics. However, nowadays two companies maintain a code base and claim to be the prime developer. CFD Direct claims to be the manager and primary developer on behalf of the OpenFOAM Foundation. They release the version with low version numbers (at the time of writing OpenFOAM 7). Development repository on GitHub and there is always a release repository in the OpenFOAM GitHub named after the last version number. OpenCFD Ltd which is in fact the original developer that got first absorbed by SGI but later got sold again to ESI Group. release versions numberd \"vYYMM\" (e.g., v1912). GitLab code repository Both of these companies also provide commercial support for OpenFOAM. General information Dependencies It is very unclear which are the mandatory dependencies and which ones are optional. The list differs depending on where you look in the documentation. Scotch version 6 seems to be mandatory. Metis seems to be a dependency also. I wonder if this is an EasyBuild issue where some EasyBuild developers think that Scotch needs it (while Scotch actually has its built-in version of Metis) FFTW is also an optional dependency according to the system requirements section on OpenFOAM.com. It is not clear if it also recognizes MKL FFT. CGAL : The system requirements section on OpenFOAM.com suggests that this is an optional dependency, but it is not clear which options of CGAL are really needed. For a cluster installation where it will run in batch mode, it looks unlikely the GUI parts will be needed as they can impossibly work in distributed mode. CGAL needs Boost. ParaView : The system requirements on OpenFOAM.com suggest this is an optional component. The build guide on OpenFOAM.com is clear that this can be omitted if using other post-processing software. Building OpenFOAM I found some pages on OpenFOAM.com particularly useful OpenFOAM installation from source with also a link to a system requirements section OpenFOAM.com also includes a build guide Yet I am still missing a page that actually tells how to configure for the compiler on the system, etc. To make life difficult it comes with its own, poorly documented, configuration tool wmake which isn't even called directly but through another script, Allwmake. It looks like several options are set through editing the /etc/bashrc file in the OpenFOAM distribution rather than command line options of the configuration tool... Running OpenFOAM OpenFOAM needs a lot of WM_ and FOAM_ environment variables. Some are already set by the module generated by EasyBuild, but one needs to source $FOAM_BASH or $FOAM_CSH to set the other variables. It doesn't make sense to try to set them all from the module file as those scripts also define functions that cannot be set from the module file anyway but may be needed in other scripts contained in OpenFOAM. Other remarks At the bottom of the system requirements section on OpenFOAM.com there is a remark that says to use LD_PRELOAD when using Intel MPI: export LD_PRELOAD=\"libmpi.so\" . Building with EasyBuild The OpenFOAM EasyBlock The block supports two modes, and it distinguishes between them based on the module name: \"OpenFOAM\" and \"OpenFOAM-Extend\". TODO: Figure out the difference as it is not properly documented. The EasyBlock currently only supports GCC and Intel (April 2020). The code supports a lot more compilers. use_mpi should not be set in the toolchainopts as the EasyBlock needs to know booth the sequential and MPI compilers and used the CC/CXX and MPICC/MPICXX environment variables respectively for that purpose. There is an extensive sanity check in the EasyBlock. The problem is that the EasyBlock itself seems to support versions of the code that do not correspond to the list of files used in the sanity check. It is always possible to avoid the sanity check in the EasyBlock by setting your own sanity_check_files in the EasyConfig file. Looking at the code of the EasyBlock confirms that it tries to set an environment variable to enable a parallel build based on the value of the EasyBuild parameter parallel . However, when monitoring the compile, parallel computation doesn't seem to work. It is not clear if this is a problem with the EasyBlock. (Maybe -j is needed with Allwmake/wmake, though that will overwrite the environment variable that sets the level of parallelism?) Version 4.1, installed in 2017 Did not use ParaView Used a modified CGAL EasyBuild recipe with all OpenGL and Qt components thrown out as they don't make sense on compute nodes Manual intervention after installation: Outcomment the line that loads the settings for ParaView in OpenFOAM-4.1/etc/bashrc. Version 6 with Intel 2019b Official dependencies in ThirdParty-6 Scotch 6.0.6 ParaView 5.4.0 CGAL 4.10 , compiled with Boost 1.55.0 libccmio 2.6.1 : Not clear what this is needed for. We stuck to CGAL 4.10, but used a more recent Boost (1.70.0), used Scotch 6.0.7 which should only be a patchlevel upgrade and hence not really different. Since none of the EasyBuild builds we looked at includes libccmio, we didn't use it. We also include Metis which is an optional dependency. The recipe is a mix of the recipe used on the VSC Tier-1 system and our recipe for version 4.1. We omitted all GUI stuff as in our 4.1 recipe, but used for the dependencies versions that were very close to those used on the Tier-1. This compiles correctly with the Intel 2019b compilers. Version v1912 with the 2019b toolchain. Does not work. There are definitely MPI link problems as the wrappers are not always used, but it is not clear if that is the only problem.","title":"OpenFOAM installation instructions"},{"location":"generated/easyconfigs/o/OpenFOAM/#openfoam-installation-instructions","text":"The organisation behind OpenFOAM is a bit of a mess. One thing is for sure: there is the OpenFOAM Foundation which is a vehicle created by the original developers to promote and manage the free, open source distribution of OpenFOAM when that company (OpenCFD) was sold to Silicon Graphics. However, nowadays two companies maintain a code base and claim to be the prime developer. CFD Direct claims to be the manager and primary developer on behalf of the OpenFOAM Foundation. They release the version with low version numbers (at the time of writing OpenFOAM 7). Development repository on GitHub and there is always a release repository in the OpenFOAM GitHub named after the last version number. OpenCFD Ltd which is in fact the original developer that got first absorbed by SGI but later got sold again to ESI Group. release versions numberd \"vYYMM\" (e.g., v1912). GitLab code repository Both of these companies also provide commercial support for OpenFOAM.","title":"OpenFOAM installation instructions"},{"location":"generated/easyconfigs/o/OpenFOAM/#general-information","text":"","title":"General information"},{"location":"generated/easyconfigs/o/OpenFOAM/#dependencies","text":"It is very unclear which are the mandatory dependencies and which ones are optional. The list differs depending on where you look in the documentation. Scotch version 6 seems to be mandatory. Metis seems to be a dependency also. I wonder if this is an EasyBuild issue where some EasyBuild developers think that Scotch needs it (while Scotch actually has its built-in version of Metis) FFTW is also an optional dependency according to the system requirements section on OpenFOAM.com. It is not clear if it also recognizes MKL FFT. CGAL : The system requirements section on OpenFOAM.com suggests that this is an optional dependency, but it is not clear which options of CGAL are really needed. For a cluster installation where it will run in batch mode, it looks unlikely the GUI parts will be needed as they can impossibly work in distributed mode. CGAL needs Boost. ParaView : The system requirements on OpenFOAM.com suggest this is an optional component. The build guide on OpenFOAM.com is clear that this can be omitted if using other post-processing software.","title":"Dependencies"},{"location":"generated/easyconfigs/o/OpenFOAM/#building-openfoam","text":"I found some pages on OpenFOAM.com particularly useful OpenFOAM installation from source with also a link to a system requirements section OpenFOAM.com also includes a build guide Yet I am still missing a page that actually tells how to configure for the compiler on the system, etc. To make life difficult it comes with its own, poorly documented, configuration tool wmake which isn't even called directly but through another script, Allwmake. It looks like several options are set through editing the /etc/bashrc file in the OpenFOAM distribution rather than command line options of the configuration tool...","title":"Building OpenFOAM"},{"location":"generated/easyconfigs/o/OpenFOAM/#running-openfoam","text":"OpenFOAM needs a lot of WM_ and FOAM_ environment variables. Some are already set by the module generated by EasyBuild, but one needs to source $FOAM_BASH or $FOAM_CSH to set the other variables. It doesn't make sense to try to set them all from the module file as those scripts also define functions that cannot be set from the module file anyway but may be needed in other scripts contained in OpenFOAM.","title":"Running OpenFOAM"},{"location":"generated/easyconfigs/o/OpenFOAM/#other-remarks","text":"At the bottom of the system requirements section on OpenFOAM.com there is a remark that says to use LD_PRELOAD when using Intel MPI: export LD_PRELOAD=\"libmpi.so\" .","title":"Other remarks"},{"location":"generated/easyconfigs/o/OpenFOAM/#building-with-easybuild","text":"","title":"Building with EasyBuild"},{"location":"generated/easyconfigs/o/OpenFOAM/#the-openfoam-easyblock","text":"The block supports two modes, and it distinguishes between them based on the module name: \"OpenFOAM\" and \"OpenFOAM-Extend\". TODO: Figure out the difference as it is not properly documented. The EasyBlock currently only supports GCC and Intel (April 2020). The code supports a lot more compilers. use_mpi should not be set in the toolchainopts as the EasyBlock needs to know booth the sequential and MPI compilers and used the CC/CXX and MPICC/MPICXX environment variables respectively for that purpose. There is an extensive sanity check in the EasyBlock. The problem is that the EasyBlock itself seems to support versions of the code that do not correspond to the list of files used in the sanity check. It is always possible to avoid the sanity check in the EasyBlock by setting your own sanity_check_files in the EasyConfig file. Looking at the code of the EasyBlock confirms that it tries to set an environment variable to enable a parallel build based on the value of the EasyBuild parameter parallel . However, when monitoring the compile, parallel computation doesn't seem to work. It is not clear if this is a problem with the EasyBlock. (Maybe -j is needed with Allwmake/wmake, though that will overwrite the environment variable that sets the level of parallelism?)","title":"The OpenFOAM EasyBlock"},{"location":"generated/easyconfigs/o/OpenFOAM/#version-41-installed-in-2017","text":"Did not use ParaView Used a modified CGAL EasyBuild recipe with all OpenGL and Qt components thrown out as they don't make sense on compute nodes Manual intervention after installation: Outcomment the line that loads the settings for ParaView in OpenFOAM-4.1/etc/bashrc.","title":"Version 4.1, installed in 2017"},{"location":"generated/easyconfigs/o/OpenFOAM/#version-6-with-intel-2019b","text":"Official dependencies in ThirdParty-6 Scotch 6.0.6 ParaView 5.4.0 CGAL 4.10 , compiled with Boost 1.55.0 libccmio 2.6.1 : Not clear what this is needed for. We stuck to CGAL 4.10, but used a more recent Boost (1.70.0), used Scotch 6.0.7 which should only be a patchlevel upgrade and hence not really different. Since none of the EasyBuild builds we looked at includes libccmio, we didn't use it. We also include Metis which is an optional dependency. The recipe is a mix of the recipe used on the VSC Tier-1 system and our recipe for version 4.1. We omitted all GUI stuff as in our 4.1 recipe, but used for the dependencies versions that were very close to those used on the Tier-1. This compiles correctly with the Intel 2019b compilers.","title":"Version 6 with Intel 2019b"},{"location":"generated/easyconfigs/o/OpenFOAM/#version-v1912-with-the-2019b-toolchain","text":"Does not work. There are definitely MPI link problems as the wrappers are not always used, but it is not clear if that is the only problem.","title":"Version v1912 with the 2019b toolchain."},{"location":"generated/easyconfigs/o/OpenMX/","text":"OpenMX instructions OpenMX web site Version check on the Donwload page General information The installation of OpenMX usually requires two files: One with the major.minor release One with the patch which is in fact not a true patchfile but simply a file that you have to untar in the source subdirectory to upgrade the sources to the major.minor.patchlevel version. The build process is very unconventional. There is no configure process. Instead one needs to either edit the makefile to define the variables CC, FC and LIB to point to the C-compiler (with all compiler flags), the Fortran compiler (again with all compiler flags) and the libraries that need to be added respectively, or define those on the command line. If this is done by specifying them on the make command line rather than by editing source/makefile then keep in mind that in the makefile, extra options are added to CC and/or FC (depending on the version of OpenMX) in a separate line which needs to be added by hand when specifying CC and FC on the make command line. With some clever shell syntax to avoid expanding the variable, this can be done on the command line also. The make target all will build the openmx executable and a bunch of utilities and copy them to the directory pointed to by the variable DESTDIR . There is also an install make target, but all it does is run the strip command on the openmx executable and copy it again to DESTDIR . Some included programs are not covered by the all target. Some executables are hard-coded for gcc and generated without using any optimization options. These seem to be small utilities that likely are not performance-critical. EasyBuild This documentation was written when installing for the 2020a toolchains, starting with redoing our 3.8.5 installation for those toolchains. At that time (EasyBuild 4.2.2), there was no support for OpenMX in EasyBuild. 3.8.5 on Intel 2020a toolchain. Older EasyConfigs in this repository required assembling a source file that contained the patches. This is not needed anymore. By specifying both source files and an extract command for the patch this is now fully automated. Since there is no configure step and no working make install that does a complete installation, we use a MakeCp EasyBlock to build OpenMX. We do use DESTDIR though to avoid that the binaries are put in the work subdirectory. From the 2020a toolchains on, we do a full copy of all files except for the sources: The binaries to the 'bin' subdirectory The PDF manual to the 'doc' subdirectory The data directory DFT_DATA13 is copied over unmodified. The contents of the work subdirectory from the distribution is put in the examples subdirectory. We do a sanity check on: The existence of the openmx executable All executables in the UTILS variable in source/makefile The executables check_lead and OpticalConductivityMain that were not covered by the all makefile target. check_lead was not included as it does not compile, and it is very likely to be an error in the code as DFT_DATA_DIR is used without including the include file that defines it, and it can't be meant to be given as a preprocessor symbol as that would break the other files that use that symbol. 3.9.2 on Intel 2020a toolchains There are a lot of new utilty programs included. check_lead is again not compiled by the all target and still does not compile. The default makefile now also has a defintion of MKLROOT in the example configurations that is not commented out. Version 3.9 now uses ELPA (also included) instead of liberi so the includes have to change when specifying CC and FC. All example configurations use the -ip option to enable extra interprocedural optimizations and -no-prec-div for less precise devisions but stick to the default floating point model otherwise. EasyBuild does set a floating point model though. We stick to fully EasyBuild-generated options, but if problems with the accuracy are found, this may be the place to look. The PDF manual is no longer included in the distribution.","title":"OpenMX instructions"},{"location":"generated/easyconfigs/o/OpenMX/#openmx-instructions","text":"OpenMX web site Version check on the Donwload page","title":"OpenMX instructions"},{"location":"generated/easyconfigs/o/OpenMX/#general-information","text":"The installation of OpenMX usually requires two files: One with the major.minor release One with the patch which is in fact not a true patchfile but simply a file that you have to untar in the source subdirectory to upgrade the sources to the major.minor.patchlevel version. The build process is very unconventional. There is no configure process. Instead one needs to either edit the makefile to define the variables CC, FC and LIB to point to the C-compiler (with all compiler flags), the Fortran compiler (again with all compiler flags) and the libraries that need to be added respectively, or define those on the command line. If this is done by specifying them on the make command line rather than by editing source/makefile then keep in mind that in the makefile, extra options are added to CC and/or FC (depending on the version of OpenMX) in a separate line which needs to be added by hand when specifying CC and FC on the make command line. With some clever shell syntax to avoid expanding the variable, this can be done on the command line also. The make target all will build the openmx executable and a bunch of utilities and copy them to the directory pointed to by the variable DESTDIR . There is also an install make target, but all it does is run the strip command on the openmx executable and copy it again to DESTDIR . Some included programs are not covered by the all target. Some executables are hard-coded for gcc and generated without using any optimization options. These seem to be small utilities that likely are not performance-critical.","title":"General information"},{"location":"generated/easyconfigs/o/OpenMX/#easybuild","text":"This documentation was written when installing for the 2020a toolchains, starting with redoing our 3.8.5 installation for those toolchains. At that time (EasyBuild 4.2.2), there was no support for OpenMX in EasyBuild.","title":"EasyBuild"},{"location":"generated/easyconfigs/o/OpenMX/#385-on-intel-2020a-toolchain","text":"Older EasyConfigs in this repository required assembling a source file that contained the patches. This is not needed anymore. By specifying both source files and an extract command for the patch this is now fully automated. Since there is no configure step and no working make install that does a complete installation, we use a MakeCp EasyBlock to build OpenMX. We do use DESTDIR though to avoid that the binaries are put in the work subdirectory. From the 2020a toolchains on, we do a full copy of all files except for the sources: The binaries to the 'bin' subdirectory The PDF manual to the 'doc' subdirectory The data directory DFT_DATA13 is copied over unmodified. The contents of the work subdirectory from the distribution is put in the examples subdirectory. We do a sanity check on: The existence of the openmx executable All executables in the UTILS variable in source/makefile The executables check_lead and OpticalConductivityMain that were not covered by the all makefile target. check_lead was not included as it does not compile, and it is very likely to be an error in the code as DFT_DATA_DIR is used without including the include file that defines it, and it can't be meant to be given as a preprocessor symbol as that would break the other files that use that symbol.","title":"3.8.5 on Intel 2020a toolchain."},{"location":"generated/easyconfigs/o/OpenMX/#392-on-intel-2020a-toolchains","text":"There are a lot of new utilty programs included. check_lead is again not compiled by the all target and still does not compile. The default makefile now also has a defintion of MKLROOT in the example configurations that is not commented out. Version 3.9 now uses ELPA (also included) instead of liberi so the includes have to change when specifying CC and FC. All example configurations use the -ip option to enable extra interprocedural optimizations and -no-prec-div for less precise devisions but stick to the default floating point model otherwise. EasyBuild does set a floating point model though. We stick to fully EasyBuild-generated options, but if problems with the accuracy are found, this may be the place to look. The PDF manual is no longer included in the distribution.","title":"3.9.2 on Intel 2020a toolchains"},{"location":"generated/easyconfigs/p/PLUMED/","text":"PLUMED installation instructions PLUMED web site PLUMED on GitHub General information Environment variables: PLUMED_KERNEL is used by the Python wrappers when they are installed from PyPi outside the regular PLUMED install process (which makes sense as this makes it easier to support multiple versions of Python). See, e.g., PLUMED manual: Installing Python wrappers PLUMED_ROOT : points to PLUMED-related source files that are used by various scripts that rely on PLUMED. This variable and the next ones are discussed on The manual page \"Installation Layout\" . PLUMED_INDLUDEDIR : Points to the include files. PLUMED_HTMLDIR : Points to the HTML documentation in share/doc/plumed, but these files are not installed by the current EasyConfigs. (We don't run make doc ) PLUMED_PROGRAM_NAME : usually plumed. EasyConfigs First version covered by this documentation: The 2019b toolchains. 2019b toolchains: 2.4.7, 2.5.4, 2.6.0 Switched from modextrapaths to modextravars, as PLUMED_ROOT and PLUMED_KERNEL shouldn't be treated as paths if the variables already exist when the module is loaded. We also added other relevant variables.","title":"PLUMED installation instructions"},{"location":"generated/easyconfigs/p/PLUMED/#plumed-installation-instructions","text":"PLUMED web site PLUMED on GitHub","title":"PLUMED installation instructions"},{"location":"generated/easyconfigs/p/PLUMED/#general-information","text":"Environment variables: PLUMED_KERNEL is used by the Python wrappers when they are installed from PyPi outside the regular PLUMED install process (which makes sense as this makes it easier to support multiple versions of Python). See, e.g., PLUMED manual: Installing Python wrappers PLUMED_ROOT : points to PLUMED-related source files that are used by various scripts that rely on PLUMED. This variable and the next ones are discussed on The manual page \"Installation Layout\" . PLUMED_INDLUDEDIR : Points to the include files. PLUMED_HTMLDIR : Points to the HTML documentation in share/doc/plumed, but these files are not installed by the current EasyConfigs. (We don't run make doc ) PLUMED_PROGRAM_NAME : usually plumed.","title":"General information"},{"location":"generated/easyconfigs/p/PLUMED/#easyconfigs","text":"First version covered by this documentation: The 2019b toolchains.","title":"EasyConfigs"},{"location":"generated/easyconfigs/p/PLUMED/#2019b-toolchains-247-254-260","text":"Switched from modextrapaths to modextravars, as PLUMED_ROOT and PLUMED_KERNEL shouldn't be treated as paths if the variables already exist when the module is loaded. We also added other relevant variables.","title":"2019b toolchains: 2.4.7, 2.5.4, 2.6.0"},{"location":"generated/easyconfigs/p/PROJ/","text":"PROJ installation instructions PROJ web site PROJ GitHub PROJ datumgrid GitHub General installation instructions PROJ by itself is not complete. It needs a data files to be useful. Before 7.0.0, none of these data files were included in the EasyBuild recipes. 7.0.0 was a pretty major release, bringing with it changes in the datumgrid files and the way they are organized. The installation instructions on the web site are pretty good. PROJ supports installation with both autotools and CMake. EasyConfigs We started from standard EasyConfigs, but reworked thoroughly for version 7.0.0. 7.0.0 - 2020a toolchains We switched the download location to downloading directly form the GitHub. We now also include the data files. It turns out that they do not decompress in their own directory. Hence, to keep things clean, we use extract_cmd to extract those files in a separate directory. We use postinstallcmds to copy the files to the right location at the end of the installation process.","title":"PROJ installation instructions"},{"location":"generated/easyconfigs/p/PROJ/#proj-installation-instructions","text":"PROJ web site PROJ GitHub PROJ datumgrid GitHub","title":"PROJ installation instructions"},{"location":"generated/easyconfigs/p/PROJ/#general-installation-instructions","text":"PROJ by itself is not complete. It needs a data files to be useful. Before 7.0.0, none of these data files were included in the EasyBuild recipes. 7.0.0 was a pretty major release, bringing with it changes in the datumgrid files and the way they are organized. The installation instructions on the web site are pretty good. PROJ supports installation with both autotools and CMake.","title":"General installation instructions"},{"location":"generated/easyconfigs/p/PROJ/#easyconfigs","text":"We started from standard EasyConfigs, but reworked thoroughly for version 7.0.0.","title":"EasyConfigs"},{"location":"generated/easyconfigs/p/PROJ/#700-2020a-toolchains","text":"We switched the download location to downloading directly form the GitHub. We now also include the data files. It turns out that they do not decompress in their own directory. Hence, to keep things clean, we use extract_cmd to extract those files in a separate directory. We use postinstallcmds to copy the files to the right location at the end of the installation process.","title":"7.0.0 - 2020a toolchains"},{"location":"generated/easyconfigs/p/Pandoc/","text":"Pandoc installation instructions General Pandoc depends on several other programs, especially for processing LaTeX and generating PDF files. As we do not want to offer LaTeX on the clusters, the functionality of Pandoc will be slightly limited. We did chose to include groff and makeinfo and include Ghostscript as a dependency to offer a different path for generating PDF to our users. Pandoc is written in Haskell so would require installing a particular Haskell environment also. Moreover, its build process uses tools that are not supported by EasyBuild. Given that Pandoc should not be the time-critical part in a HPC code it makes sense to install from binaries as otherwise getting all staps to work and integrated in EasyBuild would take ages and require the development of a new EasyBlock to support installing with the tool it uses ( stack ) which is not very compatible with the way of working in an HPC system anyway. Easybuild As some users certainly want to use Pandoc to generate PDF, we did add some additional software to the Pandoc module in the EasyBuild repository. There is support for Pandoc in the EasyBuilders repository. There is support for makeinfo in the EasyBuilders repository. There is support for groff in the EasyBuilders repository. Pandoc 2.11.4 We combined with makeinfo/texinfo 6.7 and groff 1.22.4 compiled in the GCCcore base toolchain. Hence Pandoc also lands in GCCcore even though it is simply installed from a tarball with binaries. For some reason, the second component in the Bundle failed to find the installed files of the first one, so the path is set by hand. It is not clear what is happening here as there are other Bundle-based tools where things seem to go right. In the Baselibs bundle we have various tools that need other tools.","title":"Pandoc installation instructions"},{"location":"generated/easyconfigs/p/Pandoc/#pandoc-installation-instructions","text":"","title":"Pandoc installation instructions"},{"location":"generated/easyconfigs/p/Pandoc/#general","text":"Pandoc depends on several other programs, especially for processing LaTeX and generating PDF files. As we do not want to offer LaTeX on the clusters, the functionality of Pandoc will be slightly limited. We did chose to include groff and makeinfo and include Ghostscript as a dependency to offer a different path for generating PDF to our users. Pandoc is written in Haskell so would require installing a particular Haskell environment also. Moreover, its build process uses tools that are not supported by EasyBuild. Given that Pandoc should not be the time-critical part in a HPC code it makes sense to install from binaries as otherwise getting all staps to work and integrated in EasyBuild would take ages and require the development of a new EasyBlock to support installing with the tool it uses ( stack ) which is not very compatible with the way of working in an HPC system anyway.","title":"General"},{"location":"generated/easyconfigs/p/Pandoc/#easybuild","text":"As some users certainly want to use Pandoc to generate PDF, we did add some additional software to the Pandoc module in the EasyBuild repository. There is support for Pandoc in the EasyBuilders repository. There is support for makeinfo in the EasyBuilders repository. There is support for groff in the EasyBuilders repository.","title":"Easybuild"},{"location":"generated/easyconfigs/p/Pandoc/#pandoc-2114","text":"We combined with makeinfo/texinfo 6.7 and groff 1.22.4 compiled in the GCCcore base toolchain. Hence Pandoc also lands in GCCcore even though it is simply installed from a tarball with binaries. For some reason, the second component in the Bundle failed to find the installed files of the first one, so the path is set by hand. It is not clear what is happening here as there are other Bundle-based tools where things seem to go right. In the Baselibs bundle we have various tools that need other tools.","title":"Pandoc 2.11.4"},{"location":"generated/easyconfigs/p/ParaView/","text":"ParaView installation ParaView is an open source visualisation tool. Its developers are: * Sandia National Laboratories * Los Alamos National Laboratory * Kitware Links: * ParaView web site * ParaView Wiki on that web site also contains information about some other packages having the same origins and distributed by Kitware. * That Wiki also contains the installation instructions , though this is an old text that refers to the building instructions on the ParaView GitLab server . * ParaView GitLab * build instructions Architecture Client side: paraview : The GUI-application pvpython : Python scripting interface for Paraview Server side: 2 options also pvserver : renderen and data server in one, MPI support to run on multiple nodes pvdataserver and pvrenderserver : Separate data- and renderservers. This scenario is discouraged as it if often less efficient. It does allow to do data processing and rendering on separate sets of nodes, but that rarely delivers much performance gain. paraview and pvpython have their own built-in dataserver and renderengine for local processing. pvbatch finally is a Python interpreter designed for batch processing rather than interactive visualisation and can be run in parallel. Dependencies ParaView needs Qt for the GUI component. ParaView needs Python, as one of the ways of working with it is to use it as a Python toolbox. ParaView uses the OpenGL graphics drivers from the platform it is running on. If there is no hardware OpenGL support, one can use Mesa instead. FFmpeg is an optional dependency to generate .avi movie files. It is best if the Python package six is already installed. EasyBuild There is support for ParaView in the EasyBuilders repository. ParaView 5.4.1, Intel 2018a Modified from the default EasyConfig files as we prefer to use system libraries for OpenGL trying to ensure that the OpenGL accelerator on the visualisation node can be used. Python 2 and Python 3 version. ParaView 5.8.1 As we failed to compile Qt5 in the 2020a toolchains (with different errors depending on the compiler used and version of Qt5), the initial installation was in the 2018a toolchain. We dropped support for also installing the data files and PDF copies of the tutorial. It is not clear if it even makes sense to include the data files as there was nothing about it in the install instructions, and the PDFs are not practical either on the cluster but can better be downloaded by the user to his or her desktop environment.","title":"ParaView installation"},{"location":"generated/easyconfigs/p/ParaView/#paraview-installation","text":"ParaView is an open source visualisation tool. Its developers are: * Sandia National Laboratories * Los Alamos National Laboratory * Kitware Links: * ParaView web site * ParaView Wiki on that web site also contains information about some other packages having the same origins and distributed by Kitware. * That Wiki also contains the installation instructions , though this is an old text that refers to the building instructions on the ParaView GitLab server . * ParaView GitLab * build instructions","title":"ParaView installation"},{"location":"generated/easyconfigs/p/ParaView/#architecture","text":"Client side: paraview : The GUI-application pvpython : Python scripting interface for Paraview Server side: 2 options also pvserver : renderen and data server in one, MPI support to run on multiple nodes pvdataserver and pvrenderserver : Separate data- and renderservers. This scenario is discouraged as it if often less efficient. It does allow to do data processing and rendering on separate sets of nodes, but that rarely delivers much performance gain. paraview and pvpython have their own built-in dataserver and renderengine for local processing. pvbatch finally is a Python interpreter designed for batch processing rather than interactive visualisation and can be run in parallel.","title":"Architecture"},{"location":"generated/easyconfigs/p/ParaView/#dependencies","text":"ParaView needs Qt for the GUI component. ParaView needs Python, as one of the ways of working with it is to use it as a Python toolbox. ParaView uses the OpenGL graphics drivers from the platform it is running on. If there is no hardware OpenGL support, one can use Mesa instead. FFmpeg is an optional dependency to generate .avi movie files. It is best if the Python package six is already installed.","title":"Dependencies"},{"location":"generated/easyconfigs/p/ParaView/#easybuild","text":"There is support for ParaView in the EasyBuilders repository.","title":"EasyBuild"},{"location":"generated/easyconfigs/p/ParaView/#paraview-541-intel-2018a","text":"Modified from the default EasyConfig files as we prefer to use system libraries for OpenGL trying to ensure that the OpenGL accelerator on the visualisation node can be used. Python 2 and Python 3 version.","title":"ParaView 5.4.1, Intel 2018a"},{"location":"generated/easyconfigs/p/ParaView/#paraview-581","text":"As we failed to compile Qt5 in the 2020a toolchains (with different errors depending on the compiler used and version of Qt5), the initial installation was in the 2018a toolchain. We dropped support for also installing the data files and PDF copies of the tutorial. It is not clear if it even makes sense to include the data files as there was nothing about it in the install instructions, and the PDFs are not practical either on the cluster but can better be downloaded by the user to his or her desktop environment.","title":"ParaView 5.8.1"},{"location":"generated/easyconfigs/p/Perl/","text":"Perl installation instructions General information Note that when installing an extension, EasyBuild follows the following steps as can be derived from the perlmodule generic EasyBlock: There are two possible installation procedures: either through a Makefile.PL or a Build.PL script. In case the package is installed through Makefile.PL : It calls Makefile.PL preceded by preconfigopts and with configopts appended to the command. It then calls the build, test and install steps as if they would be called by ConfigureMake. Hence one can use the options of ConfigureMake to influence the build and install behaviour. In case of installing through Build.PL: It first calls the Build.PL script, preceding with preconfigopts and adding configopts at the end of the command. Next it calls perl Build build preceded by prebuildopts and with buildopts appended to the command. If runtest is set, it then calls perl Build with runtest as the argument. Finally it calls perl Build install preceded by preinstallopts and with installopts appended to it. EasyConfigs The first version covered by this documentation is 5.30.2 (2020a toolchains) 5.30.2 for intel 2020a and matching GCCcore 9.3.0 Some extensions install without problems in the GCCcore version but fail in the Intel version. XML::Bare: The problem is in the Makefile of this package. It defines CC to be gcc in the Makefile but does not set CFLAGS. Hence CC is not picked up from the environment, but CFLAGS is and it tries to compile the extension using gcc with options for the Intel compiler which of course bombs. That Makefile is generated by the Perl script Makefile.PL during the configuration phase of the extension. The script simply checks for the executables gcc or cc in the PATH and hard-codes that in the CC Makefile variable. Solution: Add 'buildopts': 'CC=\"$CC\"' Set::IntervalTree: The problem is similar to that with XML::Bare, except that now c++ is hardcoded as the name for the C/C++ compiler. Moreover, it also adds -xc++ to the CCFLAGS variable in the Makefile that it generates. It does so in a line that is already a processed version of the CCFLAGS variable read from the environment so it is non-trivial to change through a command line option to make in the build step... Luckily the -xc++ does not cause problems with the Intel compiler. Otherwise we'd have to edit it out of the generated Makefile with a sed command in prebuildopts . Solution: Add 'buildopts': 'CC=\"$CXX\"'","title":"Perl installation instructions"},{"location":"generated/easyconfigs/p/Perl/#perl-installation-instructions","text":"","title":"Perl installation instructions"},{"location":"generated/easyconfigs/p/Perl/#general-information","text":"Note that when installing an extension, EasyBuild follows the following steps as can be derived from the perlmodule generic EasyBlock: There are two possible installation procedures: either through a Makefile.PL or a Build.PL script. In case the package is installed through Makefile.PL : It calls Makefile.PL preceded by preconfigopts and with configopts appended to the command. It then calls the build, test and install steps as if they would be called by ConfigureMake. Hence one can use the options of ConfigureMake to influence the build and install behaviour. In case of installing through Build.PL: It first calls the Build.PL script, preceding with preconfigopts and adding configopts at the end of the command. Next it calls perl Build build preceded by prebuildopts and with buildopts appended to the command. If runtest is set, it then calls perl Build with runtest as the argument. Finally it calls perl Build install preceded by preinstallopts and with installopts appended to it.","title":"General information"},{"location":"generated/easyconfigs/p/Perl/#easyconfigs","text":"The first version covered by this documentation is 5.30.2 (2020a toolchains)","title":"EasyConfigs"},{"location":"generated/easyconfigs/p/Perl/#5302-for-intel-2020a-and-matching-gcccore-930","text":"Some extensions install without problems in the GCCcore version but fail in the Intel version. XML::Bare: The problem is in the Makefile of this package. It defines CC to be gcc in the Makefile but does not set CFLAGS. Hence CC is not picked up from the environment, but CFLAGS is and it tries to compile the extension using gcc with options for the Intel compiler which of course bombs. That Makefile is generated by the Perl script Makefile.PL during the configuration phase of the extension. The script simply checks for the executables gcc or cc in the PATH and hard-codes that in the CC Makefile variable. Solution: Add 'buildopts': 'CC=\"$CC\"' Set::IntervalTree: The problem is similar to that with XML::Bare, except that now c++ is hardcoded as the name for the C/C++ compiler. Moreover, it also adds -xc++ to the CCFLAGS variable in the Makefile that it generates. It does so in a line that is already a processed version of the CCFLAGS variable read from the environment so it is non-trivial to change through a command line option to make in the build step... Luckily the -xc++ does not cause problems with the Intel compiler. Otherwise we'd have to edit it out of the generated Makefile with a sed command in prebuildopts . Solution: Add 'buildopts': 'CC=\"$CXX\"'","title":"5.30.2 for intel 2020a and matching GCCcore 9.3.0"},{"location":"generated/easyconfigs/p/Pysam/","text":"Pysam installation instructions pysam development on GitHub pysam on PyPi pysam documentation The goal of this recipe is to prepare for inclusion in the BioTools-Python bundle. However, it turns out that linking to external HTSlib/BCFtools/SAMtools is difficult as pysam is often lagging in the versions supported. There is no naming conflict with shared libraries though so it is perfectly possible to use the pysam Python module while another version of SAMtools/BCFtools/HTSlib is loaded.","title":"Pysam installation instructions"},{"location":"generated/easyconfigs/p/Pysam/#pysam-installation-instructions","text":"pysam development on GitHub pysam on PyPi pysam documentation The goal of this recipe is to prepare for inclusion in the BioTools-Python bundle. However, it turns out that linking to external HTSlib/BCFtools/SAMtools is difficult as pysam is often lagging in the versions supported. There is no naming conflict with shared libraries though so it is perfectly possible to use the pysam Python module while another version of SAMtools/BCFtools/HTSlib is loaded.","title":"Pysam installation instructions"},{"location":"generated/easyconfigs/p/Python/","text":"Python @ UAntwerp documentation This documentation was started with the 2020a toolchain. Python web site Overview of releases General information Core Python dependencies A look at the configure of 3.8.2: Core Python depends on Tcl/Tk as the tkinter package is nowadays part of the Python Standard Library Core Python can use libffi for the _ctypes module Core Python can use libmpdec for the _decimal module. Core Python can use expat for the pyexpat module. Core Python also needs the OpenSSL libraries. Dependency check via ldd on all files in `lib/pythonx.y/lib-dynlib', for Core Python 3.8: Module Package Library Python package baselibs zlib libz zlib, tkinter, binascii baselibs bzip2 libbz2 bz2 baselibs XZ liblzma lzma baselibs libffi libffi ctypes baselibs readline libreadline readline baselibs ncurses libncurses, libpanel readline, curses, curses.panel baselibs mpdecimal libmpdec decimal baselibs expat libexpat pyexpat, xml.parsers.expat baselibs PCRE libpcre ssl, hashlib baselibs util-linux libuuid uuid baselibs libpng libpng16 tkinter baselibs fontconfig libfontconfig tkinter baselibs freetype libfreetype tkinter SQLite SQLite libsqlite3 sqlite3 Tcl Tcl libtcl8.6 tkinter Tk Tk libtk8.6 tkinter X11 X11 libX11 tkinter X11 Xau libXau tkinter X11 xcb libxcb tkinter X11 Xdmcp libXdmcp tkinter X11 Xext libXext tkinter X11 Xft libXft tkinter X11 Xrender libXrender tkinter X11 Xss libXss tkinter The ssl Python package uses libssl and related libraries, but we prefer to use the OS versions since they get patched from time to time. The configure help also talks about dbm backends. TODO: Check what we need to do there. Core Python does include SQLite support, which can link to an external libsqlite3 (see table above). There is a gettext package but it seems to rely on the os package and not use the gettext from baselibs for the gettext functionality. There is likely similar functionality included in libc or so which is used instead. Packages included in Python Core Python does come with included pip , easy_install , distutils packages. Non-standard library packages that interface to external libaries NumPy: Link to MKL and can use SuiteSparse mkl_random: Link to certain MKL random number generators mkl_fft: Interface to the MKL FFT routines SciPy: Interfaces to various MKL components mpi4py: Interface to the MPI libraries h5py: Interface to the HDF5 libraries lz4: Interfaces to the lz4-library in baselibs PyYAML: Interfaces to libyaml in baselibs cffi: Interfaces to libffi in baselibs pysam: Provides a low-level interface to htslib (part of SAMtools) psutil: Interfaces to the system libraries for process management pycrypto: Interfaces to the system libraries for security features ecdsa: Interfaces to the system libraries for security features bcrypt: Interfaces to the system libraries for security/cryptography features PyNaCl: Interfaces to libsodium, a libary for enctyption, decryption, etc. (we use the included one) netifaces: Interfaces to system libraries pyzmq: Interfaces to libzmq, but seems to contain it. pysqlite3: Alternative interface to libsqlite3, needed by the USPEX code. cryptography: Interfaces to several OS security libraries, including libssl and libcrypto5 PyTables (PyPi: tables): Interface to the HDF5 libraries and to bzip2, LZO and Blosc in baselibs. Dependencies of PyTables: Cython, NumPy, Numexpr, HDF5 library, LZO/LZO2 libary, BZIP2 library. Blosc is an optional dependency; it also includes the code. Compile problems with 3.6.1 on 3.8.2, see the Python 3.8.2-section for the solution. matplotlib: Uses libbz2, libpng16, libz and libfreetype from baselibs but no direct dependencies on X11. lxml: Interfaces to libxml2/libxslt (baselibs) scikit-learn gmpy2: Interfaces to GMP, MPC and MPFR (baselibs) tinyarray: Offers a data type, uses standard system libraries greenlet: Lightweight concurrent programming, uses standard system libraries ujson: JSON encoder/decoder, uses only standard system libraries scandir: directory iteration function, uses only standard system libraries blist: Drop-in for Python lists, uses only standard system libraries memory_profiler: Uses only standard system libraries spglib: Depends on NumPy and standard system libraries pycosat: Interface to the picosat solver, included in the distribution, and uses standard system libraries. ruamel.yaml.clib: C based reader/scanner and emitter for ruamel.yaml, uses standard system libraries. sip: SIP is a collection of tools that makes it very easy to create Python bindings for C and C++ libraries, used by, e.g., PyQt. Uses standard system libraries but has a lot of other dependencies and is therefore further down in the build tree. Notes on some other packages pandas Package that generates a lot of shared libraries. However, they only interface to standard libraries Requires pytz, python-dateutil Installs without PyTables, but can use it. PyTables is on PyPi as tables and provides an interface to HDF5. sklearn Package that generates a lot of shared libraries. However, they only interface to standard libraries EasyBuild generics Python EasyBlock Python is build through an EasyBlock that does add some stuff to the installation for all the EasyBuild trickery. Python config options defined by the EasyBlock: * optimized: Build with expensive, stable optimizations (PGO, etc.,), supported from Python 3.5.4 on. The default is True. * use_lto: Build with Link Time Optimization (Python 3.7.0 and later), but this is potentially unstable on some toolchains. The default value is \"None\" which does an auto-detect based on the toolchain compiler version. * ebpythonprefixes: Create a special sitecustomize.py and allow the use of $EBPYTHONPREFIXES. The default is True. * ulimit_unlimited: Ensure the stack size limit is set to unlimited during the build. The default is False. Python packages New versions of EasyBuild now use GCCcore to compile Python rather than the Intel compilers as it can then be used in multiple toolchains and as most of Python does not really benefit from the Intel compiler anyway. At UAntwerp we still try to get Python to compile with the Intel compiler. EasyBuild does use the Intel compilers though for some performance critical packages. SciPy-bundle is a bundle containing NumPy, SciPy, mpi4py, pandas and mpmath that should benefit from the Intel compiler. It is not clear what mpmath does in this bundle though as it has no dependencies besides the Python standard libary and is pure Python. It relies on the GMP library interface in the standard Python libary. NumPy NumPy in EasyBuild is installed through an EasyBlock derived from FortranPythonPackage. Configure phase: Builds a site.cfg file Does the regular discovery for a Python package Runs python setup configure Build phase: Runs python setup build , adding buildopts and appropriate --compiler and --fcompiler options. Note that this is done through the FortranPythonPackage generic EasyBlock. Test phase: The NumPy EasyBlock does perform a number of texts. Search in the log for INFO Time for .* matrix dot product . Install phase: Uses the regular install procedure for Python packages so will honour all regular Python package installation options (such as use_pip et.) Notes: * NumPy can use SuiteSparse which is recognized by the EasyBlock and the appropriate lines for AMD and UMFPACK are added to the site.cfg file. SciPy SciPy in EasyBuild is installed through an EasyBlock derived from FortranPythonPackage. Configure phase: Just the regular discovery for a Python package. Note however that from version 0.13 onwards, preinstallopts is modified so be careful if you use that parameter. Build phase: The normal build phase for a Fortran Python package, i.e., running python setup build , adding buildopts and appropriate --compiler and --fcompiler options. Install phase: The regular install phase as for every Python package, but note that this will run with a modified preinstallopts from SciPy 0.13 onwards. EasyBuild packages in this directory Python 2.7.18 on 2020a Likely the last Python 2 module ever on our cluster. In fact, we plan to only install this module when a user has a really good case for it. Simple version update of all packages of the 2019b Python 2 module with some reordering to make sure dependencies are installed in the right order. This helps if part of the packages would be out-commented for a partial build. Python 3.7.7 on 2020a Provided as \"spare\" as we in fact also provide Intel Python to our users, which is the 3.7 branch in the 2020 compilers. This module will not be installed by default on our clusters. Added -fwrapv to the compiler flags, see remarks for 3.8 where it was essential. Packages are just a copy of the 3.8.2 ones. Python 3.8(.3) effort on 2020a. Builds with the Intel compiler need an additional option added to CFLAGS: -fwrapv . This option doesn't seem to be documented in the Intel manuals. However, the GCC manual tells that this option instructs the compiler to assume that signed arithmetic overflow of addition, subtraction and multiplication wraps around using twos-complement representation. This flag enables some optimizations and disables others. See Python bug report 40223 See also remark of Stefan Krah in this Intel forum . In fact, the bug report and remarks of Stafan Krah suggest that this option is useful in Python 3.7 also and should also be used when using GCC. After a raw Python install (without any additional packages beyond the standard library) distutils is available, version 3.8.2 pip is available through the pip3 command. The version is 19.2.3, which was not the latest one but a fairly recent one. Pip 19.3 launched on the same day as Python 3.8.2, and there have been more new version, so supposedly maintenance releases stich with the versions of packages available during testing of the .0-version. setuptools is also included, but again a slightly older version (41.2.0) All this suggest to include newer pip and setuptools in even the most basic Python module. The baselibs module was further extended to make sure that all standard library packages use our own installed versions of software rather than the system versions, with the exception of security-related libraries. Those are taken from the OS image to be sure that they get updated whenever security patches become available. We've tried to move a number of packages that require heavy compiler work as high up the chain as possible so that errors show as quickly as possible. NumPy, SciPy and pandas are three packages that take quite some time to compile. mpi4py interfaces to the MPI libraries h5py and tables (also known as PyTables) interface to the HDF5 libraries. In fact, we first install packages that use various libraries installed in the toolchain (and the dependencies that are absolutely needed for this), then most packages that need compilation but use only system libraries and compiler runtimes and after that all packages that only contain Python code. It turned out that NumPy needs a sufficiently recent version of Cython to compile even though it did not complain that Cython was not yet present; the build simply crashed. PyTables (PyPi: tables) needs some trickery Compilation of PyTables fails due to wrong linker options: It uses -l-liomp5 -l-lpthread. This may come from prepending -l to the contents of python3.8/ sysconfigdata__linux_x86_64-linux-gnu.py. The solution is to define LIBS=\"iomp5 pthread\", overwriting the _sysconfigdata . PyTables had trouble finding bzip2, LZO and Blosc, so we also defined the environment variables BZIP2_DIR, LZO_DIR and BLOSC_DIR, all with the value $EBROOTBASELIBS (which is the module that contains those libraries). Note that we also set CC and CXX to mpiicc and mpiicpc respectively as we link with the HDF5-library with MPI support. TODO Move pysam to bioinformatics module and use export HTSLIB_LIBRARY_DIR=/usr/local/lib export HTSLIB_INCLUDE_DIR=/usr/local/include pip install pysam","title":"Python @ UAntwerp documentation"},{"location":"generated/easyconfigs/p/Python/#python-uantwerp-documentation","text":"This documentation was started with the 2020a toolchain. Python web site Overview of releases","title":"Python @ UAntwerp documentation"},{"location":"generated/easyconfigs/p/Python/#general-information","text":"","title":"General information"},{"location":"generated/easyconfigs/p/Python/#core-python-dependencies","text":"A look at the configure of 3.8.2: Core Python depends on Tcl/Tk as the tkinter package is nowadays part of the Python Standard Library Core Python can use libffi for the _ctypes module Core Python can use libmpdec for the _decimal module. Core Python can use expat for the pyexpat module. Core Python also needs the OpenSSL libraries. Dependency check via ldd on all files in `lib/pythonx.y/lib-dynlib', for Core Python 3.8: Module Package Library Python package baselibs zlib libz zlib, tkinter, binascii baselibs bzip2 libbz2 bz2 baselibs XZ liblzma lzma baselibs libffi libffi ctypes baselibs readline libreadline readline baselibs ncurses libncurses, libpanel readline, curses, curses.panel baselibs mpdecimal libmpdec decimal baselibs expat libexpat pyexpat, xml.parsers.expat baselibs PCRE libpcre ssl, hashlib baselibs util-linux libuuid uuid baselibs libpng libpng16 tkinter baselibs fontconfig libfontconfig tkinter baselibs freetype libfreetype tkinter SQLite SQLite libsqlite3 sqlite3 Tcl Tcl libtcl8.6 tkinter Tk Tk libtk8.6 tkinter X11 X11 libX11 tkinter X11 Xau libXau tkinter X11 xcb libxcb tkinter X11 Xdmcp libXdmcp tkinter X11 Xext libXext tkinter X11 Xft libXft tkinter X11 Xrender libXrender tkinter X11 Xss libXss tkinter The ssl Python package uses libssl and related libraries, but we prefer to use the OS versions since they get patched from time to time. The configure help also talks about dbm backends. TODO: Check what we need to do there. Core Python does include SQLite support, which can link to an external libsqlite3 (see table above). There is a gettext package but it seems to rely on the os package and not use the gettext from baselibs for the gettext functionality. There is likely similar functionality included in libc or so which is used instead.","title":"Core Python dependencies"},{"location":"generated/easyconfigs/p/Python/#packages-included-in-python","text":"Core Python does come with included pip , easy_install , distutils packages.","title":"Packages included in Python"},{"location":"generated/easyconfigs/p/Python/#non-standard-library-packages-that-interface-to-external-libaries","text":"NumPy: Link to MKL and can use SuiteSparse mkl_random: Link to certain MKL random number generators mkl_fft: Interface to the MKL FFT routines SciPy: Interfaces to various MKL components mpi4py: Interface to the MPI libraries h5py: Interface to the HDF5 libraries lz4: Interfaces to the lz4-library in baselibs PyYAML: Interfaces to libyaml in baselibs cffi: Interfaces to libffi in baselibs pysam: Provides a low-level interface to htslib (part of SAMtools) psutil: Interfaces to the system libraries for process management pycrypto: Interfaces to the system libraries for security features ecdsa: Interfaces to the system libraries for security features bcrypt: Interfaces to the system libraries for security/cryptography features PyNaCl: Interfaces to libsodium, a libary for enctyption, decryption, etc. (we use the included one) netifaces: Interfaces to system libraries pyzmq: Interfaces to libzmq, but seems to contain it. pysqlite3: Alternative interface to libsqlite3, needed by the USPEX code. cryptography: Interfaces to several OS security libraries, including libssl and libcrypto5 PyTables (PyPi: tables): Interface to the HDF5 libraries and to bzip2, LZO and Blosc in baselibs. Dependencies of PyTables: Cython, NumPy, Numexpr, HDF5 library, LZO/LZO2 libary, BZIP2 library. Blosc is an optional dependency; it also includes the code. Compile problems with 3.6.1 on 3.8.2, see the Python 3.8.2-section for the solution. matplotlib: Uses libbz2, libpng16, libz and libfreetype from baselibs but no direct dependencies on X11. lxml: Interfaces to libxml2/libxslt (baselibs) scikit-learn gmpy2: Interfaces to GMP, MPC and MPFR (baselibs) tinyarray: Offers a data type, uses standard system libraries greenlet: Lightweight concurrent programming, uses standard system libraries ujson: JSON encoder/decoder, uses only standard system libraries scandir: directory iteration function, uses only standard system libraries blist: Drop-in for Python lists, uses only standard system libraries memory_profiler: Uses only standard system libraries spglib: Depends on NumPy and standard system libraries pycosat: Interface to the picosat solver, included in the distribution, and uses standard system libraries. ruamel.yaml.clib: C based reader/scanner and emitter for ruamel.yaml, uses standard system libraries. sip: SIP is a collection of tools that makes it very easy to create Python bindings for C and C++ libraries, used by, e.g., PyQt. Uses standard system libraries but has a lot of other dependencies and is therefore further down in the build tree.","title":"Non-standard library packages that interface to external libaries"},{"location":"generated/easyconfigs/p/Python/#notes-on-some-other-packages","text":"","title":"Notes on some other packages"},{"location":"generated/easyconfigs/p/Python/#pandas","text":"Package that generates a lot of shared libraries. However, they only interface to standard libraries Requires pytz, python-dateutil Installs without PyTables, but can use it. PyTables is on PyPi as tables and provides an interface to HDF5.","title":"pandas"},{"location":"generated/easyconfigs/p/Python/#sklearn","text":"Package that generates a lot of shared libraries. However, they only interface to standard libraries","title":"sklearn"},{"location":"generated/easyconfigs/p/Python/#easybuild-generics","text":"","title":"EasyBuild generics"},{"location":"generated/easyconfigs/p/Python/#python-easyblock","text":"Python is build through an EasyBlock that does add some stuff to the installation for all the EasyBuild trickery. Python config options defined by the EasyBlock: * optimized: Build with expensive, stable optimizations (PGO, etc.,), supported from Python 3.5.4 on. The default is True. * use_lto: Build with Link Time Optimization (Python 3.7.0 and later), but this is potentially unstable on some toolchains. The default value is \"None\" which does an auto-detect based on the toolchain compiler version. * ebpythonprefixes: Create a special sitecustomize.py and allow the use of $EBPYTHONPREFIXES. The default is True. * ulimit_unlimited: Ensure the stack size limit is set to unlimited during the build. The default is False.","title":"Python EasyBlock"},{"location":"generated/easyconfigs/p/Python/#python-packages","text":"New versions of EasyBuild now use GCCcore to compile Python rather than the Intel compilers as it can then be used in multiple toolchains and as most of Python does not really benefit from the Intel compiler anyway. At UAntwerp we still try to get Python to compile with the Intel compiler. EasyBuild does use the Intel compilers though for some performance critical packages. SciPy-bundle is a bundle containing NumPy, SciPy, mpi4py, pandas and mpmath that should benefit from the Intel compiler. It is not clear what mpmath does in this bundle though as it has no dependencies besides the Python standard libary and is pure Python. It relies on the GMP library interface in the standard Python libary.","title":"Python packages"},{"location":"generated/easyconfigs/p/Python/#numpy","text":"NumPy in EasyBuild is installed through an EasyBlock derived from FortranPythonPackage. Configure phase: Builds a site.cfg file Does the regular discovery for a Python package Runs python setup configure Build phase: Runs python setup build , adding buildopts and appropriate --compiler and --fcompiler options. Note that this is done through the FortranPythonPackage generic EasyBlock. Test phase: The NumPy EasyBlock does perform a number of texts. Search in the log for INFO Time for .* matrix dot product . Install phase: Uses the regular install procedure for Python packages so will honour all regular Python package installation options (such as use_pip et.) Notes: * NumPy can use SuiteSparse which is recognized by the EasyBlock and the appropriate lines for AMD and UMFPACK are added to the site.cfg file.","title":"NumPy"},{"location":"generated/easyconfigs/p/Python/#scipy","text":"SciPy in EasyBuild is installed through an EasyBlock derived from FortranPythonPackage. Configure phase: Just the regular discovery for a Python package. Note however that from version 0.13 onwards, preinstallopts is modified so be careful if you use that parameter. Build phase: The normal build phase for a Fortran Python package, i.e., running python setup build , adding buildopts and appropriate --compiler and --fcompiler options. Install phase: The regular install phase as for every Python package, but note that this will run with a modified preinstallopts from SciPy 0.13 onwards.","title":"SciPy"},{"location":"generated/easyconfigs/p/Python/#easybuild-packages-in-this-directory","text":"","title":"EasyBuild packages in this directory"},{"location":"generated/easyconfigs/p/Python/#python-2718-on-2020a","text":"Likely the last Python 2 module ever on our cluster. In fact, we plan to only install this module when a user has a really good case for it. Simple version update of all packages of the 2019b Python 2 module with some reordering to make sure dependencies are installed in the right order. This helps if part of the packages would be out-commented for a partial build.","title":"Python 2.7.18 on 2020a"},{"location":"generated/easyconfigs/p/Python/#python-377-on-2020a","text":"Provided as \"spare\" as we in fact also provide Intel Python to our users, which is the 3.7 branch in the 2020 compilers. This module will not be installed by default on our clusters. Added -fwrapv to the compiler flags, see remarks for 3.8 where it was essential. Packages are just a copy of the 3.8.2 ones.","title":"Python 3.7.7 on 2020a"},{"location":"generated/easyconfigs/p/Python/#python-383-effort-on-2020a","text":"Builds with the Intel compiler need an additional option added to CFLAGS: -fwrapv . This option doesn't seem to be documented in the Intel manuals. However, the GCC manual tells that this option instructs the compiler to assume that signed arithmetic overflow of addition, subtraction and multiplication wraps around using twos-complement representation. This flag enables some optimizations and disables others. See Python bug report 40223 See also remark of Stefan Krah in this Intel forum . In fact, the bug report and remarks of Stafan Krah suggest that this option is useful in Python 3.7 also and should also be used when using GCC. After a raw Python install (without any additional packages beyond the standard library) distutils is available, version 3.8.2 pip is available through the pip3 command. The version is 19.2.3, which was not the latest one but a fairly recent one. Pip 19.3 launched on the same day as Python 3.8.2, and there have been more new version, so supposedly maintenance releases stich with the versions of packages available during testing of the .0-version. setuptools is also included, but again a slightly older version (41.2.0) All this suggest to include newer pip and setuptools in even the most basic Python module. The baselibs module was further extended to make sure that all standard library packages use our own installed versions of software rather than the system versions, with the exception of security-related libraries. Those are taken from the OS image to be sure that they get updated whenever security patches become available. We've tried to move a number of packages that require heavy compiler work as high up the chain as possible so that errors show as quickly as possible. NumPy, SciPy and pandas are three packages that take quite some time to compile. mpi4py interfaces to the MPI libraries h5py and tables (also known as PyTables) interface to the HDF5 libraries. In fact, we first install packages that use various libraries installed in the toolchain (and the dependencies that are absolutely needed for this), then most packages that need compilation but use only system libraries and compiler runtimes and after that all packages that only contain Python code. It turned out that NumPy needs a sufficiently recent version of Cython to compile even though it did not complain that Cython was not yet present; the build simply crashed. PyTables (PyPi: tables) needs some trickery Compilation of PyTables fails due to wrong linker options: It uses -l-liomp5 -l-lpthread. This may come from prepending -l to the contents of python3.8/ sysconfigdata__linux_x86_64-linux-gnu.py. The solution is to define LIBS=\"iomp5 pthread\", overwriting the _sysconfigdata . PyTables had trouble finding bzip2, LZO and Blosc, so we also defined the environment variables BZIP2_DIR, LZO_DIR and BLOSC_DIR, all with the value $EBROOTBASELIBS (which is the module that contains those libraries). Note that we also set CC and CXX to mpiicc and mpiicpc respectively as we link with the HDF5-library with MPI support.","title":"Python 3.8(.3) effort on 2020a."},{"location":"generated/easyconfigs/p/Python/#todo","text":"Move pysam to bioinformatics module and use export HTSLIB_LIBRARY_DIR=/usr/local/lib export HTSLIB_INCLUDE_DIR=/usr/local/include pip install pysam","title":"TODO"},{"location":"generated/easyconfigs/p/parallel/","text":"GNU parallel instructions GNU parallel web site Check version GNU parallel on gnu.org git repository on GNU git GNU parallel tutorial General information GNU parallel is just a bunch of Perl scripts. There are no compiled binaries. EasyBuild There is support for parallel in the EasyBuilders repository . The EasyBuilders recipes rely on a Perl build in the toolchain. At UAntwerp, we have chosen to use the OS-provided perl distribution instead as this seems to be sufficient to run GNU parallel. We also put more documentation in our module files. Version 20220922 Added an additional sanity check, basically running the command with --help as in this way we may already detect missing Perl packages that might be needed.","title":"GNU parallel instructions"},{"location":"generated/easyconfigs/p/parallel/#gnu-parallel-instructions","text":"GNU parallel web site Check version GNU parallel on gnu.org git repository on GNU git GNU parallel tutorial","title":"GNU parallel instructions"},{"location":"generated/easyconfigs/p/parallel/#general-information","text":"GNU parallel is just a bunch of Perl scripts. There are no compiled binaries.","title":"General information"},{"location":"generated/easyconfigs/p/parallel/#easybuild","text":"There is support for parallel in the EasyBuilders repository . The EasyBuilders recipes rely on a Perl build in the toolchain. At UAntwerp, we have chosen to use the OS-provided perl distribution instead as this seems to be sufficient to run GNU parallel. We also put more documentation in our module files.","title":"EasyBuild"},{"location":"generated/easyconfigs/p/parallel/#version-20220922","text":"Added an additional sanity check, basically running the command with --help as in this way we may already detect missing Perl packages that might be needed.","title":"Version 20220922"},{"location":"generated/easyconfigs/p/phonopy/","text":"phonpy instructions phonopy home page with documentation phonopy on PyPi phonopy development on GitHub General information phonopy is a Python package. It does contain C code also though. Dependencies include matplotlib. lxml, PyYAML and h5py which are in the base Python modules at UAntwerpen. We should consider bundling with, among others, phono3py. EasyBuild There is support for phonopy in the EasyBuilders repository . Our EasyBuild recipes need to be adapted since we include a lot more packages in the base Python modules. 2.6.0 for 2020a toolchains We had to stuck to 2.6.0 instead of 2.6.1 since the latter was not available in a suitable source form. Build for Python 3.8 and the Intel Python 3 distribution. Verified with the state of the EasyBuilders modules. Updated the documentation section in the module file.","title":"phonpy instructions"},{"location":"generated/easyconfigs/p/phonopy/#phonpy-instructions","text":"phonopy home page with documentation phonopy on PyPi phonopy development on GitHub","title":"phonpy instructions"},{"location":"generated/easyconfigs/p/phonopy/#general-information","text":"phonopy is a Python package. It does contain C code also though. Dependencies include matplotlib. lxml, PyYAML and h5py which are in the base Python modules at UAntwerpen. We should consider bundling with, among others, phono3py.","title":"General information"},{"location":"generated/easyconfigs/p/phonopy/#easybuild","text":"There is support for phonopy in the EasyBuilders repository . Our EasyBuild recipes need to be adapted since we include a lot more packages in the base Python modules.","title":"EasyBuild"},{"location":"generated/easyconfigs/p/phonopy/#260-for-2020a-toolchains","text":"We had to stuck to 2.6.0 instead of 2.6.1 since the latter was not available in a suitable source form. Build for Python 3.8 and the Intel Python 3 distribution. Verified with the state of the EasyBuilders modules. Updated the documentation section in the module file.","title":"2.6.0 for 2020a toolchains"},{"location":"generated/easyconfigs/p/protobuf/","text":"Installing protobuf with EasyBuild Protobuf supports two build procedures: * autotools-based: I read somewhere on a forum that this is deprecated and will be removed in a future version (read this while trying to build protobuf for the 2019b toolchains). We did run into problems trying to compile protobuf 3.10 and 3.11 with GCCcore 8.3.0. * cmake: In late 2019 still marked as \"experimental\" and in the corresponding documentation as meant for Windows. This is demonstrated by python/setup.py which looks in the directories that would have been created with an autotools-based build. Protobuf contains several components * The src directory contains the actual sources for the protoc command and the protobuf libraries. * Then there are various interfaces to other languages * python contains the Python interfaces. There are multiple options for the Python protobuf. There is a full Python implementation but that is less speedy than the implementation that connects to a C++ library. We use the latter. Problems installing protobuf Protobuf has been generating warnings for a long time when compiled with the Intel compilers. This can be circumvented by adding a suitable -wd -option to CXXFLAGS , e.g., -wd2196,858,177 . However, even with these options, protobuf cannot be compiled with the Intel 2019 compilers. It fails with a \"static assertion failed\" error on line 341 of src/google/protobuf/arena.h triggered by a template instantiation on line 193 of src/google/protobuf/extension_set.cc (line numbers for protobuf 3.11.2). It does compile however with the GNU compilers which encourages to move protobuf from the Intel toolchain to the matching GCCcore toolchain. However, this then gives problems to generate the Python-interfaces when Python is compiled with the Intel compilers. The setup.py script uses distutils (and some elements from setuptools) and that is very rigid when configuring the compiler. It gets the compiler and flags from the file _sysconfigdata_m_linux_x86_64-linux-gnu.py in lib python* from the Python module. However, it does allow to replace or augment certain options. The way in which it does this however is broken. * The C and C++ compiler can be overwritten by setting the environment variables CC or CXX respectively to the desired compiler. * However, there is no way to overwrite the recorded compiler flags. One can define CFLAGS` in the environment and these flags will be added to the once recorded. If the compiler gives precedence to a flag further on the command line, this may still have the desired effect. However, it does not work if the new C or C++ compiler does not accept all the options that were used when building Python. This is definitel the case for a Python build with EasyBuild using the Intel toolchain. The workaround consists of: * Copy the file _sysconfigdata_m_linux_x86_64-linux-gnu.py from the Python module directories to the directory with setup.py of protobuf. * Edit that file with sed : We rename the CFLAGS directory entry to CFLAGS1 (because renaming is easier than to delete this often multiline entry) and re-add the entry CFLAGS with an empty string as a value. * And we make sure that the file is in front of the system one in the PYTHONPATH by putting the current directory in front. Our EasyConfig files Protobuf for the 2017 and 2018 toolchains Those for the 2017 toolchains did not include Python support Some EasyConfigs for the 2018 toolchains do include Python support. This was needed for, e.g., TensorFlow. We build upon an EasyConfig we found elsewhere: Autotools-based configuration Build process also includes the build step for the Python extension. This was added through buildopts that adds extra commands after the make process. Install process also includes the install step for the Python extension, implemented through installopts that adds additional commands after the make install for protobuf. Protobuf 3.11.2 for the 2019b toolchains (intel/2019b with GCCcore/8.3.0) We made a switch to CMake. A consequence of this is that only a shared library could be generated. We did try autoconf also, but the compilation failed with errors about undefined symbols (and it was googling for those error messages that we found the suggestion to try CMake instead). Even though we still emply the Intel compiler for some Python modules, due to the difficulties to compile with the Intel 2019 compilers we switched to the GCCcore subtoolchain. This implies that we faced difficulties to generate the Python interface as explained above. We implemented the solution as explained above. Preparing the Python module is done during the build step, after building the libraries but before these are moved to their final location. The installation of the Python interface is then done at the end of the install step. As the EasyConfig is just a CMakeMake file, it does not know about Python. Hence there is no automatic check of the Python module nor is the PYTHONPATH adjusted automatically in the module. We added the necessary line for the latter one but do not yet implement a test for the Python module. Note that we've tried to implement a test using tests = [\"PYTHONPATH=%(installdir)s/lib/python%(pyshortver)s/site-packages:$PYTHONPATH python -c 'import google.protobuf'\"] but that doesn't work due to strange restrictions in EasyBuild. The test should start with a very specific absolute path or the test script should be found in one of the source directories.","title":"Installing protobuf with EasyBuild"},{"location":"generated/easyconfigs/p/protobuf/#installing-protobuf-with-easybuild","text":"Protobuf supports two build procedures: * autotools-based: I read somewhere on a forum that this is deprecated and will be removed in a future version (read this while trying to build protobuf for the 2019b toolchains). We did run into problems trying to compile protobuf 3.10 and 3.11 with GCCcore 8.3.0. * cmake: In late 2019 still marked as \"experimental\" and in the corresponding documentation as meant for Windows. This is demonstrated by python/setup.py which looks in the directories that would have been created with an autotools-based build. Protobuf contains several components * The src directory contains the actual sources for the protoc command and the protobuf libraries. * Then there are various interfaces to other languages * python contains the Python interfaces. There are multiple options for the Python protobuf. There is a full Python implementation but that is less speedy than the implementation that connects to a C++ library. We use the latter.","title":"Installing protobuf with EasyBuild"},{"location":"generated/easyconfigs/p/protobuf/#problems-installing-protobuf","text":"Protobuf has been generating warnings for a long time when compiled with the Intel compilers. This can be circumvented by adding a suitable -wd -option to CXXFLAGS , e.g., -wd2196,858,177 . However, even with these options, protobuf cannot be compiled with the Intel 2019 compilers. It fails with a \"static assertion failed\" error on line 341 of src/google/protobuf/arena.h triggered by a template instantiation on line 193 of src/google/protobuf/extension_set.cc (line numbers for protobuf 3.11.2). It does compile however with the GNU compilers which encourages to move protobuf from the Intel toolchain to the matching GCCcore toolchain. However, this then gives problems to generate the Python-interfaces when Python is compiled with the Intel compilers. The setup.py script uses distutils (and some elements from setuptools) and that is very rigid when configuring the compiler. It gets the compiler and flags from the file _sysconfigdata_m_linux_x86_64-linux-gnu.py in lib python* from the Python module. However, it does allow to replace or augment certain options. The way in which it does this however is broken. * The C and C++ compiler can be overwritten by setting the environment variables CC or CXX respectively to the desired compiler. * However, there is no way to overwrite the recorded compiler flags. One can define CFLAGS` in the environment and these flags will be added to the once recorded. If the compiler gives precedence to a flag further on the command line, this may still have the desired effect. However, it does not work if the new C or C++ compiler does not accept all the options that were used when building Python. This is definitel the case for a Python build with EasyBuild using the Intel toolchain. The workaround consists of: * Copy the file _sysconfigdata_m_linux_x86_64-linux-gnu.py from the Python module directories to the directory with setup.py of protobuf. * Edit that file with sed : We rename the CFLAGS directory entry to CFLAGS1 (because renaming is easier than to delete this often multiline entry) and re-add the entry CFLAGS with an empty string as a value. * And we make sure that the file is in front of the system one in the PYTHONPATH by putting the current directory in front.","title":"Problems installing protobuf"},{"location":"generated/easyconfigs/p/protobuf/#our-easyconfig-files","text":"","title":"Our EasyConfig files"},{"location":"generated/easyconfigs/p/protobuf/#protobuf-for-the-2017-and-2018-toolchains","text":"Those for the 2017 toolchains did not include Python support Some EasyConfigs for the 2018 toolchains do include Python support. This was needed for, e.g., TensorFlow. We build upon an EasyConfig we found elsewhere: Autotools-based configuration Build process also includes the build step for the Python extension. This was added through buildopts that adds extra commands after the make process. Install process also includes the install step for the Python extension, implemented through installopts that adds additional commands after the make install for protobuf.","title":"Protobuf for the 2017 and 2018 toolchains"},{"location":"generated/easyconfigs/p/protobuf/#protobuf-3112-for-the-2019b-toolchains-intel2019b-with-gcccore830","text":"We made a switch to CMake. A consequence of this is that only a shared library could be generated. We did try autoconf also, but the compilation failed with errors about undefined symbols (and it was googling for those error messages that we found the suggestion to try CMake instead). Even though we still emply the Intel compiler for some Python modules, due to the difficulties to compile with the Intel 2019 compilers we switched to the GCCcore subtoolchain. This implies that we faced difficulties to generate the Python interface as explained above. We implemented the solution as explained above. Preparing the Python module is done during the build step, after building the libraries but before these are moved to their final location. The installation of the Python interface is then done at the end of the install step. As the EasyConfig is just a CMakeMake file, it does not know about Python. Hence there is no automatic check of the Python module nor is the PYTHONPATH adjusted automatically in the module. We added the necessary line for the latter one but do not yet implement a test for the Python module. Note that we've tried to implement a test using tests = [\"PYTHONPATH=%(installdir)s/lib/python%(pyshortver)s/site-packages:$PYTHONPATH python -c 'import google.protobuf'\"] but that doesn't work due to strange restrictions in EasyBuild. The test should start with a very specific absolute path or the test script should be found in one of the source directories.","title":"Protobuf 3.11.2 for the 2019b toolchains (intel/2019b with GCCcore/8.3.0)"},{"location":"generated/easyconfigs/q/Quantum-KITE/","text":"KITE aka Quantum-KITE installation KITE web site Installation instructions , however they only function with GCC and not on our clusters. KITE GitHub repository General information The documentation talks about version 1.0, but there is no such tag in the GitHub repository. Hence we work from a commit. KITE needs C++11 and one of the components needs C++17. KITE has a CMake build process but it is broken in many ways (early 2021): It only supports the GNU C++ compiler and several CMAKE variables are hard set that shouldn't be hard set. There is no procedure to build everything in a single cmake - make - make install cycle. The KITEx executable needs to be build from the CMakeLists.txt in the kite root directory while the KITE-tools executable needs to be build from the CMakeLists.txt in the tools subdirectory, so effectively using two build directories. This last bullet makes it a lot harder to integrate properly with the software that HPC centres use to manage their software stack such as EasyBuild or Spack. In early 2021, there is an old build script in the KITE directory: compile_Script.sh which again is GNU only and broken in many ways. Some of the source files needed for KITE-tools are not included. There is no way to configure the location of tools that are needed etc. It is really the kind of script that only works on the system of the developer. Specific install problems Compiling with the Intel compilers works but is hard, and there are several problems on the way Linking fails when compiling with an optimization level higher than -O0 without inter-procedural optimisations. The easy solution is to add -ipo to the compile and link command lines. Adapting the CMakeLists.txt files seems to be non-trivial (see also below). Simply deleting the lines that reset the compilers to the GNU compilers is and ensuring that the right compiler flags are used via -DCMAKE_... definitions when calling cmake turned out not to be enough. We still observed link problems for KITE-tools . It is not clear if this is due to the way CMake links the HDF5 libraries (with rpath) or not. The list of minor and major issues with the CMakeLists.txt files in the root directory and the tools subdirectory is extensive. At the top, CMAKE_C_COMPILER and CMAKE_CXX_COMPILER are redefined to be gcc and g++ respectively. Redefining those variables is a very bad idea... We removed those lines when using cmake. Setting CMAKE_CXX_STANDARD to 17 choses the wrong option for the Intel compiler. We removed those lines and selected the C++-standard via CMAKE_CXX_FLAGS . The messages printed during the HDF5 detection are wrong (the message for Hdf5 shows the information for Hdf5hl and vice-versa). The files also define a CORRECT_CODING_FLAGS variable whose function may be unclear, but the CMakeLists.txt -file for KITE-tools shows that it is meant to add additional warning flags. However, it is used inconsistently in both files and it should really be defined when calling cmake so that it can work with all compilers. When OpenMP is not detected, some of the CMAKE_CXX_FLAGS* -variables get overwritten rather than added to. This is again a bad idea. At several places, directories are added to the include path that are only relevant to the system of the developer. On other systems this may even lead to problems should that directory exist but, e.g., contain a wrong version of the include files. If essential software (such as the Eigen 3 include files) is missing, cmake should produce a clear error than just a status message. I'm not sure that not giving the option to overwrite the information that should go in the compiletime_info.h -file with something more meaningfull than what is auto-detected is a good idea. EasyBuild There is no support for KITE in EasyBuild. Given the number of packages that are called kite , we decided to name the package Quantum-KITE after the URL of the web site. Commit 7e298ea - version 1.0 with Intel 2020a Starting point: commit 7e298ea of February 2.21 where the file version.md claims to be 1.0. We studied 3 approaches to install KITE Correcting the CMakeLists.txt files with the suggestions above did not work. There were still undefined symbols when linking KITE-tools . Adapting the compile_script.sh file (more a rewrite) was successful. See below for the script. It is not the most elegant procedure to integrate with EasyBuild though. Construct Makefiles for the Src subdirectory ( KITEx ) and tools/src subdirectory ( KITE-tools ). For the EasyBuild buld process, we decided to go for this approach and to inject them in the MakeCp EasyConfig file. To avoid having two build process we also created a top-level Makefile that simply starts both build processes. Since there are more files that need copying we didn't implement a make install but simply use a MakeCp EasyBlock. Note that these Makefiles are not suitable for code development as most dependencies on include files are missing. Hence an update of the include files will not trigger the correct rebuilds. Even though the executables KITEx and KITE-tools do not require Python at all, we decided to make kwant-bundle a dependency to ensure that it is used with a Python version with a compatible HDF5 library and since Pybinding is an official dependency in the documentation of KITE. Eigen is only a template library so is a build dependency. Since some examples import kite.py, this has been put in a directory in the PYTHONPATH and precompiled via postinstallcmds . #!/bin/bash ml calcua/2020a ml intel/2020a ml HDF5/1.10.6-intel-2020a-MPI ml Eigen/3.3.7 CXX = icpc CXXFLAGS = \"-O2 -ipo -march=core-avx2 -mtune=core-avx2 -std=c++17 -ftz -fp-speculation=safe -fp-model source -fPIC -qopenmp -Wall\" CXXDEF = \"-DVERBOSE=0 -DDEBUG=0\" CXXINCL = \"-I $EBROOTEIGEN /include -I $EBROOTHDF5 /include\" LDFLAGS = \"-L $EBROOTHDF5 /lib\" LDLIBS = \"-lhdf5_hl_cpp -lhdf5_cpp -lhdf5_hl -lhdf5 -lz -lm\" echo \"Compiling KITEx and KITE-tools. To have the full functionality, you need to have at least version 8 of gcc.\" echo \"If you do not, you will not be able to run guassian_wavepacket. To enable compiling with this feature, please edit this file and set WAVEPACKET=1\" WAVEPACKET = 1 # make the directory structure rm -rf build tools/build mkdir -p build mkdir -p tools/build mkdir -p bin echo \"Compiling KITEx\" cd build for i in ../Src/*.cpp ; do echo \"Compiling $i \" $CXX -c $i -DCOMPILE_WAVEPACKET = $WAVEPACKET $CXXDEF $CXXFLAGS -I../Src/ $CXXINCL done echo \"Linking\" $CXX $CXXFLAGS *.o -o ../bin/KITEx $LDFLAGS $LDLIBS echo \"Done compiling KITEx.\" cd .. echo \"Compiling KITE-tools\" cd tools/build cat >compiletime_info.h <<EOF #define MACHINE_NAME \"${VSC_INSTITUTE_CLUSTER}\" #define SYSTEM_NAME \"${VSC_ARCH_LOCAL}\" #define TODAY \"$(date)\" EOF for i in ../src/*.cpp ; do echo \"Compiling $i \" $CXX -c $i $CXXDEF $CXXFLAGS -I. -I../Src/ -I../src/cond_2order $CXXINCL done for i in ../src/cond_2order/*.cpp ; do echo \"Compiling $i \" $CXX -c $i $CXXDEF $CXXFLAGS -I. -I../Src/ -I../src/cond_2order $CXXINCL done for i in ../src/cond_dc/*.cpp ; do echo \"Compiling $i \" $CXX -c $i $CXXDEF $CXXFLAGS -I. -I../Src/ -I../src/cond_2order $CXXINCL done echo \"Linking\" $CXX $CXXFLAGS *.o -o ../../bin/KITE-tools $LDFLAGS $LDLIBS echo \"Done compiling KITE-tools.\" cd ../.. echo \"Add the directory $( pwd ) /bin to the PATH and make sure HDF5/1.10.6-intel-2020a-MPI or kwant-bundle/1.4.2-intel-2020a-Python-3.8.3 is loaded when running KITE.\"","title":"KITE aka Quantum-KITE installation"},{"location":"generated/easyconfigs/q/Quantum-KITE/#kite-aka-quantum-kite-installation","text":"KITE web site Installation instructions , however they only function with GCC and not on our clusters. KITE GitHub repository","title":"KITE aka Quantum-KITE installation"},{"location":"generated/easyconfigs/q/Quantum-KITE/#general-information","text":"The documentation talks about version 1.0, but there is no such tag in the GitHub repository. Hence we work from a commit. KITE needs C++11 and one of the components needs C++17. KITE has a CMake build process but it is broken in many ways (early 2021): It only supports the GNU C++ compiler and several CMAKE variables are hard set that shouldn't be hard set. There is no procedure to build everything in a single cmake - make - make install cycle. The KITEx executable needs to be build from the CMakeLists.txt in the kite root directory while the KITE-tools executable needs to be build from the CMakeLists.txt in the tools subdirectory, so effectively using two build directories. This last bullet makes it a lot harder to integrate properly with the software that HPC centres use to manage their software stack such as EasyBuild or Spack. In early 2021, there is an old build script in the KITE directory: compile_Script.sh which again is GNU only and broken in many ways. Some of the source files needed for KITE-tools are not included. There is no way to configure the location of tools that are needed etc. It is really the kind of script that only works on the system of the developer.","title":"General information"},{"location":"generated/easyconfigs/q/Quantum-KITE/#specific-install-problems","text":"Compiling with the Intel compilers works but is hard, and there are several problems on the way Linking fails when compiling with an optimization level higher than -O0 without inter-procedural optimisations. The easy solution is to add -ipo to the compile and link command lines. Adapting the CMakeLists.txt files seems to be non-trivial (see also below). Simply deleting the lines that reset the compilers to the GNU compilers is and ensuring that the right compiler flags are used via -DCMAKE_... definitions when calling cmake turned out not to be enough. We still observed link problems for KITE-tools . It is not clear if this is due to the way CMake links the HDF5 libraries (with rpath) or not. The list of minor and major issues with the CMakeLists.txt files in the root directory and the tools subdirectory is extensive. At the top, CMAKE_C_COMPILER and CMAKE_CXX_COMPILER are redefined to be gcc and g++ respectively. Redefining those variables is a very bad idea... We removed those lines when using cmake. Setting CMAKE_CXX_STANDARD to 17 choses the wrong option for the Intel compiler. We removed those lines and selected the C++-standard via CMAKE_CXX_FLAGS . The messages printed during the HDF5 detection are wrong (the message for Hdf5 shows the information for Hdf5hl and vice-versa). The files also define a CORRECT_CODING_FLAGS variable whose function may be unclear, but the CMakeLists.txt -file for KITE-tools shows that it is meant to add additional warning flags. However, it is used inconsistently in both files and it should really be defined when calling cmake so that it can work with all compilers. When OpenMP is not detected, some of the CMAKE_CXX_FLAGS* -variables get overwritten rather than added to. This is again a bad idea. At several places, directories are added to the include path that are only relevant to the system of the developer. On other systems this may even lead to problems should that directory exist but, e.g., contain a wrong version of the include files. If essential software (such as the Eigen 3 include files) is missing, cmake should produce a clear error than just a status message. I'm not sure that not giving the option to overwrite the information that should go in the compiletime_info.h -file with something more meaningfull than what is auto-detected is a good idea.","title":"Specific install problems"},{"location":"generated/easyconfigs/q/Quantum-KITE/#easybuild","text":"There is no support for KITE in EasyBuild. Given the number of packages that are called kite , we decided to name the package Quantum-KITE after the URL of the web site.","title":"EasyBuild"},{"location":"generated/easyconfigs/q/Quantum-KITE/#commit-7e298ea-version-10-with-intel-2020a","text":"Starting point: commit 7e298ea of February 2.21 where the file version.md claims to be 1.0. We studied 3 approaches to install KITE Correcting the CMakeLists.txt files with the suggestions above did not work. There were still undefined symbols when linking KITE-tools . Adapting the compile_script.sh file (more a rewrite) was successful. See below for the script. It is not the most elegant procedure to integrate with EasyBuild though. Construct Makefiles for the Src subdirectory ( KITEx ) and tools/src subdirectory ( KITE-tools ). For the EasyBuild buld process, we decided to go for this approach and to inject them in the MakeCp EasyConfig file. To avoid having two build process we also created a top-level Makefile that simply starts both build processes. Since there are more files that need copying we didn't implement a make install but simply use a MakeCp EasyBlock. Note that these Makefiles are not suitable for code development as most dependencies on include files are missing. Hence an update of the include files will not trigger the correct rebuilds. Even though the executables KITEx and KITE-tools do not require Python at all, we decided to make kwant-bundle a dependency to ensure that it is used with a Python version with a compatible HDF5 library and since Pybinding is an official dependency in the documentation of KITE. Eigen is only a template library so is a build dependency. Since some examples import kite.py, this has been put in a directory in the PYTHONPATH and precompiled via postinstallcmds . #!/bin/bash ml calcua/2020a ml intel/2020a ml HDF5/1.10.6-intel-2020a-MPI ml Eigen/3.3.7 CXX = icpc CXXFLAGS = \"-O2 -ipo -march=core-avx2 -mtune=core-avx2 -std=c++17 -ftz -fp-speculation=safe -fp-model source -fPIC -qopenmp -Wall\" CXXDEF = \"-DVERBOSE=0 -DDEBUG=0\" CXXINCL = \"-I $EBROOTEIGEN /include -I $EBROOTHDF5 /include\" LDFLAGS = \"-L $EBROOTHDF5 /lib\" LDLIBS = \"-lhdf5_hl_cpp -lhdf5_cpp -lhdf5_hl -lhdf5 -lz -lm\" echo \"Compiling KITEx and KITE-tools. To have the full functionality, you need to have at least version 8 of gcc.\" echo \"If you do not, you will not be able to run guassian_wavepacket. To enable compiling with this feature, please edit this file and set WAVEPACKET=1\" WAVEPACKET = 1 # make the directory structure rm -rf build tools/build mkdir -p build mkdir -p tools/build mkdir -p bin echo \"Compiling KITEx\" cd build for i in ../Src/*.cpp ; do echo \"Compiling $i \" $CXX -c $i -DCOMPILE_WAVEPACKET = $WAVEPACKET $CXXDEF $CXXFLAGS -I../Src/ $CXXINCL done echo \"Linking\" $CXX $CXXFLAGS *.o -o ../bin/KITEx $LDFLAGS $LDLIBS echo \"Done compiling KITEx.\" cd .. echo \"Compiling KITE-tools\" cd tools/build cat >compiletime_info.h <<EOF #define MACHINE_NAME \"${VSC_INSTITUTE_CLUSTER}\" #define SYSTEM_NAME \"${VSC_ARCH_LOCAL}\" #define TODAY \"$(date)\" EOF for i in ../src/*.cpp ; do echo \"Compiling $i \" $CXX -c $i $CXXDEF $CXXFLAGS -I. -I../Src/ -I../src/cond_2order $CXXINCL done for i in ../src/cond_2order/*.cpp ; do echo \"Compiling $i \" $CXX -c $i $CXXDEF $CXXFLAGS -I. -I../Src/ -I../src/cond_2order $CXXINCL done for i in ../src/cond_dc/*.cpp ; do echo \"Compiling $i \" $CXX -c $i $CXXDEF $CXXFLAGS -I. -I../Src/ -I../src/cond_2order $CXXINCL done echo \"Linking\" $CXX $CXXFLAGS *.o -o ../../bin/KITE-tools $LDFLAGS $LDLIBS echo \"Done compiling KITE-tools.\" cd ../.. echo \"Add the directory $( pwd ) /bin to the PATH and make sure HDF5/1.10.6-intel-2020a-MPI or kwant-bundle/1.4.2-intel-2020a-Python-3.8.3 is loaded when running KITE.\"","title":"Commit 7e298ea - version 1.0 with Intel 2020a"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/","text":"QuantumESPRESSO instructions QuantumESPRESSO website QunatumESPRESSO development on GitLab Releases on GitLab Mirror on GitHub Releases on GitHub General information The downloads of QuantumESPRESSO are a complete mess as different options use a different directory structure and not all of them hence work with EasyBuild. Moreover, the names change from time to time also. The recommended download site accordint to the QuantumESPRESSO website is via the releases on GitHub which contain two sets of files with different content and different directory names after untaring. Files with something like ReleasePack in their name (changes from version to version) The regular compressed tar and zip file from GitHub. There are also downloads on GitLab Plug-ins The working of QuantumEspresso can be extended greatly by adding plug-ins. Some of the plug-ins are in fact auto-downloaded by QuantumESPRESSO. To find a list of recommended versions, check the install\\plugins_list file. Some of those links will actually be transformed into a URL for a particular version (certainly the case for d3q). Note that this file may even contain invalid links. Installation of most plugins is not checked by the developers of QuantumESPRESSO. It depends on the work of individual plugin maintainers and if they don't do their work, it may be that a package is shown as a possible plugin but does not work anymore. QE-GIPAW QE-GIPAW development on GitHub Releases on GitHub - always use the version number corresponding to the QE version number! d3q d3q development on SourceForge Releases on SourceForge Versions are specific for versions of QuantumESPRESSO. Recent versions of QuantumESPRESSO (certainly 6.4) auto-download this plug-in if its build is requested. To figure out the exact versions, check the releases on SourceForge. wannier90 wannier90 development on GitHub Releases on GitHub Compatible version: Check install/plugins_list in the QuantumESPRESSO directories. PLUMED - DROPPED PLUMED web site Support is only for outdated versions of QuantumESPRESSO via patching procedures provided by PLUMED. E.g., PLUMED 2.6, which was the current release of PLUMED when we started the development of this documentation in July 2020, supported QuantumESPRESSO 5.0.2 and 6.2, while the then current version of QuantumESPRESSO was 6.5. WanNT - Wannier Transport - DROPPED WanT web site Links on the web site are dead as QE-Forge does no longer exist. The official download site has disappeared. Hence we dropped this plugin from our installations. West - Without Empty STates - DROPPED WEST web site Releases on this site may be out-of-date. Main WEST development site on a private GitLab Releases WEST development mirror on GitHub Releases on GitHub At the time of writing (July 2020), WEST only supports QuantumESPRESSO 6.1 even though the WEST releases are much newer than that version of QuantumESPRESSO. Hence it is dropped from recent installations at UAntwerp. Yambo - DROPPED Yambo web site Yambo Wiki contains the installation instructions . Yambo development on GitHub Releases on GitHub Even though we used to install older versions of Yambo in older versions of QuantumESPRESSO (which required a patch) it is not clear if and how this can be done with newer versions of Yambo and QuantumESPRESSO. Hence we dropped support for Yambo in recent installations of QuantumESPRESSO at UAntwerp. Yambo is also known to cause strange crashes with at least some versions of the Intel compilers. EasyBuild support There is support in EasyBuild for QuantumESPRESSO which makes use of an EasyBlock . There are often problems though using the EasyBlock. They are due to the different downloads for QuantumESPRESSO with different directory structure, and the changes in that structure between versions of QuantumESPRESSO that occur more often than one would like. Also, it occasionally happens that the list of executables for plugins encoded in the EasyBlock gets out-of-date, resulting in crashes during the sanity check. 2020a toolchains - QuantumESPRESSO 6.4.1 and 6.5 with EasyBuild 4.2.2. The QuantumESPRESSO EasyBlock in EasyBuild 4.2.2 contains a bug in the list of binaries for the d3q plugin. The d3_import3py.x is not included in d3q 1.1.4 or newer. Hence we needed to adapt the EasyBlock to avoid the sanity check failing. 6.4.1: Source file https://github.com/QEF/q-e/releases/download/qe-6.4.1/qe-6.4.1_release_pack.tgz Builds in qe-6.4.1 (no further subdirectory). 6.5: Source file https://github.com/QEF/q-e/releases/download/qe-6.5/qe-6.5-ReleasePack.tgz Builds in qe-6.5 (no further subdirectory). Note: Source file https://github.com/QEF/q-e/archive/qe-6.5.tar.gz or https://gitlab.com/QEF/q-e/-/archive/qe-6.5/q-e-qe-6.5.tar.bz2: Builds in qe-6.5/q-e-qe-6.5 Fails during the install step as EasyBuild tries to copy from q-e-qe-6.5/bin instead of qe-6.5/q-e-qe-6.5.","title":"QuantumESPRESSO instructions"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#quantumespresso-instructions","text":"QuantumESPRESSO website QunatumESPRESSO development on GitLab Releases on GitLab Mirror on GitHub Releases on GitHub","title":"QuantumESPRESSO instructions"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#general-information","text":"The downloads of QuantumESPRESSO are a complete mess as different options use a different directory structure and not all of them hence work with EasyBuild. Moreover, the names change from time to time also. The recommended download site accordint to the QuantumESPRESSO website is via the releases on GitHub which contain two sets of files with different content and different directory names after untaring. Files with something like ReleasePack in their name (changes from version to version) The regular compressed tar and zip file from GitHub. There are also downloads on GitLab","title":"General information"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#plug-ins","text":"The working of QuantumEspresso can be extended greatly by adding plug-ins. Some of the plug-ins are in fact auto-downloaded by QuantumESPRESSO. To find a list of recommended versions, check the install\\plugins_list file. Some of those links will actually be transformed into a URL for a particular version (certainly the case for d3q). Note that this file may even contain invalid links. Installation of most plugins is not checked by the developers of QuantumESPRESSO. It depends on the work of individual plugin maintainers and if they don't do their work, it may be that a package is shown as a possible plugin but does not work anymore.","title":"Plug-ins"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#qe-gipaw","text":"QE-GIPAW development on GitHub Releases on GitHub - always use the version number corresponding to the QE version number!","title":"QE-GIPAW"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#d3q","text":"d3q development on SourceForge Releases on SourceForge Versions are specific for versions of QuantumESPRESSO. Recent versions of QuantumESPRESSO (certainly 6.4) auto-download this plug-in if its build is requested. To figure out the exact versions, check the releases on SourceForge.","title":"d3q"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#wannier90","text":"wannier90 development on GitHub Releases on GitHub Compatible version: Check install/plugins_list in the QuantumESPRESSO directories.","title":"wannier90"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#plumed-dropped","text":"PLUMED web site Support is only for outdated versions of QuantumESPRESSO via patching procedures provided by PLUMED. E.g., PLUMED 2.6, which was the current release of PLUMED when we started the development of this documentation in July 2020, supported QuantumESPRESSO 5.0.2 and 6.2, while the then current version of QuantumESPRESSO was 6.5.","title":"PLUMED - DROPPED"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#wannt-wannier-transport-dropped","text":"WanT web site Links on the web site are dead as QE-Forge does no longer exist. The official download site has disappeared. Hence we dropped this plugin from our installations.","title":"WanNT - Wannier Transport - DROPPED"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#west-without-empty-states-dropped","text":"WEST web site Releases on this site may be out-of-date. Main WEST development site on a private GitLab Releases WEST development mirror on GitHub Releases on GitHub At the time of writing (July 2020), WEST only supports QuantumESPRESSO 6.1 even though the WEST releases are much newer than that version of QuantumESPRESSO. Hence it is dropped from recent installations at UAntwerp.","title":"West - Without Empty STates - DROPPED"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#yambo-dropped","text":"Yambo web site Yambo Wiki contains the installation instructions . Yambo development on GitHub Releases on GitHub Even though we used to install older versions of Yambo in older versions of QuantumESPRESSO (which required a patch) it is not clear if and how this can be done with newer versions of Yambo and QuantumESPRESSO. Hence we dropped support for Yambo in recent installations of QuantumESPRESSO at UAntwerp. Yambo is also known to cause strange crashes with at least some versions of the Intel compilers.","title":"Yambo - DROPPED"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#easybuild-support","text":"There is support in EasyBuild for QuantumESPRESSO which makes use of an EasyBlock . There are often problems though using the EasyBlock. They are due to the different downloads for QuantumESPRESSO with different directory structure, and the changes in that structure between versions of QuantumESPRESSO that occur more often than one would like. Also, it occasionally happens that the list of executables for plugins encoded in the EasyBlock gets out-of-date, resulting in crashes during the sanity check.","title":"EasyBuild support"},{"location":"generated/easyconfigs/q/QuantumESPRESSO/#2020a-toolchains-quantumespresso-641-and-65-with-easybuild-422","text":"The QuantumESPRESSO EasyBlock in EasyBuild 4.2.2 contains a bug in the list of binaries for the d3q plugin. The d3_import3py.x is not included in d3q 1.1.4 or newer. Hence we needed to adapt the EasyBlock to avoid the sanity check failing. 6.4.1: Source file https://github.com/QEF/q-e/releases/download/qe-6.4.1/qe-6.4.1_release_pack.tgz Builds in qe-6.4.1 (no further subdirectory). 6.5: Source file https://github.com/QEF/q-e/releases/download/qe-6.5/qe-6.5-ReleasePack.tgz Builds in qe-6.5 (no further subdirectory). Note: Source file https://github.com/QEF/q-e/archive/qe-6.5.tar.gz or https://gitlab.com/QEF/q-e/-/archive/qe-6.5/q-e-qe-6.5.tar.bz2: Builds in qe-6.5/q-e-qe-6.5 Fails during the install step as EasyBuild tries to copy from q-e-qe-6.5/bin instead of qe-6.5/q-e-qe-6.5.","title":"2020a toolchains - QuantumESPRESSO 6.4.1 and 6.5 with EasyBuild 4.2.2."},{"location":"generated/easyconfigs/r/R/","text":"R instructions R web site Comprehensive R Archive Network Bioconductor General information Each R package corresponds to a subdirectory in lib64/R/library . These subdirectories have a very fixed structure. Shared objects corresponding to a package are in the subdirectory libs of the package directory. Checking if a package can be loaded: Start R and execute library(<ext>) with <ext> the name of the package. Problems with specific packages Rmpi Rmpi support in EasyBuild for the Intel toolchain consists of two components: A patch that corrects the name of the MPI library in the configure script (as it is not called libmpich even though Intel MPI is derived from that implementation and the other libraries don't even exist anymore). An EasyBlock that generates the --configure-args argument. On our local installation of the Intel toolchain, the EasyBlock generates the wrong values for --with-Rmpi-libpath and --with-Rmpi-include in --configure-args . We solve this in two components We add the right --configure-args flag via installopts . Since forcing the easyblock for the Rmpi extension to RPackage doesn't seem to work in the R easyconfig and since removing that EasyBlock from the EasyBuild installation is prone to error as it needs to be redone whenever a new version of EasyBuild is installed, we include an empty EasyBlock for Rmpi in our own EasyBlock repository that overwrites the default one. Note that we have corrected this problem in the 2020a Intel modules. Hence the regular Rmpi EasyBlock can now be used. We continue to compile the old way however as the dummy EasyBlock is still in place to be able to restore former R configurations. XML There is an EasyBlock that adds a flag pointing to the zlib library. That turns out not to be needed on our installation and since it also refers to EBROOTZLIB, doesn't even work in our case. As for Rmpi, rather than removing the EasyBlock, we use an empty one in our custom EasyBlocks. tseries version 0.10-47 In version 0.10-47, tseries introduced a new file, cfuncs.f95 . This file is in free form source format. There is one other Fortran file, dsumsl.f , which is in fixed form source format. The problem is that the Intel compiler does not recognize the (non-standard) .f95 extension and considers it as fixed form source format. On the other hand, adding the option -free through PKG_FFLAGS also does not work as then all source files are considered to be in free form source format. Possible solutions: Unpack and rename, but it seems that this can not be done easily in EasyBuild 4.2 as the unpack_sources of RPackage doesn't seem to work the way it should. An idea of Davide Vanzo: Add the directive !DIR$ FREEFORM to the top of cfuncs.f95 . This can be done with a patch file. Rtsne and several other packages that produce error #308: member \"std::complex ::_M_value\" Rtsne does not compile anymore with the Intel C++ compiler. It produces a lot of error #308. The root of the problems is actually in the GCC complex header file and not in Rtsne itself. However, R shouldn't be using that header file in the first place but should use the one in the Intel system directories. Possible solutions: Make sure the option -diag-disable 308 is used. This is sometimes easier said than done. In some packages you can inject this option by setting the environment variable PKG_CXXFLAGS . But in recent versions of Rtsne this does not work anymore. It sets the variable in its own src/Makevars file and refuses to pick it up from the environment. One workaround is to change it permanently in all CXXFLAGS lines in the system Makeconf file ($EBROOTR/lib64/R/etc/Makeconf). TODO: One way to accomplish this may be to simply add that option already to the CXXFLAGS when compiling R. Some sources suggest to use -isystem<dir> to add the Intel compiler include file directory to the front of the directory search path. This can also be done by editing Makeconf . However, even though the line was added to the compiler command line, R kept using the complex header file from GCC which caused the error. git2r git2r depends on libgit2 but will use an internal version if none can be found. git2r optionally depends on LibSSH2 (libssh2-devel package on CentOS). It should find those libraries automatically if they are in a standard location where configure can find them. git2r also uses the openssl package and zlib. openssl openssl provides an interface to libssl and libcrypto and hence depends on development packages for those libraries. On CentOS 8 the necessary headers are provided by openssl-devel , the libraries by openssl-libs . V8 The V8 package needs v8-devel which was omitted in early (and later?) versions of CentOS 8, so one needs to look elsewhere. Interesting issue thread (issue 84) on the V8 GitHub . Problems on CentOS 8 and some other linux versions with a recent glibc (2.27 or later?) Compilation fails in artihmetic.c . A good source of information for the deeper roots of the cause is the answer from rolandd to a question in the Intel community forum . The problem has to do with the removel of the SVID math library exception handling from math.h in glibc 2.27 and later (and the corresponding macro definitions, which is where the compilation fails). The test in configure fails with gcc during the link phase but does not fail with the Intel compiler as it has a replacement function but the corresponding Intel header does not define the excpetion structure for whatever reason. The workaround suggested in that forum post is to remove the HAVE_MATHERR symbol from src/include/config.h between the configure and the build steps. Scripts to ease testing and writing EasyConfigs When installing version 4.0.2, we developed three Perl scripts that help to generate various files to ease the installation process. This scripts start from a list file. This is a tab-separated file with up to four fields per record 1. Type of the package, with currently 4 different values: 1. CRAN for a package that can be found on CRAN 2. BIOC for a regular Bioconductor package 3. BIOCDA for a Bioconductor data annotation package 4. BIOCEX for a Bioconductor experiment package 2. Name of the package 3. Environment variable to be specified when building 4. Add an additional remark if this field is non-empty. It is used simply to warn that some manual intervention will be needed in the files generated by\\ the scripts. The scripts all work as a filter, i.e., they take input from standard input and print to standard output. So simply use I/O redirection to use the scripts. The scripts are: * list2htmml.pl : Generates a HTML-file with a link to the repository for each package. It is useful to check version numbers etc. * list2ext_list.pl : Generates the ext_list structure without the version numbers. Some packages may need some more work than just adding the version number. * list2R.pl generates a R script to install the packages instead. Again, it may need some work. We found this very useful to install packages by hand to check for dependencies when easy_update failed on us. sf in combination with ncdf4 The package sf links to PROJ, GDAL and GEOS. The package ncdf4 is an interface to the netCDF 4 libraries (version 4.1 and later). If you explicitly load netCDF as a dependency, be careful as it already comes in as a dependency of GDAL, so make sure you use the right netCDF library. Otherwise GDAL may fail to load leading to a failure to load the package sf . EasyBuild This documentation was started when installing R 4.0.1 in the 2020a toolchains. We started from an older EasyConfig in the R EasyBuilders repository and updated from there as we have added several packages requested by our users throughout the years. We do also have a number of Bioconductor modules in our base R setup. Version 4.0.2 in the Intel toolchains Adapted the syntax in the exts_list to the current way of doing things in EasyBuild. Moved initialisation of Bioconductor up to the front of the extensions list. Packages that require special care or have caused trouble recently are moved up the list as much as possible to detect compile problems early. Package tseries: We tried several solutions and stuck to the last one in the end as there was no working solution without a patch file anyway: Using unpack_sources , then using preconfigopts to rename cfuncs.f95 and adapt the corresponding line in MD5 : Does not work due to bugs in the RPackage EasyBlock. Using a patch for MD5 that simply changes cfuncs.f95 to cfuncs.f90 in that file to force unpacking of the sources, then renaming cfuncs.f95 in preinstallopts : Works. Using a patch from Davide Vanzo that adds !DIR$ FREEFORM to the top of cfuncs.f95 also works. In the end we used this solution as there is no solution without a patch file that actually works. Rmpi: Even though we corrected the problem in the Intel modules that cause the Rmpi EasyBlock to fail, we continue using our previous approach with installopts and the empty EasyBlock to be able to recompile old versions without having to change the EasyConfig. Rtsne: We decided to simply add -diag-disable 308 to CXXFLAGS when compiling R itself, even though R itself doesn't need it. But we assume it will do no harm to other packages anyway. XML: This package install perfectly fine and can find the libraries it needs (which come from the Baselibs packages in this repository), but there is an EasyBlock that sits in the way since that checks for EBROOTZLIB and sets an option which is not needed at all. We moved that package to the front of the list so that problems become clear quickly and have our own empty custom easyblock as the easyblock parameter doesn't seem to work in an extension list. ModelMetrics supports OpenMP. We've enabled that support through PKG_CXXFLAGS and moved that package far to the front so that we don't have to scroll that far through the logs to check if everything is OK. When compiling the git2r package, we use the embedded libgit2 implementation and a libssh2 installed on the system. Package penalized: We define ARMA_ALLOW_FAKE_GCC (via PKG_CXXFLAGS ) which according to the messages in the log when this symbol is not set, should give enable data alignment. For CentOS 8 we need to remove HAVE_MATHERR from src/include/config.h between the configure and build steps. It turns out that the only solution to do this is via prebuildopts as the R EasyBlock adds further options after adding configopts to the configure command line. Future version RcppParallel: Illegal options because it set compiler to gcc? But it does use icpc as intended. Try to move packages that provide interfaces to external libraries forward as much as possible?","title":"R instructions"},{"location":"generated/easyconfigs/r/R/#r-instructions","text":"R web site Comprehensive R Archive Network Bioconductor","title":"R instructions"},{"location":"generated/easyconfigs/r/R/#general-information","text":"Each R package corresponds to a subdirectory in lib64/R/library . These subdirectories have a very fixed structure. Shared objects corresponding to a package are in the subdirectory libs of the package directory. Checking if a package can be loaded: Start R and execute library(<ext>) with <ext> the name of the package.","title":"General information"},{"location":"generated/easyconfigs/r/R/#problems-with-specific-packages","text":"","title":"Problems with specific packages"},{"location":"generated/easyconfigs/r/R/#rmpi","text":"Rmpi support in EasyBuild for the Intel toolchain consists of two components: A patch that corrects the name of the MPI library in the configure script (as it is not called libmpich even though Intel MPI is derived from that implementation and the other libraries don't even exist anymore). An EasyBlock that generates the --configure-args argument. On our local installation of the Intel toolchain, the EasyBlock generates the wrong values for --with-Rmpi-libpath and --with-Rmpi-include in --configure-args . We solve this in two components We add the right --configure-args flag via installopts . Since forcing the easyblock for the Rmpi extension to RPackage doesn't seem to work in the R easyconfig and since removing that EasyBlock from the EasyBuild installation is prone to error as it needs to be redone whenever a new version of EasyBuild is installed, we include an empty EasyBlock for Rmpi in our own EasyBlock repository that overwrites the default one. Note that we have corrected this problem in the 2020a Intel modules. Hence the regular Rmpi EasyBlock can now be used. We continue to compile the old way however as the dummy EasyBlock is still in place to be able to restore former R configurations.","title":"Rmpi"},{"location":"generated/easyconfigs/r/R/#xml","text":"There is an EasyBlock that adds a flag pointing to the zlib library. That turns out not to be needed on our installation and since it also refers to EBROOTZLIB, doesn't even work in our case. As for Rmpi, rather than removing the EasyBlock, we use an empty one in our custom EasyBlocks.","title":"XML"},{"location":"generated/easyconfigs/r/R/#tseries-version-010-47","text":"In version 0.10-47, tseries introduced a new file, cfuncs.f95 . This file is in free form source format. There is one other Fortran file, dsumsl.f , which is in fixed form source format. The problem is that the Intel compiler does not recognize the (non-standard) .f95 extension and considers it as fixed form source format. On the other hand, adding the option -free through PKG_FFLAGS also does not work as then all source files are considered to be in free form source format. Possible solutions: Unpack and rename, but it seems that this can not be done easily in EasyBuild 4.2 as the unpack_sources of RPackage doesn't seem to work the way it should. An idea of Davide Vanzo: Add the directive !DIR$ FREEFORM to the top of cfuncs.f95 . This can be done with a patch file.","title":"tseries version 0.10-47"},{"location":"generated/easyconfigs/r/R/#rtsne-and-several-other-packages-that-produce-error-308-member-stdcomplex_m_value","text":"Rtsne does not compile anymore with the Intel C++ compiler. It produces a lot of error #308. The root of the problems is actually in the GCC complex header file and not in Rtsne itself. However, R shouldn't be using that header file in the first place but should use the one in the Intel system directories. Possible solutions: Make sure the option -diag-disable 308 is used. This is sometimes easier said than done. In some packages you can inject this option by setting the environment variable PKG_CXXFLAGS . But in recent versions of Rtsne this does not work anymore. It sets the variable in its own src/Makevars file and refuses to pick it up from the environment. One workaround is to change it permanently in all CXXFLAGS lines in the system Makeconf file ($EBROOTR/lib64/R/etc/Makeconf). TODO: One way to accomplish this may be to simply add that option already to the CXXFLAGS when compiling R. Some sources suggest to use -isystem<dir> to add the Intel compiler include file directory to the front of the directory search path. This can also be done by editing Makeconf . However, even though the line was added to the compiler command line, R kept using the complex header file from GCC which caused the error.","title":"Rtsne and several other packages that produce error #308: member \"std::complex::_M_value\""},{"location":"generated/easyconfigs/r/R/#git2r","text":"git2r depends on libgit2 but will use an internal version if none can be found. git2r optionally depends on LibSSH2 (libssh2-devel package on CentOS). It should find those libraries automatically if they are in a standard location where configure can find them. git2r also uses the openssl package and zlib.","title":"git2r"},{"location":"generated/easyconfigs/r/R/#openssl","text":"openssl provides an interface to libssl and libcrypto and hence depends on development packages for those libraries. On CentOS 8 the necessary headers are provided by openssl-devel , the libraries by openssl-libs .","title":"openssl"},{"location":"generated/easyconfigs/r/R/#v8","text":"The V8 package needs v8-devel which was omitted in early (and later?) versions of CentOS 8, so one needs to look elsewhere. Interesting issue thread (issue 84) on the V8 GitHub .","title":"V8"},{"location":"generated/easyconfigs/r/R/#problems-on-centos-8-and-some-other-linux-versions-with-a-recent-glibc-227-or","text":"later?) Compilation fails in artihmetic.c . A good source of information for the deeper roots of the cause is the answer from rolandd to a question in the Intel community forum . The problem has to do with the removel of the SVID math library exception handling from math.h in glibc 2.27 and later (and the corresponding macro definitions, which is where the compilation fails). The test in configure fails with gcc during the link phase but does not fail with the Intel compiler as it has a replacement function but the corresponding Intel header does not define the excpetion structure for whatever reason. The workaround suggested in that forum post is to remove the HAVE_MATHERR symbol from src/include/config.h between the configure and the build steps.","title":"Problems on CentOS 8 and some other linux versions with a recent glibc (2.27 or"},{"location":"generated/easyconfigs/r/R/#scripts-to-ease-testing-and-writing-easyconfigs","text":"When installing version 4.0.2, we developed three Perl scripts that help to generate various files to ease the installation process. This scripts start from a list file. This is a tab-separated file with up to four fields per record 1. Type of the package, with currently 4 different values: 1. CRAN for a package that can be found on CRAN 2. BIOC for a regular Bioconductor package 3. BIOCDA for a Bioconductor data annotation package 4. BIOCEX for a Bioconductor experiment package 2. Name of the package 3. Environment variable to be specified when building 4. Add an additional remark if this field is non-empty. It is used simply to warn that some manual intervention will be needed in the files generated by\\ the scripts. The scripts all work as a filter, i.e., they take input from standard input and print to standard output. So simply use I/O redirection to use the scripts. The scripts are: * list2htmml.pl : Generates a HTML-file with a link to the repository for each package. It is useful to check version numbers etc. * list2ext_list.pl : Generates the ext_list structure without the version numbers. Some packages may need some more work than just adding the version number. * list2R.pl generates a R script to install the packages instead. Again, it may need some work. We found this very useful to install packages by hand to check for dependencies when easy_update failed on us.","title":"Scripts to ease testing and writing EasyConfigs"},{"location":"generated/easyconfigs/r/R/#sf-in-combination-with-ncdf4","text":"The package sf links to PROJ, GDAL and GEOS. The package ncdf4 is an interface to the netCDF 4 libraries (version 4.1 and later). If you explicitly load netCDF as a dependency, be careful as it already comes in as a dependency of GDAL, so make sure you use the right netCDF library. Otherwise GDAL may fail to load leading to a failure to load the package sf .","title":"sf in combination with ncdf4"},{"location":"generated/easyconfigs/r/R/#easybuild","text":"This documentation was started when installing R 4.0.1 in the 2020a toolchains. We started from an older EasyConfig in the R EasyBuilders repository and updated from there as we have added several packages requested by our users throughout the years. We do also have a number of Bioconductor modules in our base R setup.","title":"EasyBuild"},{"location":"generated/easyconfigs/r/R/#version-402-in-the-intel-toolchains","text":"Adapted the syntax in the exts_list to the current way of doing things in EasyBuild. Moved initialisation of Bioconductor up to the front of the extensions list. Packages that require special care or have caused trouble recently are moved up the list as much as possible to detect compile problems early. Package tseries: We tried several solutions and stuck to the last one in the end as there was no working solution without a patch file anyway: Using unpack_sources , then using preconfigopts to rename cfuncs.f95 and adapt the corresponding line in MD5 : Does not work due to bugs in the RPackage EasyBlock. Using a patch for MD5 that simply changes cfuncs.f95 to cfuncs.f90 in that file to force unpacking of the sources, then renaming cfuncs.f95 in preinstallopts : Works. Using a patch from Davide Vanzo that adds !DIR$ FREEFORM to the top of cfuncs.f95 also works. In the end we used this solution as there is no solution without a patch file that actually works. Rmpi: Even though we corrected the problem in the Intel modules that cause the Rmpi EasyBlock to fail, we continue using our previous approach with installopts and the empty EasyBlock to be able to recompile old versions without having to change the EasyConfig. Rtsne: We decided to simply add -diag-disable 308 to CXXFLAGS when compiling R itself, even though R itself doesn't need it. But we assume it will do no harm to other packages anyway. XML: This package install perfectly fine and can find the libraries it needs (which come from the Baselibs packages in this repository), but there is an EasyBlock that sits in the way since that checks for EBROOTZLIB and sets an option which is not needed at all. We moved that package to the front of the list so that problems become clear quickly and have our own empty custom easyblock as the easyblock parameter doesn't seem to work in an extension list. ModelMetrics supports OpenMP. We've enabled that support through PKG_CXXFLAGS and moved that package far to the front so that we don't have to scroll that far through the logs to check if everything is OK. When compiling the git2r package, we use the embedded libgit2 implementation and a libssh2 installed on the system. Package penalized: We define ARMA_ALLOW_FAKE_GCC (via PKG_CXXFLAGS ) which according to the messages in the log when this symbol is not set, should give enable data alignment. For CentOS 8 we need to remove HAVE_MATHERR from src/include/config.h between the configure and build steps. It turns out that the only solution to do this is via prebuildopts as the R EasyBlock adds further options after adding configopts to the configure command line.","title":"Version 4.0.2 in the Intel toolchains"},{"location":"generated/easyconfigs/r/R/#future-version","text":"RcppParallel: Illegal options because it set compiler to gcc? But it does use icpc as intended. Try to move packages that provide interfaces to external libraries forward as much as possible?","title":"Future version"},{"location":"generated/easyconfigs/r/RAxML-NG/","text":"RAxML-NG installation instructions GitHub The authors mentioned seem to have one thing in common: They are or have been at HITS, the Heidelberg Institute for Theoretical Studies , so that is the likely source of the code. General information RAxML has a number of dependencies that have no official releases. They are not included in the source code if you download a tar file from the GitHub repository. Therefore it is better to get the sources via a recursive git clone. Dependencies: RAxML-NG uses the following libraries ( libs subdirectory) terraphast which is a clone of the original terraphast Github is on the upsj account developed by Tobias Ribizel from KIT, Karlsruhe. pll-modules which has a different author (Diego Darriba, now at Coru\u00f1a but before at HITS). It looks like this subproject is pretty dead at the moment. pll-modules uses libpll which has yet another author, Tomas Flouri who seems to be linked to both UCL, london and HITS. The libpll code seems to be as dead as pll-modules, both saw their last update in June 2017. Stored in the libs/libpll subdirectory of libs/pll-modules . RAxML-NG supports distributed memory parallelisation with MPI and shared memory parallelisation using pthreads. It is not clear if using both together really gives a hybrid executable as the documentation is mostly missing. In any case, MPI is only used by the RAxML-NG code itself, not by any of the dependencies (terraphast or pll-modules/libpll). libpll has different code for SSE, AVX and AVX2, and a generic code path as an alternative. The SSE, AVX and AVX2 codes use intrinsics so this may give problems with some compilers that don't support certain intrinsics. Even if a system supports AVX2, it is best to leave the lower options activated also as a lower code path is sometimes used if there is no benefit of using a higher one. All three code paths should always be activated if supported, unless -DENABLE_SSE=false , -DENABLE_AVX=false and/or -DENABLE_AVX2=false are specified. Check what CMake does to be sure if the right code paths are compiled into the code. The main RAxML-NG code is C++. Hence the CXX-range of options matter. pll-modules and libpll is C-code. terraphast is C++-code. Moreover, it uses some intrinsics that may be compiler-specific in the code in the lib subdirectory. There are some header files to solve portability problems, with support for GCC, Clang and the Microsoft compiler. It does turn out that all the directives that are used in the GCC/Clang version are also supported by the Intel compiler since the 2018 releases. This is also the component of the code that can use the GNU GMP library. CMake configuration as it is in the code Variable Default Meaning RAXML_BINARY_NAME Custom RAxML-NG binary name USE_LIBPLL_CMAKE ON Use CMake to build libpll and pll-modules. Will be forced to ON when building as a library BUILD_AS_LIBRARY OFF Build RAxML-NG as shared library (instead of stand-alone executable). Naming is derived from the name of the matching executable. USE_PTHREADS ON Enable multi-threading support (PTHREADS) USE_MPI OFF Enable MPI support USE_VCF OFF ??? No explanation available ENABLE_PLLMOD_SIMD OFF Enable SIMD instructions in pll-modules (non-portable but slightly faster) STATIC_BUILD OFF Build static binary (don't use with MPI!) USE_TERRAPHAST ON Use phylogentic terraces library (terraphast) USE_GMP OFF Use Gnu Muliple precision (GMP) library, only relevant for terraphast MPI_CXX_COMPILER Used if USED_MPI is ON MPI_CXX_COMPILER_FLAGS Used if USE_MPI is ON ENABLE_SSE True Used to enable or explicitly disable SSE3 optimizations in libpll ENABLE_AVX True Used to enable or explicitly disable AVX optimizations in libpll ENABLE_AVX2 True Used to enable or explicitly disable AVX2 optimizations in libpll When no custom binary name is given (which is advised), the name of the binary depends on a number of options. No MPI, no static linking: raxml-ng No MPI, static linking: raxml-ng-static MPI, no static linking: raxml-ng-mpi MPI with static linking is not supported?? There are more options, e.g., PLLMOD_DEBUG , that are only relevant to code development and not to creating a user installation. It is really the implementation of ENABLE_RAXML_SIMD and ENABLE_PLLMOD_SIMD that is problematic (at least in version 0.9.0, when this was written) as it currently enforces the selection between very bad and bad compiler options for performance on modern hardware. The CMAKE_C_... and CMAKE_CXX_... are used in the recipe but not taken from the environment or defines on the command line as they have the type INTERNAL . From version 0.9.0 on, make install is supported to install the binaries, but it does not work correctly when installing RAxML-NG as a shared library as it is put in bin instead of in lib or lib64. EasyConfigs EasyConfigs were developed at UAntwerp as there was no support for the -NG version in EasyBuild when we started. By now there is an official EasuBuilders recipe for RAxML-NG that however suffers from most of the problems that we indicate below. This documentation was started when installing version 0.9.0. However, we did try to reconstruct what we did in earlier versions and cover this in the documentation also. The main problem with installing RAxML-NG is the poor quality of the build scripts. They impose certain compiler options that 1. are only valid for GCC and 2. are for ancient systems (Ivy Bridge and before) and suboptimal for later systems. As the root CMake file then further calls configure scripts Version 0.4.1 This was still without terraphast. We needed to adapt the main CMakeList.txt file to recognize the Intel compilers. This was not sufficient though, libpll was still compiled in a suboptimal way (no -xHost ). The recipe does not do a download of the sources from GitHub, this still has to be done manually. Version 0.8.1 This version now comes with with terraphast which we didn't get to compile with the Intel compiler. Since we didn't have a full FOSS toolchain with working MPI set up at the time, we did not build a distributed memory version. We made similar changes to the main CMakeLists.txt file. We now had to stick to the GCC compilers though. Which also were used in a suboptimal way for the dependencies. We tried to add the Intel compiler to the recognized compilers, and additional options for the SIMD settings chosing better compiler options. Version 0.9.0 Rather than using the system of the EasyBuilders recipe of downloading 4 files, 3 of them based on commits that have to be looked up first, we use EasyBuild's feature to use a recursive download of a tag on the GitHub and assemble the download file that way. This saves us from looking up that commits that are needed for the dependencies every time we install a new version. Added GMP which turned out to be useless in the original code as the support in a subproject was turned off, see below. The CMake configuration now supports \"make install\" so we now use that to install the software. Using the default CMakeLists.txt file: Compiled with GCCcore for Hopper (Ivybridge architecture) as this is the only machine for which the compiler flags are somewhat reasonable. This still implies that the terraphast component is compiled without any optimizations, just the default generic architecture -O0 of GCC due to bugs in the configuration procedure. We made a lot of fixes to the whole CMake configuration process (see the subsections below). We ended up with the following configuration: Ensure that the compiler options are picked up from EasyBuild and that the defaults from CMake are fully surpressed by setting ENFORCE_COMPILER_FLAGS to OFF and initializing all CMAKE_C FLAGS (some are already done automatically by EasyBuild). Turn on SIMD optimizations for pll-modules (in fact, for libpll), but setting the specific optimization options in SSE_FLAGS, AVX_FLAGS and AVX2_FLAGS off as they should already be covered by the options to optimize for a specific architecture chosen by EasyBuild (and if they are not, that level of SIMD is not supported anyway by the architecture). We turn on terraphast and compile it with GMP support. Threading support is also turned on, also in the MPI version. There is something we cannot explain yet: libpll did compile on Hopper (Ivybridge) using only -xHost and no AVX2-specific option. CMake did not detect that there was no AVX2 and the code also compiled without problems. So it looks like what is called AVX2 might not be AVX2? OR what happened to the intrinsics, becasue they do look like AVX2 intrinsics... Remarks on the configuration and compilation process pll-modules and libpll pick up compiler options that are passed through the environment, but add additional ones that may not be optimal for the chosen compiler or not be valid. terraphast looked like a complete disaster, but this was more due to RAxML-NG making a mess of compiler settings and configuration. The main code is compiled OK now and with a few corrections to the CMakeLists.txt file and a bug fix which was not yet included in the version that ships with RAxML-NG, it now compiles OK with Intel also. Main RAxML-NG package Problems with the main code configuration: One problematic block is the block that starts with if (ENABLE_RAXML_SIMD ) in the CMakeLists.txt file in the root directory of RAxML-NG as it imposes compiler options and does so at the end, overwriting options that a knowledgeble user may consider more appropriate. Moreover, a compiler may have different options to activate AVX or SSE3 support. The variables that are defined ( __SSE3 and __AVX ) aren't used in the code. CMAKE_CXX_FLAGS_DEBUG and several similar variables were set to type INTERNAL rather than STRING , probihibiting the user to overwrite them, which is a very bad idea. After all, the user may need different flags for his/her compiler. Adding -D_RAXML_PTHREADS -pthread to CMAKE_C_FLAGS while compiling the dependencies, does not make sense at all as that information is not used in those codes anyway. The block that is executed when ENABLE_GMP is ON comes too early. Moreover, it assumes that the terraphast initialization will indeed succeed in locating the GMP libraries which is also not the case, especially since TERRAPHAST_USE_GMP seems to be turned off in raxml-ng/libs/CMakeLists.txt . Modifications to CMakeLists.txt in raxml-ng , the main RAxML-ng directory: Also recognize the Intel compiler. Made compiler arguments that are added dependent on the compiler as not all options are accepted by all compilers. Made it possible for a user to overwrite all default compiler flags so that it is possible to more or less adapt to other compilers, or chose better options for a particular architecture. Removed the block that starts with if (ENABLE_GMP) as it does nothing useful: GMP is only used through terraphast, and when done correctly, no information about GMP is needed in the main project. Removed the problematic block that starts with if (ENABLE_RAXML_SIMD ) and\\ the option ENABLE_RAXML_SIMD as it only hurts. Provide a lot more feedback to the user. It may not be important to a very inexperienced user, but it does help an experienced user to check if everything works as intended. Modifications to CMakeLists.txt in raxml-ng/src : The terrace library from the terraphast subproject was included in the wrong way and therefore the GMP libraries were also not found when linking. Non-essential modifications that provide additional output to diagnose what CMake is doing. It is not strictly needed to patch this file. Modifications to CMakeLists.txt in raxml-ng/test/src : Got rid of the GMP block as that is not needed if the terraces project is included in the right way (which is the case now with the patches to CMakeLists.txt in `raxml-ng ). Modifications to CMakeLists.txt in raxml-ng/libs : USE_GMP or its processed value in ENABLE_GMP was not passed on to the terraphast code. This has been corrected. It was probably due to the author not being able to figure out how to link properly to the GMP library. USE_THREADS or its processed value ENABLE_THREADS is not passed on to the terraphast code. This has been corrected. It is not clear if this is due to problems with threading between the main RAxML-NG and the terraphast code or not. pll-modules and libpll pll-modules uses the compiler options passed to it in CMAKE_C_FLAGS augmented with options hard-coded in PLLMOD_CFLAGS defined in its src subdirectory. It does add an optimization option to this variable also, which is a bad idea as they belong are better set in CMAKE_C_FLAGS_RELEASE or CMAKE_C_FLAGS (and is set as the options are passed earlier in the hierarchy). libpll uses the compiler options passed to in it in CMAKE_C_FLAGS augmented with options hard-coded in LIBPLL_BASE_FLAGS defined in its src subdirectory. And it is not clear what it does with the SIMD_FLAGS that it builds beyond testing for SSE3, AVX and AVX2 support. The individual SIMD flags for each type are used though when compiling a file of that type. Hence it might be better to allow to replace them by empty options to not conflict with other target-specific options that may be specified. Modifications to CMakeLists.txt in libpll/src : Removed the optimization level from LIBPLL_BASE_FLAGS as this should be set in CMAKE_C_FLAGS_RELEASE or CMAKE_C_FLAGS (and is set as the options are passed earlier in the hierarchy). Note that if libpll is to be used as an independent component, some of the code to detect the C compiler and set appropriate defaults for these variables should probably be copied from raxml-ng/CMakeLists.txt to libpll/CMakeLists.txt . Allow the user to specify SSE_FLAGS, AVX_FLAGS and AVX2_FLAGS, with the old GNU values as the default. If the user specifies architectural compile options through CMAKE_C_FLAGS that support certain SIMD extensions, these options are not needed, not to detect if support is present and not to compile the code. Modifications to CMakeLists.txt in pll-modules : Only non-essential modifications producing additional output to make it easier to follow what CMake is doing have been added to the file. Modifications to CMakeLists.txt in pll-modules/src : Removed the optimization option -O3 from PLLMOD_CFLAGS as it is better to get those from CMAKE_C_FLAGS_RELEASE or CMAKE_C_FLAGS . Note that if pll-modules is to be used as an independent component, some of the code to detect the C compiler and set appropriate defaults for these variables should probably be copied from raxml-ng/CMakeLists.txt to pll-modules/CMakeLists.txt . terraphast Analysis of portability problems in the code lib subdirectory: The files gcc-clang/intrinsics.hpp and cl/intrinsics.hpp map a number of functions to intrinsics for respectively GCC and Clang and the Microsoft C/C++ compiler. The GNU/Clang version uses: __builtin_popcountll : This intrinsic is supported in the Intel 16 (2016) and later compilers. We didn't check older Intel compilers. __builtin_ctzll and __builtin_clzll : Supported in the Intel compilers version 16 (2016) and later. I didn't try older compilers. __builtin_add_overflow , __builtin_mul_overflow are supported since version 18 (2018 suite). The whole configuration process is organised from a single CMakeLists.txt file in the root terraphast directory. There is a double problem with the option TERRAPHAST_ARCH_NATIVE : The option it adds is not valid for all compilers. E.g., for the Intel compiler it should be -xHost (which then is bad for AMD) It does not make sense to add -march=native automatically if you don't add other optimization options, as the default for the GNU C++ compiler is -O0 . The support for the GNU GMP library is broken. For some reason, the Makefile sees gmpxx and gmp as targets rather than libraries it should add when linking, resulting in a message from make it cannot build those targets. Building libterraces.a works (-DTERRAPHAST_BUILD_CLIB=OFF -DTERRAPHAST_BUILD_APPS=OFF -DTERRAPHAST_BUILD_TESTS=OFF) and this is all that is needed to build RAxML-NG. Building the C library also works (-DTERRAPHAST_BUILD_CLIB=ON which is the default). Building the apps fails (-DTERRAPHAST_BUILD_APPS=ON which is the default) Building the unit tests fails (-DTERRAPHAST_BUILD_TESTS=ON which is the default). The root cause of the problems is a badly designed FindGMP.cmake block that lacked a crucial line, and was also not optimal in helping the user to find the correct library files. Modifications to terraphast/cmake/FindGMP.cmake Defined a new CMake variable, GMP_PREFIX that points to the directory where the include and lib or lib64 subdirectories can be found, and added corresponding PATHS clauses to the find_path and find_library calls. This may not be optimal in Windows, but on our system for some reason CMake failed to find the include files and libraries using the standard search directories that we have defined in our environment ( CFILES and LIBRARY_PATH ). It also ensures that now at the end of the function, GMP_LIBRARIES and GMPXX_LIBRARIES contain the full library name including the path, which was needed for a correct dependency search when building the apps and test executables. Added a crucial set_target_properties line for gmpxx that was missing. Without that line, linking failed. Modifications to terraphast/CMakeLists.txt : Changed the processing of TERRAPHAST_ARCH_NATIVE so that it now also adds an optimization level and produces options that as far as I know are correct on GCC, Clang and Intel (though the Intel option is not so good for AMD). Note that a user doesn't have to set this flag to True as it is perfectly possible to pass suitable optimization options through CMAKE_CXX_FLAGS and CMAKE_CXX_FLAGS_RELEASE . The change is not really needed to compile RAxML-NG as we can make sure the options are indeed passed through other CMAKE variables and that TERRAPHAST_ARCH_NATIOVE is turned off. (Nonessential) Additional information given when GMP is found, and some other changes to messages to make clearer where the message comes from. Fixes to the terraphast code Fixed an issue in lib/bits.hpp with an implicit type conversion from sizt_t/uin64_t to uint8_t: inline uint8_t shift_index(index i) { return i % word_bits; } should be replaced with inline uint8_t shift_index(index i) { return (uint8_t) (i % word_bits); } Even with the fix we already have, there are still issues with verbose_run CMake configuration after our patches Variable Default Meaning RAXML_BINARY_NAME Custom RAxML-NG binary name USE_LIBPLL_CMAKE ON Use CMake to build libpll and pll-modules. Will be forced to ON when building as a library BUILD_AS_LIBRARY OFF Build RAxML-NG as shared library (instead of stand-alone executable). Naming is derived from the name of the matching executable. USE_PTHREADS ON Enable multi-threading support (PTHREADS) USE_MPI OFF Enable MPI support USE_VCF OFF ??? No explanation available ENABLE_PLLMOD_SIMD OFF Enable SIMD instructions in pll-modules (non-portable but slightly faster) STATIC_BUILD OFF Build static binary (don't use with MPI!) USE_TERRAPHAST ON Use phylogentic terraces library (terraphast) USE_GMP OFF Use Gnu Muliple precision (GMP) library, only relevant for terraphast CMAKE_C_FLAGS \"\" CMAKE_C_FLAGS_RELEASE compiler-dependent CMAKE_C_FLAGS_DEBUG -O0 -g CMAKE_CXX_FLAGS \"\" Also used for MPI. CMAKE_CXX_FLAGS_RELEASE compiler-dependent CMAKE_CXX_FLAGS_DEBUG -O0 -g MPI_CXX_COMPILER Used if USED_MPI is ON MPI_CXX_COMPILER_FLAGS Used if USE_MPI is ON ENABLE_SSE True Used to enable or explicitly disable SSE3 optimizations in libpll ENABLE_AVX True Used to enable or explicitly disable AVX optimizations in libpll ENABLE_AVX2 True Used to enable or explicitly disable AVX2 optimizations in libpll SSE_FLAGS \"-msse3\" Added to the compiler options by libpll for SSE3-specific files. AVX_FLAGS \"-mavx\" Added to the compiler options by libpll for AVX-specific files. AVX2_FLAGS \"-mfma -mavx2\" Added to the compiler options by libpll for AVX2-specific files. ENFORCE_COMPILER_FLAGS ON Enforce hard-coded compiler options for CMAKEX*_FLAGS_* in the RAxML-NG configuration files rather than the CMake defaults or user-specified ones.","title":"RAxML-NG installation instructions"},{"location":"generated/easyconfigs/r/RAxML-NG/#raxml-ng-installation-instructions","text":"GitHub The authors mentioned seem to have one thing in common: They are or have been at HITS, the Heidelberg Institute for Theoretical Studies , so that is the likely source of the code.","title":"RAxML-NG installation instructions"},{"location":"generated/easyconfigs/r/RAxML-NG/#general-information","text":"RAxML has a number of dependencies that have no official releases. They are not included in the source code if you download a tar file from the GitHub repository. Therefore it is better to get the sources via a recursive git clone. Dependencies: RAxML-NG uses the following libraries ( libs subdirectory) terraphast which is a clone of the original terraphast Github is on the upsj account developed by Tobias Ribizel from KIT, Karlsruhe. pll-modules which has a different author (Diego Darriba, now at Coru\u00f1a but before at HITS). It looks like this subproject is pretty dead at the moment. pll-modules uses libpll which has yet another author, Tomas Flouri who seems to be linked to both UCL, london and HITS. The libpll code seems to be as dead as pll-modules, both saw their last update in June 2017. Stored in the libs/libpll subdirectory of libs/pll-modules . RAxML-NG supports distributed memory parallelisation with MPI and shared memory parallelisation using pthreads. It is not clear if using both together really gives a hybrid executable as the documentation is mostly missing. In any case, MPI is only used by the RAxML-NG code itself, not by any of the dependencies (terraphast or pll-modules/libpll). libpll has different code for SSE, AVX and AVX2, and a generic code path as an alternative. The SSE, AVX and AVX2 codes use intrinsics so this may give problems with some compilers that don't support certain intrinsics. Even if a system supports AVX2, it is best to leave the lower options activated also as a lower code path is sometimes used if there is no benefit of using a higher one. All three code paths should always be activated if supported, unless -DENABLE_SSE=false , -DENABLE_AVX=false and/or -DENABLE_AVX2=false are specified. Check what CMake does to be sure if the right code paths are compiled into the code. The main RAxML-NG code is C++. Hence the CXX-range of options matter. pll-modules and libpll is C-code. terraphast is C++-code. Moreover, it uses some intrinsics that may be compiler-specific in the code in the lib subdirectory. There are some header files to solve portability problems, with support for GCC, Clang and the Microsoft compiler. It does turn out that all the directives that are used in the GCC/Clang version are also supported by the Intel compiler since the 2018 releases. This is also the component of the code that can use the GNU GMP library.","title":"General information"},{"location":"generated/easyconfigs/r/RAxML-NG/#cmake-configuration-as-it-is-in-the-code","text":"Variable Default Meaning RAXML_BINARY_NAME Custom RAxML-NG binary name USE_LIBPLL_CMAKE ON Use CMake to build libpll and pll-modules. Will be forced to ON when building as a library BUILD_AS_LIBRARY OFF Build RAxML-NG as shared library (instead of stand-alone executable). Naming is derived from the name of the matching executable. USE_PTHREADS ON Enable multi-threading support (PTHREADS) USE_MPI OFF Enable MPI support USE_VCF OFF ??? No explanation available ENABLE_PLLMOD_SIMD OFF Enable SIMD instructions in pll-modules (non-portable but slightly faster) STATIC_BUILD OFF Build static binary (don't use with MPI!) USE_TERRAPHAST ON Use phylogentic terraces library (terraphast) USE_GMP OFF Use Gnu Muliple precision (GMP) library, only relevant for terraphast MPI_CXX_COMPILER Used if USED_MPI is ON MPI_CXX_COMPILER_FLAGS Used if USE_MPI is ON ENABLE_SSE True Used to enable or explicitly disable SSE3 optimizations in libpll ENABLE_AVX True Used to enable or explicitly disable AVX optimizations in libpll ENABLE_AVX2 True Used to enable or explicitly disable AVX2 optimizations in libpll When no custom binary name is given (which is advised), the name of the binary depends on a number of options. No MPI, no static linking: raxml-ng No MPI, static linking: raxml-ng-static MPI, no static linking: raxml-ng-mpi MPI with static linking is not supported?? There are more options, e.g., PLLMOD_DEBUG , that are only relevant to code development and not to creating a user installation. It is really the implementation of ENABLE_RAXML_SIMD and ENABLE_PLLMOD_SIMD that is problematic (at least in version 0.9.0, when this was written) as it currently enforces the selection between very bad and bad compiler options for performance on modern hardware. The CMAKE_C_... and CMAKE_CXX_... are used in the recipe but not taken from the environment or defines on the command line as they have the type INTERNAL . From version 0.9.0 on, make install is supported to install the binaries, but it does not work correctly when installing RAxML-NG as a shared library as it is put in bin instead of in lib or lib64.","title":"CMake configuration as it is in the code"},{"location":"generated/easyconfigs/r/RAxML-NG/#easyconfigs","text":"EasyConfigs were developed at UAntwerp as there was no support for the -NG version in EasyBuild when we started. By now there is an official EasuBuilders recipe for RAxML-NG that however suffers from most of the problems that we indicate below. This documentation was started when installing version 0.9.0. However, we did try to reconstruct what we did in earlier versions and cover this in the documentation also. The main problem with installing RAxML-NG is the poor quality of the build scripts. They impose certain compiler options that 1. are only valid for GCC and 2. are for ancient systems (Ivy Bridge and before) and suboptimal for later systems. As the root CMake file then further calls configure scripts","title":"EasyConfigs"},{"location":"generated/easyconfigs/r/RAxML-NG/#version-041","text":"This was still without terraphast. We needed to adapt the main CMakeList.txt file to recognize the Intel compilers. This was not sufficient though, libpll was still compiled in a suboptimal way (no -xHost ). The recipe does not do a download of the sources from GitHub, this still has to be done manually.","title":"Version 0.4.1"},{"location":"generated/easyconfigs/r/RAxML-NG/#version-081","text":"This version now comes with with terraphast which we didn't get to compile with the Intel compiler. Since we didn't have a full FOSS toolchain with working MPI set up at the time, we did not build a distributed memory version. We made similar changes to the main CMakeLists.txt file. We now had to stick to the GCC compilers though. Which also were used in a suboptimal way for the dependencies. We tried to add the Intel compiler to the recognized compilers, and additional options for the SIMD settings chosing better compiler options.","title":"Version 0.8.1"},{"location":"generated/easyconfigs/r/RAxML-NG/#version-090","text":"Rather than using the system of the EasyBuilders recipe of downloading 4 files, 3 of them based on commits that have to be looked up first, we use EasyBuild's feature to use a recursive download of a tag on the GitHub and assemble the download file that way. This saves us from looking up that commits that are needed for the dependencies every time we install a new version. Added GMP which turned out to be useless in the original code as the support in a subproject was turned off, see below. The CMake configuration now supports \"make install\" so we now use that to install the software. Using the default CMakeLists.txt file: Compiled with GCCcore for Hopper (Ivybridge architecture) as this is the only machine for which the compiler flags are somewhat reasonable. This still implies that the terraphast component is compiled without any optimizations, just the default generic architecture -O0 of GCC due to bugs in the configuration procedure. We made a lot of fixes to the whole CMake configuration process (see the subsections below). We ended up with the following configuration: Ensure that the compiler options are picked up from EasyBuild and that the defaults from CMake are fully surpressed by setting ENFORCE_COMPILER_FLAGS to OFF and initializing all CMAKE_C FLAGS (some are already done automatically by EasyBuild). Turn on SIMD optimizations for pll-modules (in fact, for libpll), but setting the specific optimization options in SSE_FLAGS, AVX_FLAGS and AVX2_FLAGS off as they should already be covered by the options to optimize for a specific architecture chosen by EasyBuild (and if they are not, that level of SIMD is not supported anyway by the architecture). We turn on terraphast and compile it with GMP support. Threading support is also turned on, also in the MPI version. There is something we cannot explain yet: libpll did compile on Hopper (Ivybridge) using only -xHost and no AVX2-specific option. CMake did not detect that there was no AVX2 and the code also compiled without problems. So it looks like what is called AVX2 might not be AVX2? OR what happened to the intrinsics, becasue they do look like AVX2 intrinsics...","title":"Version 0.9.0"},{"location":"generated/easyconfigs/r/RAxML-NG/#remarks-on-the-configuration-and-compilation-process","text":"pll-modules and libpll pick up compiler options that are passed through the environment, but add additional ones that may not be optimal for the chosen compiler or not be valid. terraphast looked like a complete disaster, but this was more due to RAxML-NG making a mess of compiler settings and configuration. The main code is compiled OK now and with a few corrections to the CMakeLists.txt file and a bug fix which was not yet included in the version that ships with RAxML-NG, it now compiles OK with Intel also.","title":"Remarks on the configuration and compilation process"},{"location":"generated/easyconfigs/r/RAxML-NG/#cmake-configuration-after-our-patches","text":"Variable Default Meaning RAXML_BINARY_NAME Custom RAxML-NG binary name USE_LIBPLL_CMAKE ON Use CMake to build libpll and pll-modules. Will be forced to ON when building as a library BUILD_AS_LIBRARY OFF Build RAxML-NG as shared library (instead of stand-alone executable). Naming is derived from the name of the matching executable. USE_PTHREADS ON Enable multi-threading support (PTHREADS) USE_MPI OFF Enable MPI support USE_VCF OFF ??? No explanation available ENABLE_PLLMOD_SIMD OFF Enable SIMD instructions in pll-modules (non-portable but slightly faster) STATIC_BUILD OFF Build static binary (don't use with MPI!) USE_TERRAPHAST ON Use phylogentic terraces library (terraphast) USE_GMP OFF Use Gnu Muliple precision (GMP) library, only relevant for terraphast CMAKE_C_FLAGS \"\" CMAKE_C_FLAGS_RELEASE compiler-dependent CMAKE_C_FLAGS_DEBUG -O0 -g CMAKE_CXX_FLAGS \"\" Also used for MPI. CMAKE_CXX_FLAGS_RELEASE compiler-dependent CMAKE_CXX_FLAGS_DEBUG -O0 -g MPI_CXX_COMPILER Used if USED_MPI is ON MPI_CXX_COMPILER_FLAGS Used if USE_MPI is ON ENABLE_SSE True Used to enable or explicitly disable SSE3 optimizations in libpll ENABLE_AVX True Used to enable or explicitly disable AVX optimizations in libpll ENABLE_AVX2 True Used to enable or explicitly disable AVX2 optimizations in libpll SSE_FLAGS \"-msse3\" Added to the compiler options by libpll for SSE3-specific files. AVX_FLAGS \"-mavx\" Added to the compiler options by libpll for AVX-specific files. AVX2_FLAGS \"-mfma -mavx2\" Added to the compiler options by libpll for AVX2-specific files. ENFORCE_COMPILER_FLAGS ON Enforce hard-coded compiler options for CMAKEX*_FLAGS_* in the RAxML-NG configuration files rather than the CMake defaults or user-specified ones.","title":"CMake configuration after our patches"},{"location":"generated/easyconfigs/r/Racon/","text":"Racon instructions Racon on Github - New site, changed with 1.4.4. General instructions Racon optionally supports CUDA Racon is build through a CMakeMake process The build process only generates the executable bin racon`. EasyBuild There is support for Racon in the EasyBuilders repository . 1.4.13, 2020a toolchains Preparing for a move to the BioTools bundle. Updated to the new GitHub.","title":"Racon instructions"},{"location":"generated/easyconfigs/r/Racon/#racon-instructions","text":"Racon on Github - New site, changed with 1.4.4.","title":"Racon instructions"},{"location":"generated/easyconfigs/r/Racon/#general-instructions","text":"Racon optionally supports CUDA Racon is build through a CMakeMake process The build process only generates the executable bin racon`.","title":"General instructions"},{"location":"generated/easyconfigs/r/Racon/#easybuild","text":"There is support for Racon in the EasyBuilders repository .","title":"EasyBuild"},{"location":"generated/easyconfigs/r/Racon/#1413-2020a-toolchains","text":"Preparing for a move to the BioTools bundle. Updated to the new GitHub.","title":"1.4.13, 2020a toolchains"},{"location":"generated/easyconfigs/r/Roary/","text":"Roary instructions EasyBuild Roary 3.13.0 in the Intel 2020a toolchains Roary seems incompatible with BioPerl 1.7.7 so cannot be installed in this toolchain.","title":"Roary instructions"},{"location":"generated/easyconfigs/r/Roary/#roary-instructions","text":"","title":"Roary instructions"},{"location":"generated/easyconfigs/r/Roary/#easybuild","text":"","title":"EasyBuild"},{"location":"generated/easyconfigs/r/Roary/#roary-3130-in-the-intel-2020a-toolchains","text":"Roary seems incompatible with BioPerl 1.7.7 so cannot be installed in this toolchain.","title":"Roary 3.13.0 in the Intel 2020a toolchains"},{"location":"generated/easyconfigs/r/re2c/","text":"re2c instructions re2c web site , which is also the main documentation web site. re2c on GitHub latest release re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. The resulting programs are faster and often smaller than their table-driven analogues, and they are much easier to debug and understand. re2c applies quite a few optimizations in order to speed up and compress the generated code. Another distinctive feature is its flexible interface: instead of assuming a fixed program template, re2c lets the programmer write most of the interface code and adapt the generated lexer to any particular environment. General information re2c has a very standard configure - make - make install build process and no particular dependencies. EasyBuild There is support for re2c in the EasyBuilders repository . At UAntwerp, we started from those recipes but switched to the SYSTEM toolchain and included the package in our buildtools module. One user of re2c is Qt5.","title":"re2c instructions"},{"location":"generated/easyconfigs/r/re2c/#re2c-instructions","text":"re2c web site , which is also the main documentation web site. re2c on GitHub latest release re2c is a free and open-source lexer generator for C and C++. Its main goal is generating fast lexers: at least as fast as their reasonably optimized hand-coded counterparts. Instead of using traditional table-driven approach, re2c encodes the generated finite state automata directly in the form of conditional jumps and comparisons. The resulting programs are faster and often smaller than their table-driven analogues, and they are much easier to debug and understand. re2c applies quite a few optimizations in order to speed up and compress the generated code. Another distinctive feature is its flexible interface: instead of assuming a fixed program template, re2c lets the programmer write most of the interface code and adapt the generated lexer to any particular environment.","title":"re2c instructions"},{"location":"generated/easyconfigs/r/re2c/#general-information","text":"re2c has a very standard configure - make - make install build process and no particular dependencies.","title":"General information"},{"location":"generated/easyconfigs/r/re2c/#easybuild","text":"There is support for re2c in the EasyBuilders repository . At UAntwerp, we started from those recipes but switched to the SYSTEM toolchain and included the package in our buildtools module. One user of re2c is Qt5.","title":"EasyBuild"},{"location":"generated/easyconfigs/s/SAMtools/","text":"SAMtools installation instructions SAMtools web site SAMtools on GitHub General information Note that some time ago SAMtools was split in three different packages HTSlib SAMtools itself BCFtools Which inspired us to bundle them again... SAMtools contains a number of binaries, a static libraries, several Perl scripts but also two LUA scripts. EasyConfigs This documentation starts with version 1.10 in the 2020a toolchains. There is support for SAMtools in the EasyBuilders repository . The support uses an EasyBlock that installs a library and include file that are not installed by make install . Version 1.10 for the Intel 2020 toolchains Moved to the BioTools bundle, but there is an EasyBuild recipe here for the Bundle development. HTSlib is linked from the appropriate module. That module is compiled with support for various compression libraries through the baselibs module. There is an error message in the config logs about not finding ncursesw.h, but this probably doesn't matter since ncursesw/curses.h is found? It also complains about the ncurses.h header file. It is not clear how serious this error message is. The actual error is an undefined type, wint_t , which actually should come from wchar.h . REMARK: The LUA-dependency is still missing, so the two commands that are in fact LUA scripts will not work.","title":"SAMtools installation instructions"},{"location":"generated/easyconfigs/s/SAMtools/#samtools-installation-instructions","text":"SAMtools web site SAMtools on GitHub","title":"SAMtools installation instructions"},{"location":"generated/easyconfigs/s/SAMtools/#general-information","text":"Note that some time ago SAMtools was split in three different packages HTSlib SAMtools itself BCFtools Which inspired us to bundle them again... SAMtools contains a number of binaries, a static libraries, several Perl scripts but also two LUA scripts.","title":"General information"},{"location":"generated/easyconfigs/s/SAMtools/#easyconfigs","text":"This documentation starts with version 1.10 in the 2020a toolchains. There is support for SAMtools in the EasyBuilders repository . The support uses an EasyBlock that installs a library and include file that are not installed by make install .","title":"EasyConfigs"},{"location":"generated/easyconfigs/s/SAMtools/#version-110-for-the-intel-2020-toolchains","text":"Moved to the BioTools bundle, but there is an EasyBuild recipe here for the Bundle development. HTSlib is linked from the appropriate module. That module is compiled with support for various compression libraries through the baselibs module. There is an error message in the config logs about not finding ncursesw.h, but this probably doesn't matter since ncursesw/curses.h is found? It also complains about the ncurses.h header file. It is not clear how serious this error message is. The actual error is an undefined type, wint_t , which actually should come from wchar.h . REMARK: The LUA-dependency is still missing, so the two commands that are in fact LUA scripts will not work.","title":"Version 1.10 for the Intel 2020 toolchains"},{"location":"generated/easyconfigs/s/SCOTCH/","text":"SCOTCH installation remarks SCOTCH is installed via an EasyBlock. Most options are set through the toolchainopts. The EasyBlock has a very nasty side effect. The author thought it was a bad idea to install the metis.h and parmetis.h files that are needed to use the Metis/parMetis compatibility interfaces of SCOTCH. But these files are really needed if one want to compile a package with SCOTCH through those interfaces. E.g., to have SCOTCH support in SuperLU which is advised for big matrices. So we put these files back through postinstallcmds.","title":"SCOTCH installation remarks"},{"location":"generated/easyconfigs/s/SCOTCH/#scotch-installation-remarks","text":"SCOTCH is installed via an EasyBlock. Most options are set through the toolchainopts. The EasyBlock has a very nasty side effect. The author thought it was a bad idea to install the metis.h and parmetis.h files that are needed to use the Metis/parMetis compatibility interfaces of SCOTCH. But these files are really needed if one want to compile a package with SCOTCH through those interfaces. E.g., to have SCOTCH support in SuperLU which is advised for big matrices. So we put these files back through postinstallcmds.","title":"SCOTCH installation remarks"},{"location":"generated/easyconfigs/s/SCons/","text":"SCons instructions SCons web site SCons on GitHub Releases SCons on SourceForge Downloads on SourceForge General information SCons is Python software Version 3.1 (when this documentation was first written) Requires 2.7 or 3.5+ EasyBuild This documentation was written when installing version 3.1.2 in the 2020a build round. There is support for SCons in the EasyBuilders repository . It however relies on EasyBuild-generated Python modules which is kind of stupid for a tool that is used to install some very basic software. 3.1.2 Developed with the goal of integrating into out buildtools module. Started from an EasyBuilders recipe. Switched to the SYSTEM toolchain and using a system-installed Python. Since we already had a package in the buildtools bundle that requires Python 3 (Meson) we decided to go for Python 3. This implies: Forcing EasyBuild to use the right Python executable by setting req_py_majver and req_py_minver. Since the wrapper script use /usr/bin/env python , we edited them with sed to use python3 instead to ensure that the right version of Python corresponding to the installed libraries is used. Otherwise we could expect conflicts with the PYTHONPATH in buildtools. Since our system Python does not have pip installed, we turned off pip installation.","title":"SCons instructions"},{"location":"generated/easyconfigs/s/SCons/#scons-instructions","text":"SCons web site SCons on GitHub Releases SCons on SourceForge Downloads on SourceForge","title":"SCons instructions"},{"location":"generated/easyconfigs/s/SCons/#general-information","text":"SCons is Python software Version 3.1 (when this documentation was first written) Requires 2.7 or 3.5+","title":"General information"},{"location":"generated/easyconfigs/s/SCons/#easybuild","text":"This documentation was written when installing version 3.1.2 in the 2020a build round. There is support for SCons in the EasyBuilders repository . It however relies on EasyBuild-generated Python modules which is kind of stupid for a tool that is used to install some very basic software.","title":"EasyBuild"},{"location":"generated/easyconfigs/s/SCons/#312","text":"Developed with the goal of integrating into out buildtools module. Started from an EasyBuilders recipe. Switched to the SYSTEM toolchain and using a system-installed Python. Since we already had a package in the buildtools bundle that requires Python 3 (Meson) we decided to go for Python 3. This implies: Forcing EasyBuild to use the right Python executable by setting req_py_majver and req_py_minver. Since the wrapper script use /usr/bin/env python , we edited them with sed to use python3 instead to ensure that the right version of Python corresponding to the installed libraries is used. Otherwise we could expect conflicts with the PYTHONPATH in buildtools. Since our system Python does not have pip installed, we turned off pip installation.","title":"3.1.2"},{"location":"generated/easyconfigs/s/SMALT/","text":"SMALT installation instructions SMALT web site @ Wellcome Sanger Institute SMALT on SourceForge Note that SMALT is unmaintained since late 2014. Dependencies SMALT uses libz to read gzipped FASTA/FASTQ files as input. SMALT looks for SAMtools (or at least its library libbam.a). However, it turns out that it needs an old, pre-1.0 version as the function it is looking for in the configure script, sam_open, has been replaced later on by samopen. However, the variable HAVE_SAMTOOLS_API which is defined by the configure, is used nowhere in the code, so this appears to be a fake dependency, probably a leftover from old code, which is now replaced by the next dependency. Optional dependency: bambamc library . This library is unmaintained since May 2014. SMALT also includes a number of Python scripts in the subdirectory test . These scripts are only examples and not executable (the lack the shebang). EasyConfig The SMALT EasyConfig is based on one found in the EasyBuild 3.8.1 distribution. Given that SMALT hasn't seen any decent update for years and nobody contributed newer EeasyBuild recipes, support has disappeared in recent versions of EasyBuild. SMALT 0.7.6 with Intel 2019b. SMALT is installed as a bundle. We first install the bambamc library as a static library and then install SMALT. Since bambamc is effectively just a build dependency for SMALT (as we create a static library) we remove it again after building SMALT to make sure we minimize interference between such old tools and newer tools in the toolchain. The regular build process of SMALT puts a number of test scripts (in Python) in the share subdirectory. We rename that directory to test and also copy the datafiles which were forgotten in the installation Makefiles.","title":"SMALT installation instructions"},{"location":"generated/easyconfigs/s/SMALT/#smalt-installation-instructions","text":"SMALT web site @ Wellcome Sanger Institute SMALT on SourceForge Note that SMALT is unmaintained since late 2014.","title":"SMALT installation instructions"},{"location":"generated/easyconfigs/s/SMALT/#dependencies","text":"SMALT uses libz to read gzipped FASTA/FASTQ files as input. SMALT looks for SAMtools (or at least its library libbam.a). However, it turns out that it needs an old, pre-1.0 version as the function it is looking for in the configure script, sam_open, has been replaced later on by samopen. However, the variable HAVE_SAMTOOLS_API which is defined by the configure, is used nowhere in the code, so this appears to be a fake dependency, probably a leftover from old code, which is now replaced by the next dependency. Optional dependency: bambamc library . This library is unmaintained since May 2014. SMALT also includes a number of Python scripts in the subdirectory test . These scripts are only examples and not executable (the lack the shebang).","title":"Dependencies"},{"location":"generated/easyconfigs/s/SMALT/#easyconfig","text":"The SMALT EasyConfig is based on one found in the EasyBuild 3.8.1 distribution. Given that SMALT hasn't seen any decent update for years and nobody contributed newer EeasyBuild recipes, support has disappeared in recent versions of EasyBuild.","title":"EasyConfig"},{"location":"generated/easyconfigs/s/SMALT/#smalt-076-with-intel-2019b","text":"SMALT is installed as a bundle. We first install the bambamc library as a static library and then install SMALT. Since bambamc is effectively just a build dependency for SMALT (as we create a static library) we remove it again after building SMALT to make sure we minimize interference between such old tools and newer tools in the toolchain. The regular build process of SMALT puts a number of test scripts (in Python) in the share subdirectory. We rename that directory to test and also copy the datafiles which were forgotten in the installation Makefiles.","title":"SMALT 0.7.6 with Intel 2019b."},{"location":"generated/easyconfigs/s/SPAdes/","text":"SPAdes installation instructions SPAdes website SPAdes GitHub General remarks GNU C++ is the only compiler supported by the development team (demonstrated also by hard-coded GNU compiler options). Several versions don't compile with the Intel compilers, producing tons of warnings and a fatal error. The problem of using #import (an Objective C construct) rather then #include was solved in 3.13, but other problems remained. The compiler instructions on the web site are out-of-date, recent versions of SPAdes do support CMake. EasyConfigs 3.13.1 (2018b toolchain) SPAdes does not compile correctly with Intel. Hence we used g++ but did chose to keep SPAdes in the Intel toolchain as it does rely on a number of components that were compiled with the Intel compilers. As there is CMake support in the code, we did use that even though the web site told to use a shell script. 3.14.0 (2019b toolchain) The adaptations were fairly trivial. We just bumped the versions and switched to baselibs for zlib and bzip2. Tested again with Intel, but the same problems as with 3.13.1 are still present.","title":"SPAdes installation instructions"},{"location":"generated/easyconfigs/s/SPAdes/#spades-installation-instructions","text":"SPAdes website SPAdes GitHub","title":"SPAdes installation instructions"},{"location":"generated/easyconfigs/s/SPAdes/#general-remarks","text":"GNU C++ is the only compiler supported by the development team (demonstrated also by hard-coded GNU compiler options). Several versions don't compile with the Intel compilers, producing tons of warnings and a fatal error. The problem of using #import (an Objective C construct) rather then #include was solved in 3.13, but other problems remained. The compiler instructions on the web site are out-of-date, recent versions of SPAdes do support CMake.","title":"General remarks"},{"location":"generated/easyconfigs/s/SPAdes/#easyconfigs","text":"","title":"EasyConfigs"},{"location":"generated/easyconfigs/s/SPAdes/#3131-2018b-toolchain","text":"SPAdes does not compile correctly with Intel. Hence we used g++ but did chose to keep SPAdes in the Intel toolchain as it does rely on a number of components that were compiled with the Intel compilers. As there is CMake support in the code, we did use that even though the web site told to use a shell script.","title":"3.13.1 (2018b toolchain)"},{"location":"generated/easyconfigs/s/SPAdes/#3140-2019b-toolchain","text":"The adaptations were fairly trivial. We just bumped the versions and switched to baselibs for zlib and bzip2. Tested again with Intel, but the same problems as with 3.13.1 are still present.","title":"3.14.0 (2019b toolchain)"},{"location":"generated/easyconfigs/s/ScaFaCos/","text":"ScaFaCos installation instructions ScaFaCoS is a library of scalable fast coulomb solvers used by LAMMPS. ScaFaCos web site ScaFaCos on GitHub GitHub releases EasyBuild There is sopport for ScaFaCos in the EasyBuilders repository . Version 1.0.1 in intel 2020a Installed as a dependency of LAMMPS Regular EasyConfig edited to use our buildtools and baselibs modules, brought in line with our style and with updated help text for the module file.","title":"ScaFaCos installation instructions"},{"location":"generated/easyconfigs/s/ScaFaCos/#scafacos-installation-instructions","text":"ScaFaCoS is a library of scalable fast coulomb solvers used by LAMMPS. ScaFaCos web site ScaFaCos on GitHub GitHub releases","title":"ScaFaCos installation instructions"},{"location":"generated/easyconfigs/s/ScaFaCos/#easybuild","text":"There is sopport for ScaFaCos in the EasyBuilders repository .","title":"EasyBuild"},{"location":"generated/easyconfigs/s/ScaFaCos/#version-101-in-intel-2020a","text":"Installed as a dependency of LAMMPS Regular EasyConfig edited to use our buildtools and baselibs modules, brought in line with our style and with updated help text for the module file.","title":"Version 1.0.1 in intel 2020a"},{"location":"generated/easyconfigs/s/SuperLU_DIST/","text":"SuperLU_DIST installation instructions SuperLU_DIST is the distributed memory version of the SuperLU solver. SuperLU home page SuperLU_DIST GitHub EasyConfig The development of the EasyConfigs is based upon those of the Electronic Structure Library github . Instead of using the patches they use there to use the Metis/parMetis compatibility interface of SuperLU, we simply reinstalled the necessary header files in the SCOTCH include dirs (removed there by the SCOTCH EasyBlock) and can then compile SuperLU without additional patches, at least if no Metis or parMetis module is loaded which may conflict with those header files. For the 2020a toolchains, we switched to downloads from the GitHub.","title":"SuperLU_DIST installation instructions"},{"location":"generated/easyconfigs/s/SuperLU_DIST/#superlu_dist-installation-instructions","text":"SuperLU_DIST is the distributed memory version of the SuperLU solver. SuperLU home page SuperLU_DIST GitHub","title":"SuperLU_DIST installation instructions"},{"location":"generated/easyconfigs/s/SuperLU_DIST/#easyconfig","text":"The development of the EasyConfigs is based upon those of the Electronic Structure Library github . Instead of using the patches they use there to use the Metis/parMetis compatibility interface of SuperLU, we simply reinstalled the necessary header files in the SCOTCH include dirs (removed there by the SCOTCH EasyBlock) and can then compile SuperLU without additional patches, at least if no Metis or parMetis module is loaded which may conflict with those header files. For the 2020a toolchains, we switched to downloads from the GitHub.","title":"EasyConfig"},{"location":"generated/easyconfigs/s/snappy/","text":"snappy instructions snappy on GitHub Releases on GitHub General information Snappy is build using CMake. Snappy can use zlib and lzo2. There are CMake flags to enforce AVX or AVX2 but it is not clear if these are needed as I cannot find where they define symbols that would then be used in the code. It does influence compiler options that are added though. Note that building snappy requires two iterations if we want both static and shared libraries. EasyBuild This README was developed starting with snappy 1.1.8 in the 2020a toolchains. There is EasyBuilders support for snappy but the dependencies are incomplete. 1.1.8 in 2020a This EasyConfig was made to prepare for inclusion in the baselibs Bundle. The dependencies on zlib and LZO (lzo2) were added to the dependencies list to ensure a build with maximum potential.","title":"snappy instructions"},{"location":"generated/easyconfigs/s/snappy/#snappy-instructions","text":"snappy on GitHub Releases on GitHub","title":"snappy instructions"},{"location":"generated/easyconfigs/s/snappy/#general-information","text":"Snappy is build using CMake. Snappy can use zlib and lzo2. There are CMake flags to enforce AVX or AVX2 but it is not clear if these are needed as I cannot find where they define symbols that would then be used in the code. It does influence compiler options that are added though. Note that building snappy requires two iterations if we want both static and shared libraries.","title":"General information"},{"location":"generated/easyconfigs/s/snappy/#easybuild","text":"This README was developed starting with snappy 1.1.8 in the 2020a toolchains. There is EasyBuilders support for snappy but the dependencies are incomplete.","title":"EasyBuild"},{"location":"generated/easyconfigs/s/snappy/#118-in-2020a","text":"This EasyConfig was made to prepare for inclusion in the baselibs Bundle. The dependencies on zlib and LZO (lzo2) were added to the dependencies list to ensure a build with maximum potential.","title":"1.1.8 in 2020a"},{"location":"generated/easyconfigs/t/TensorFlow/","text":"TensorFlow installation notes Problems encountered 2019b toolchains, Intel-compiled Python 3.7.4, Tensorflow 1.15.0 - 2.1.0 Dependency astor: Version 0.8.1 does not work (fails during the sanity check); version 0.8.0 passes the tests. Dependency grcpio: Version 1.25.0 and 1.26.0 do not compile with the Intel compiler (the compiler complains about a statement mixing vector and scalar computations). 1.24.3 is the last version that installs without problems. The fatal error actually happens in included third-party code, the boringssl crypto library. Boringssl is yet another piece of Google junk (an OpenSSL fork) that only compiles with compilers that Google likes and not with others. Checking the gRPC GitHub suggests that they do test the code with Clang, so there is hope for the future. Keras-metrics, added in earlier TensorFlow installations with Keras, doesn't work with the Keras 2.3.x - TenserFlow 1.15 or later combinations. Hence it is removed from our installation. Note that Keras 2.3.x is the last version of the multi-backend Keras implementation. Keras is now implemented directly in TensorFlow. However, using it that way does require changes to the code so we decided to keep Keras 2.3 in the TensorFlow modules for 1.15 and 2.1. Since compiling TensorFlow is difficult, we went for binaries that were readily available. We now get them from PyPi, which contains 4 different versions of TensorFlow * tensorflow : For versions 1.14 and earlier and 2.0, this is a version compiled for CPU. For 1.15.0 and 2.1.0 (and likely later versions) the binaries support both GPU and CPU. It is not clear to what extent the GPU binary is optimized for the P100. The CPU version contained in the module does not yet seem to use AVX2/FMA as available on our Broadwell and AND Rome nodes. Note also that it was tricky to find out which versions of CUDA and cuDNN and of TensorRT (TensorFlow 2.1.0 only) were used to generate the binaries, and the major version numbers need to be correct for TensorFlow to find the libraries. * tensorflow-gpu : TensorFlow compiled for use with NVIDIA GPUs. It is not clear to what extent the binaries are optimized for the P100. As for the regular tensorflow package, some reverse engineering is required to find the correct NVIDIA libraries. * tensorflow-cpu : CPU-only binaries for those versions of TensorFlow for which the tensorflow package contains a BPU/CPU combined binary. The same remarks with respect to optmization hold as for the regular tensorflow package. * intel-tensorflow : This package is compiled to use the Intel MKLDNN libraries for better performance on mondern Intel CPUs (and likely give some advantages on AMD Rome also). For the 2019b toolchains, we created the universal and -gpu modules for TensorFlow 1.15.0 and 2.1.0, and Intel-optimized once for TensorFlow 1.15.0 and 2.0.0 as the 2.1.0 binaries of the latter were not yet available when we first did the installation. 2020a toolchains, Intel-compiled Python 3.8.3, TensorFlow 2.2.0 We base our installs on the same 4 versions from PyPi mentioned above for the 2019b toolchains. We no longer include Keras. Keras is included in TensorFlow for a while already and the separate modules are no longer maintained. It does require changes to user code though. The official dependencies for TensorFlow 2.2.0 (from the instructions to install from source) are pip, six, wheel, setuptools, NumPy (<1.19.0), mock, future (>=0.17.1), keras_applications and keras_preprocessing. However, when running a pip install on TensorFlow, a whole different list of dependencies shows up of packages that are not yet in our base Python module: oauthlib (3.1.0) cachetools (4.1.1) rsa (4.6) pyasn1-modules (0.2.8) WerkZeug (at least 0.11.15) Markdown (at least 2.6.8) grpcio (1.30.0) opt-einsum (3.3.0) astunparse (1.6.3) google-auth-oauthlib (0.4.1) google-auth (1.19.2) google-pasta (0.2.0) Keras_Preprocessing (As expected), version 1.1.2 tensorflow_estimator (2.2.0). TensorFlow installs it as a requirement, but it also loads the tensorflow package... tensorflow-plutin-wit 1.7.0 tensorboard, version 2.2.2 was chosen above the more recent 2.3.0. TensorFlow is very picky on the versions of tensorflow_estimator and tensorboard. In general, they should come from the same major.minor-series as TensorFlow itself. So the right way to build the extension list is to try a manual install of TensorFlow using pip, see which dependencies it pics and then put them in the right order (the inverse of what is downloaded) in the EasyConfig.","title":"TensorFlow installation notes"},{"location":"generated/easyconfigs/t/TensorFlow/#tensorflow-installation-notes","text":"","title":"TensorFlow installation notes"},{"location":"generated/easyconfigs/t/TensorFlow/#problems-encountered","text":"","title":"Problems encountered"},{"location":"generated/easyconfigs/t/TensorFlow/#2019b-toolchains-intel-compiled-python-374-tensorflow-1150-210","text":"Dependency astor: Version 0.8.1 does not work (fails during the sanity check); version 0.8.0 passes the tests. Dependency grcpio: Version 1.25.0 and 1.26.0 do not compile with the Intel compiler (the compiler complains about a statement mixing vector and scalar computations). 1.24.3 is the last version that installs without problems. The fatal error actually happens in included third-party code, the boringssl crypto library. Boringssl is yet another piece of Google junk (an OpenSSL fork) that only compiles with compilers that Google likes and not with others. Checking the gRPC GitHub suggests that they do test the code with Clang, so there is hope for the future. Keras-metrics, added in earlier TensorFlow installations with Keras, doesn't work with the Keras 2.3.x - TenserFlow 1.15 or later combinations. Hence it is removed from our installation. Note that Keras 2.3.x is the last version of the multi-backend Keras implementation. Keras is now implemented directly in TensorFlow. However, using it that way does require changes to the code so we decided to keep Keras 2.3 in the TensorFlow modules for 1.15 and 2.1. Since compiling TensorFlow is difficult, we went for binaries that were readily available. We now get them from PyPi, which contains 4 different versions of TensorFlow * tensorflow : For versions 1.14 and earlier and 2.0, this is a version compiled for CPU. For 1.15.0 and 2.1.0 (and likely later versions) the binaries support both GPU and CPU. It is not clear to what extent the GPU binary is optimized for the P100. The CPU version contained in the module does not yet seem to use AVX2/FMA as available on our Broadwell and AND Rome nodes. Note also that it was tricky to find out which versions of CUDA and cuDNN and of TensorRT (TensorFlow 2.1.0 only) were used to generate the binaries, and the major version numbers need to be correct for TensorFlow to find the libraries. * tensorflow-gpu : TensorFlow compiled for use with NVIDIA GPUs. It is not clear to what extent the binaries are optimized for the P100. As for the regular tensorflow package, some reverse engineering is required to find the correct NVIDIA libraries. * tensorflow-cpu : CPU-only binaries for those versions of TensorFlow for which the tensorflow package contains a BPU/CPU combined binary. The same remarks with respect to optmization hold as for the regular tensorflow package. * intel-tensorflow : This package is compiled to use the Intel MKLDNN libraries for better performance on mondern Intel CPUs (and likely give some advantages on AMD Rome also). For the 2019b toolchains, we created the universal and -gpu modules for TensorFlow 1.15.0 and 2.1.0, and Intel-optimized once for TensorFlow 1.15.0 and 2.0.0 as the 2.1.0 binaries of the latter were not yet available when we first did the installation.","title":"2019b toolchains, Intel-compiled Python 3.7.4, Tensorflow 1.15.0 - 2.1.0"},{"location":"generated/easyconfigs/t/TensorFlow/#2020a-toolchains-intel-compiled-python-383-tensorflow-220","text":"We base our installs on the same 4 versions from PyPi mentioned above for the 2019b toolchains. We no longer include Keras. Keras is included in TensorFlow for a while already and the separate modules are no longer maintained. It does require changes to user code though. The official dependencies for TensorFlow 2.2.0 (from the instructions to install from source) are pip, six, wheel, setuptools, NumPy (<1.19.0), mock, future (>=0.17.1), keras_applications and keras_preprocessing. However, when running a pip install on TensorFlow, a whole different list of dependencies shows up of packages that are not yet in our base Python module: oauthlib (3.1.0) cachetools (4.1.1) rsa (4.6) pyasn1-modules (0.2.8) WerkZeug (at least 0.11.15) Markdown (at least 2.6.8) grpcio (1.30.0) opt-einsum (3.3.0) astunparse (1.6.3) google-auth-oauthlib (0.4.1) google-auth (1.19.2) google-pasta (0.2.0) Keras_Preprocessing (As expected), version 1.1.2 tensorflow_estimator (2.2.0). TensorFlow installs it as a requirement, but it also loads the tensorflow package... tensorflow-plutin-wit 1.7.0 tensorboard, version 2.2.2 was chosen above the more recent 2.3.0. TensorFlow is very picky on the versions of tensorflow_estimator and tensorboard. In general, they should come from the same major.minor-series as TensorFlow itself. So the right way to build the extension list is to try a manual install of TensorFlow using pip, see which dependencies it pics and then put them in the right order (the inverse of what is downloaded) in the EasyConfig.","title":"2020a toolchains, Intel-compiled Python 3.8.3, TensorFlow 2.2.0"},{"location":"generated/easyconfigs/t/Towhee/","text":"Towhee instructions Towhee web site is maintained on SourceForge . Code Manual , useful when installing the code. Download from SourceForge EasyConfigs There was no official EasyBuild support when we first installed Towhee on our systems. There was however a EasyConfig file by ComputeCanada . Towhee 8.2.0 The code violates the Fortran standard for fixed form source files. In Source/ffdreiding.F there is a line that is 74 characters long. The line shrinks when the preprocessor is applied, but a Fortran-zware preprocessor that neglects characters on position 73 and further as it should, will fail. This is the case with the Intel Fortran compiler. Most compilers have options to allow for longer source lines, but that might also fail as the code is linked with the C compiler but the options of the Fortran compiler are used. So when adding the flag required for longer source lines, the C compiler may not recognize that option and fail. -extend-source worked in our case, but -extend-source 80 or -extend-source 132 did not because of problems while linking.","title":"Towhee instructions"},{"location":"generated/easyconfigs/t/Towhee/#towhee-instructions","text":"Towhee web site is maintained on SourceForge . Code Manual , useful when installing the code. Download from SourceForge","title":"Towhee instructions"},{"location":"generated/easyconfigs/t/Towhee/#easyconfigs","text":"There was no official EasyBuild support when we first installed Towhee on our systems. There was however a EasyConfig file by ComputeCanada .","title":"EasyConfigs"},{"location":"generated/easyconfigs/t/Towhee/#towhee-820","text":"The code violates the Fortran standard for fixed form source files. In Source/ffdreiding.F there is a line that is 74 characters long. The line shrinks when the preprocessor is applied, but a Fortran-zware preprocessor that neglects characters on position 73 and further as it should, will fail. This is the case with the Intel Fortran compiler. Most compilers have options to allow for longer source lines, but that might also fail as the code is linked with the C compiler but the options of the Fortran compiler are used. So when adding the flag required for longer source lines, the C compiler may not recognize that option and fail. -extend-source worked in our case, but -extend-source 80 or -extend-source 132 did not because of problems while linking.","title":"Towhee 8.2.0"},{"location":"generated/easyconfigs/t/Trimmomatic/","text":"Trimmomatic installation instructions Trimmomatic webpage @ USADELLAB.org General information Trimmomatic needs JAVA. EasyConfig There is support for Trimmomatic in EasyBuild. We use the standard EasyBuild recipes with only cosmetic changes to bring them in line with our documentation standards. We also define the bash alias 'trimmomatic' to start Trimmomatic. It essentially produce the java -jar - command that would otherwise be needed to start Trimmomatic.","title":"Trimmomatic installation instructions"},{"location":"generated/easyconfigs/t/Trimmomatic/#trimmomatic-installation-instructions","text":"Trimmomatic webpage @ USADELLAB.org","title":"Trimmomatic installation instructions"},{"location":"generated/easyconfigs/t/Trimmomatic/#general-information","text":"Trimmomatic needs JAVA.","title":"General information"},{"location":"generated/easyconfigs/t/Trimmomatic/#easyconfig","text":"There is support for Trimmomatic in EasyBuild. We use the standard EasyBuild recipes with only cosmetic changes to bring them in line with our documentation standards. We also define the bash alias 'trimmomatic' to start Trimmomatic. It essentially produce the java -jar - command that would otherwise be needed to start Trimmomatic.","title":"EasyConfig"},{"location":"generated/easyconfigs/u/Unicycler/","text":"Unicycler instructions Unicycler on GitHub REleases Unicycler is no longer on PyPi. General information Unicycler is a Python package interfacing to a lot of other tools. Dependencies There is no clear list of dependencies in the documentation. This list is assembled from whatever is in the README.md file and from the EasyBuilders recipes. Python Pilon : Java package, and very slow in development (1.23 release in November 2018) - So Java also enters as a dependency BLAST+ Bowtie2 miniasm - part of our BioTools package since 2020a Racon - part of our BioTools package since 2020a SAMtools - part of our BioTools package since 2020a SPAdes - separate since it poses compile challenges EasyBuild This documentation was first written when installing version 0.4.8 in the 2020a toolchains. There is support for UniCycler in the EasyBuilders repository . Version 0.4.8, 2020 toolchains Developed from the 0.4.7 one, but updated to use BioTools were possible. Added miniasm as a dependency, which was missing in the EasyBuilders EasyConfig.","title":"Unicycler instructions"},{"location":"generated/easyconfigs/u/Unicycler/#unicycler-instructions","text":"Unicycler on GitHub REleases Unicycler is no longer on PyPi.","title":"Unicycler instructions"},{"location":"generated/easyconfigs/u/Unicycler/#general-information","text":"Unicycler is a Python package interfacing to a lot of other tools.","title":"General information"},{"location":"generated/easyconfigs/u/Unicycler/#dependencies","text":"There is no clear list of dependencies in the documentation. This list is assembled from whatever is in the README.md file and from the EasyBuilders recipes. Python Pilon : Java package, and very slow in development (1.23 release in November 2018) - So Java also enters as a dependency BLAST+ Bowtie2 miniasm - part of our BioTools package since 2020a Racon - part of our BioTools package since 2020a SAMtools - part of our BioTools package since 2020a SPAdes - separate since it poses compile challenges","title":"Dependencies"},{"location":"generated/easyconfigs/u/Unicycler/#easybuild","text":"This documentation was first written when installing version 0.4.8 in the 2020a toolchains. There is support for UniCycler in the EasyBuilders repository .","title":"EasyBuild"},{"location":"generated/easyconfigs/u/Unicycler/#version-048-2020-toolchains","text":"Developed from the 0.4.7 one, but updated to use BioTools were possible. Added miniasm as a dependency, which was missing in the EasyBuilders EasyConfig.","title":"Version 0.4.8, 2020 toolchains"},{"location":"generated/easyconfigs/v/VMD/README.VMD/","text":"Installing VMD @ UAntwerp General info VMD web site VMD documentation : the instructions to install from source are not in the VMD Installation Guide but in the VMD Programmer's Guide (at least for version 1.9.3 which was the most recent released version when this document was written) And some of the requirements are only explained properly in the VMD User's Guide . VMD has an EasyBlock as the build process has some additional steps due to the required installation of plug-ins and some other dependencies that come with the code. It also tries to auto-generate a number of options for the configure command. Those options are also given in a special way. Problems met installing 1.9.3 with EasyBuild 3.9.4 (2019b) There is an error in the EasyBlock in the Python statement that calls numpy.get_includes() : it is not compatible with Python 3. Correction: Add parentheses to the print statement: python -c 'import numpy; print( numpy.get_include() )' The EasyBlock does not set PYTHON_INCLUDE_DIR to the correct directory. E.g., for Python 3.7 it should be $EBROOTPYHTON/include/python3.7m' rather than $EBROOTPYTHON/include/python3.7 (lacking the m` at the end). I modified the EasyBlock to distinguish between Python 2 and 3. Moreover, Python is an optional component of VMD while the EasyBlock makes this a mandatory one. I've made modifications to make this an optional component. VMD does not yet compile with Python 3.7. There are problems with some header files. It is not clear if earlier 3.x-versions are supported. The documentation is very unclear on the necessary software. netCDF should also be an optional component and I've modified the EasyBlock in that way. However, currently the build procedure fails when no netCDF module is specified because the list of plugins is not properly adapted and the build procedure tries to build plugins that do require netCDF. I've also added a custom parameter ' vmd_arch ' to set the VMD architecture. The default and only one tested is ' LINUXAMD64 '. I've also added a second custom parameter, ' vmd_extra_ops ' that is used to specify additional options for the build process as it is impossible to specify them through ' configopts '. The problem with configopts is that the options are added at the wrong position which causes the build process to fail (they are added before the architecture options which should always be the first one).","title":"Installing VMD @ UAntwerp"},{"location":"generated/easyconfigs/v/VMD/README.VMD/#installing-vmd-uantwerp","text":"","title":"Installing VMD @ UAntwerp"},{"location":"generated/easyconfigs/v/VMD/README.VMD/#general-info","text":"VMD web site VMD documentation : the instructions to install from source are not in the VMD Installation Guide but in the VMD Programmer's Guide (at least for version 1.9.3 which was the most recent released version when this document was written) And some of the requirements are only explained properly in the VMD User's Guide . VMD has an EasyBlock as the build process has some additional steps due to the required installation of plug-ins and some other dependencies that come with the code. It also tries to auto-generate a number of options for the configure command. Those options are also given in a special way.","title":"General info"},{"location":"generated/easyconfigs/v/VMD/README.VMD/#problems-met-installing-193-with-easybuild-394-2019b","text":"There is an error in the EasyBlock in the Python statement that calls numpy.get_includes() : it is not compatible with Python 3. Correction: Add parentheses to the print statement: python -c 'import numpy; print( numpy.get_include() )' The EasyBlock does not set PYTHON_INCLUDE_DIR to the correct directory. E.g., for Python 3.7 it should be $EBROOTPYHTON/include/python3.7m' rather than $EBROOTPYTHON/include/python3.7 (lacking the m` at the end). I modified the EasyBlock to distinguish between Python 2 and 3. Moreover, Python is an optional component of VMD while the EasyBlock makes this a mandatory one. I've made modifications to make this an optional component. VMD does not yet compile with Python 3.7. There are problems with some header files. It is not clear if earlier 3.x-versions are supported. The documentation is very unclear on the necessary software. netCDF should also be an optional component and I've modified the EasyBlock in that way. However, currently the build procedure fails when no netCDF module is specified because the list of plugins is not properly adapted and the build procedure tries to build plugins that do require netCDF. I've also added a custom parameter ' vmd_arch ' to set the VMD architecture. The default and only one tested is ' LINUXAMD64 '. I've also added a second custom parameter, ' vmd_extra_ops ' that is used to specify additional options for the build process as it is impossible to specify them through ' configopts '. The problem with configopts is that the options are added at the wrong position which causes the build process to fail (they are added before the architecture options which should always be the first one).","title":"Problems met installing 1.9.3 with EasyBuild 3.9.4 (2019b)"},{"location":"generated/easyconfigs/v/VSEARCH/","text":"VSEARCH installation instrictions VSEARCH on GitHub General information VSEARCH needs zlib and bzip2. The build process only produces a single executable. EasyConfigs There is support for VSEARCH in EasyBuild. Version 2.14.2, intel 2019b Started from an EasyBuilders recipe for Intel for an older version of the package. Checking the log files however shows that the wrong compiler options are used. The root of the problem is that the C++ compiler options are hardcoded in src/Makefile.am. Rather than working with a patch file, we chose to edit the file with sed in preconfigopts which may be more portable to newer versions and keeps everything in a single file.","title":"VSEARCH installation instrictions"},{"location":"generated/easyconfigs/v/VSEARCH/#vsearch-installation-instrictions","text":"VSEARCH on GitHub","title":"VSEARCH installation instrictions"},{"location":"generated/easyconfigs/v/VSEARCH/#general-information","text":"VSEARCH needs zlib and bzip2. The build process only produces a single executable.","title":"General information"},{"location":"generated/easyconfigs/v/VSEARCH/#easyconfigs","text":"There is support for VSEARCH in EasyBuild.","title":"EasyConfigs"},{"location":"generated/easyconfigs/v/VSEARCH/#version-2142-intel-2019b","text":"Started from an EasyBuilders recipe for Intel for an older version of the package. Checking the log files however shows that the wrong compiler options are used. The root of the problem is that the C++ compiler options are hardcoded in src/Makefile.am. Rather than working with a patch file, we chose to edit the file with sed in preconfigopts which may be more portable to newer versions and keeps everything in a single file.","title":"Version 2.14.2, intel 2019b"},{"location":"generated/easyconfigs/w/Wannier90/","text":"Wannier90 install instructions Wannier90 web site Wannier90 on GitHub General instructions Wannier90 has no configure script. Options are set by copying one of files in config to make.inc and editing that file. EasyConfigs The first version covered by this documentation is 3.1.0. 3.1.0 for Intel 2020a We started from the MakeCp-based file for 3.0.0 in the EasyBuilders repository. That recipe however doesn't work properly as not all executables that are generated during the build process are copied to the bin directory. In version 3.2.0 there is a install make target which is why we decided to switch to a ConfigureMake-based recipe skipping the configure step. We also removed the patch and instead create an empty make.inc file via prebuildopts . Patches may need to be rewritten for new versions if the Makefiles change while this procedure has more chance to continue working. We do need to check which variables need to be defined though, but this was also the case with the patch. As there is no configure step, we do need to define the PREFIX in the install step.","title":"Wannier90 install instructions"},{"location":"generated/easyconfigs/w/Wannier90/#wannier90-install-instructions","text":"Wannier90 web site Wannier90 on GitHub","title":"Wannier90 install instructions"},{"location":"generated/easyconfigs/w/Wannier90/#general-instructions","text":"Wannier90 has no configure script. Options are set by copying one of files in config to make.inc and editing that file.","title":"General instructions"},{"location":"generated/easyconfigs/w/Wannier90/#easyconfigs","text":"The first version covered by this documentation is 3.1.0.","title":"EasyConfigs"},{"location":"generated/easyconfigs/w/Wannier90/#310-for-intel-2020a","text":"We started from the MakeCp-based file for 3.0.0 in the EasyBuilders repository. That recipe however doesn't work properly as not all executables that are generated during the build process are copied to the bin directory. In version 3.2.0 there is a install make target which is why we decided to switch to a ConfigureMake-based recipe skipping the configure step. We also removed the patch and instead create an empty make.inc file via prebuildopts . Patches may need to be rewritten for new versions if the Makefiles change while this procedure has more chance to continue working. We do need to check which variables need to be defined though, but this was also the case with the patch. As there is no configure step, we do need to define the PREFIX in the install step.","title":"3.1.0 for Intel 2020a"},{"location":"generated/easyconfigs/w/WannierTools/","text":"WannierTools instructions Main WannierTools web site WannierTools documentation Developement of WannierTools on GitHub GitHub releases General instructions A simple Makefile with a number of examples given in the src directory. EasyBuild There is support for WannierTools in the EasyBuilders repository . 2.5.1 in the 2020a toolchain A simple copy and adaptation from the one for 2.3.0 in the EasyBuilders tree, with an update of the documentation. More work could be done to better conform to the settings given in the EasyConfig file as the Makefile now hardcodes all compiler options.","title":"WannierTools instructions"},{"location":"generated/easyconfigs/w/WannierTools/#wanniertools-instructions","text":"Main WannierTools web site WannierTools documentation Developement of WannierTools on GitHub GitHub releases","title":"WannierTools instructions"},{"location":"generated/easyconfigs/w/WannierTools/#general-instructions","text":"A simple Makefile with a number of examples given in the src directory.","title":"General instructions"},{"location":"generated/easyconfigs/w/WannierTools/#easybuild","text":"There is support for WannierTools in the EasyBuilders repository .","title":"EasyBuild"},{"location":"generated/easyconfigs/w/WannierTools/#251-in-the-2020a-toolchain","text":"A simple copy and adaptation from the one for 2.3.0 in the EasyBuilders tree, with an update of the documentation. More work could be done to better conform to the settings given in the EasyConfig file as the Makefile now hardcodes all compiler options.","title":"2.5.1 in the 2020a toolchain"},{"location":"generated/easyconfigs/x/X11/","text":"X11 bundle EasyConfigs Our X11 bundle is derived from the standard EasyBuild one, but with some local additions to avoid installing those package separately. 2019b build set (GCCcore 8.3.0) Switched to GCCcore as the default EasyBuild recipes use GCCcore also. These libraries are likely not performance-critical for scientific applications on a cluster, and it avoids having to develop patches for software that doesn't properly recognize the intel compilers or even breaks with them. Added two packages: libdrm DBus 2020a build set (GCCcore 9.3.0) We were running ahead of the default EasyBuild recipes with our installation for 2020a. As a result, we were the first to run into problems with libxkbcommon which switched to the Meson build system in version 0.9.0. See the EasyConfig for the parameters that we used to get it to work with Meson. Note that our Meson setup is different from the standard EasyBuild one. We don't have standard Meson and Ninja modules, but include both tools in our buildtools bundle and set appropriate EB values so that the MesonNinja EasyBlock recognizes them and thinks they are properly installed. Added package: xprop","title":"X11 bundle"},{"location":"generated/easyconfigs/x/X11/#x11-bundle","text":"","title":"X11 bundle"},{"location":"generated/easyconfigs/x/X11/#easyconfigs","text":"Our X11 bundle is derived from the standard EasyBuild one, but with some local additions to avoid installing those package separately.","title":"EasyConfigs"},{"location":"generated/easyconfigs/x/X11/#2019b-build-set-gcccore-830","text":"Switched to GCCcore as the default EasyBuild recipes use GCCcore also. These libraries are likely not performance-critical for scientific applications on a cluster, and it avoids having to develop patches for software that doesn't properly recognize the intel compilers or even breaks with them. Added two packages: libdrm DBus","title":"2019b build set (GCCcore 8.3.0)"},{"location":"generated/easyconfigs/x/X11/#2020a-build-set-gcccore-930","text":"We were running ahead of the default EasyBuild recipes with our installation for 2020a. As a result, we were the first to run into problems with libxkbcommon which switched to the Meson build system in version 0.9.0. See the EasyConfig for the parameters that we used to get it to work with Meson. Note that our Meson setup is different from the standard EasyBuild one. We don't have standard Meson and Ninja modules, but include both tools in our buildtools bundle and set appropriate EB values so that the MesonNinja EasyBlock recognizes them and thinks they are properly installed. Added package: xprop","title":"2020a build set (GCCcore 9.3.0)"},{"location":"generated/easyconfigs/y/Yambo/","text":"Yambo install instructions Yambo home page Yambo development on GitHub GitHub releases . Note that there are often hidden tags on that page, even at the top, hiding newer versions. The documentation is poor and outdated. Many of the features mentioned do not work anymore or do no exist anymore (such as the test suite), and when installing version 4.5, the information about building Yambo was still for 4.4 and some pages even for earlier versions. Yambo Wiki Yambo forum may provide useful information also. There is a scary page with test results , unfortunately the testing is mostly done with a single compiler, GNU Fortran, except for some GPU builds, and with a single set of linear algebra libraries, where it is not even clear where they come from. It may simply be compiles of the reference libraries from NetLib, or maybe OpenBLAS for BLAS (though on our systems that one is called libopenblas.a). Some articles that may provide additional information: Many-body perturbation theory calculations using the yambo code : Likely for yambo 4.4 as that is the most recent version mentioned in the paper, but contains more information than the documentation does. General information Dependencies libxc : Yambo needs libxc. It however only seems to work with very old versions of libxc. It downloads its own libxc if it cannot find a compatible one. Search for version.*used in `config/libxc.m4'. Yambo binaries (updated for 4.5.x) Executables in the bin directory: Yambo interfaces: a2y : ABINIT interface based on binary Fortran files, lways part of the interfaces e2y : ABINIT interface based on ETSF-IO. Only built when etsf-io support is turned on p2y : This stands for pwscf-2-yambo, the interface to QuantumESPRESSO. Only built when IOTK is enabled, additional features depending on the HDF5 configuration ( `--enable-hdf5-p2y-support ). Core of Yambo consists of: yambo ypp , the Yambo Post- and Pre-Processing tool The Yambo interfaces The main Makefile suggests 4 Yambo projects: PH: ''yambo_ph'' ''ypp_ph'' RT: ''yambo_rt'' ''ypp_rt'': Pre-and postpprocessing for `yambo_rt NL: ''yambo_nl'' ''ypp_nl'': Pre-and postpprocessing for yambo_nl KERR: ''yambo_kerr'' Build process Building with iotk does not make much sense if QuantumESPRESSO supports netCDF/HDF5. It is then better to build with netCDF and HDF5 support. The Yambo build process does download its own internal dependencies during the build step. This can be avoided by putting the files into lib/archive beforehand. The problem is that some of those files don't seem to be the regular downloads, but come from elsewhere. E.g., current downloads of libxc don't contain the configure script ; it has to be generated with autoreconf -i , but the Yambo build process assumes a file that does contain the configure script already. Only in-place builds work. The Makefiles and configure script is buggy as hell and doesn't follow the GNU conventions at all (we checked 4.2.1 and 4.4.1). A proper configure script and associated Makefiles are assumed to distinguish between the source tree (where the configure script is located), the build tree (where configure is run) and the install tree. But the Yambo build process only works OK if all tree are the same (and there is no formal install target for make). Quite often the wrong variable is used in the Makefiles leading to various problems: The Yambo build process has an option to keep object files. That option doesn't seem to work in a non-inplace build as it goes looking for some scripts in the install directory rather than where they are in the build directories. According to the spack package for Yambo, this bug is present since version 4.2.1 (\"As of version 4.2.1 there are hard-coded paths that make the build process fail if the target prefix is not the configure directory\"). It is definitely still present in the version 4.4.1. There is an configure option to not keep them, but that is pretty useless also due to other bugs. A very similar bug also hits in other circumstances. When iotk is enabled, it tries to copy a file to the install directories rather than to the build directories. Although Yambo claims to have a make install target it is actually not there (see also remarks in the spack code which confirm this). --enable-dp has a different meaning than what one would expect. When it is set to yes, it effectively promotes variables that would be single precision otherwise to double precision. There used to be a test suite and it is still mentioned in the documentation . However, the repo is nowhere to be found. The conclusion can only be that Yambo can only be compiled in-place due to a very buggy and non-standard install procedure. Therefore a clean central install is rather difficult. One would have to build with --prefix set to the build directory during the configure phase and then copy whatever need to be copied. There is also no proper make install support, and the configure process has several options that are documented but do not work as advertised. EasyBuild The most elegant solution to compile Yambo is to write a custom EasyBlock that does an in-place compile rather than compiling on /dev/shm due to bugs in the build procedure. There is however a workaround as a second --prefix argument to configure will overwrite the first one. As of September 2020, spack only supports up to 4.2.2 (version from May 2018) and EasyBuild has given up at version 3.4.2. We made our EasyConfig for Yambo 4.4.1 based on the spack recipe for Yambo . Yambo 4.4.1 Dependencies: Yambo 4.4.1 relies on libxc 2.2.3. The files it auto-downloads are different though from those in the standard libxc repositories (see also the build process). We used an externally compiled version as the Yambo build process otherwise failed. Yambo 4.4.1 downloads etsf_io 1.0.4 if needed Yambo 4.4.1 downloads iotk 1.2.2. Even though it can be disabled via configure, it shouldn't be disabled as due to bugs in the build process, Yambo will always try to compile some code that needs iotk. It doesn't make much sense though with modern versions of QuantumESPRESSO that use HDF5. Problems Only an in-place build works, so we had to add --prefix to configopts . Luckily it overwrites the default option added by EasyBuild. There is no install target despite claims on some places. Our files_to_copy is derived form the spack recipe. etsf_io-1.0.4.tar.gz could not be downloaded from the location indicated in the code. Yambo 4.5.3 on intel/2020a Based on the setup of a user who reported problems with 4.4.1 Yambo 4.5.x still does not support a proper make install (nor a proper make help to get decent information about what the Makefiles support). In fact, giving --prefix to configure and pointing to the directory where you ultimately want to install the software, still causes a failure as it did before, so we still have to use the same trick with --prefix as in earlier versions. The use of flags in configure is inconsistent and some have buggy implementations, see below. Dependencies downloaded as much as possible by Yambo to let Yambo configure them Exception: FFTW as that needs a patch to compile with Intel. We did chech the checsum of unzipped tar files with those that we downloaded manually, and all were different. It is not clear if the Yambo authors made any significant changes or if they simple generated their own tar files from a clone repository which may have, e.g., one file different. We did extract the URLs for the dependencies from lib/archive/package.list to download them by EasyBuild and put them in lib/archive to ensure that we can reproduce the install at a later date, even if those links may be dead by then. --disable-iotk still does not work. Neither does --enable-etsf-io. Contact with the author: ETSF-IO is not used anymore in practice and should be removed in a future version. We did try to expand all options that the user did not specify to their default options just to make it as reproducible as possible to later versions which may have different defaults. We need to unset a number of variables the EasyBuild sets. The most crucial one is CPPFLAGS . If that is set, it may be the trigger for very strange compile problems in some routines such as those in lib/qe_pseudo with compilation failing with an error message about a separator in the Makefile. See the issue below. Option Value Where? --enable-keep-objects no (yes needed for debug only) yambo_specific.m4 --enable-keep-src no (default) yambo_specific.m4 --enable-keep-extlibs yes (default) yambo_specific.m4 --enable-dp no (default) BUG yambo_specific.m4 --enable-time-profile yes (user+default) yambo_specific.m4 --enable-memory-profile yes (user) yambo_specific.m4 --enable-uspp no (default) yambo_specific.m4 --enable-msgs-comps yes (user, default is no) yambo_specific.m4 --enable-options-check yes (default) configure.ac --enable-debug-flags no (default) acx_fortran_flags.m4 --enable-open-mp no (default) configure.ac --enable-mpi yes (default+user) configure.ac --enable-int-linalg no (default) blas.m4 --enable-openmp-int-linalg no (default) blas.m4 --enable-internal-fftqe no (default) fft.m4 --enable-internal-fftsg no (default) fft.m4 --enable-3d-fft depends on FFT implementation fft.m4 --enable-slepc-linalg yes (as we want PETSc and SLEPc) petsc_slepc.m4 --enable-par-linalg no (default) scalapack.m4 --enable-netcdf-classic no (default) netcdf_f90.m4 --enable-netcdf-hdf5 no (default) netcdf_f90.m4 --enable-hdf5-comnpression no (default) netcdf_f90.m4 --enable-hdf5-par-io no (default) netcdf_f90.m4 --enable-hdf5-p2y-support no (default) netcdf_f90.m4 --enable-iotk yes (default) iotk.m4 --enable-etsf-io no (default) etsf_io.m4 --disable-cuda cuda.m4 --disable-nvtx cuda.m4 Order of the build process make ext-libs : Build the libraries that need to be downloaded from external sources. The netCDF Fortran build has problems with a parallel build. make int-libs : Builds the external libraries if not yet build Build the internal libraries ( qe_pseudo , slatec , math77 , `local ) make yambo : Builds the internal and external libraries if not yet build Builds yambo . make ypp : Builds the internal and external libraries if not yet build Builds ypp make a2y (if relevant) Builds the internal and external libraries if not yet build Builds a2y make p2y (if relevant) Builds the internal and external libraries if not yet build Builds p2y make e2y (if relevant) Builds the internal and external libraries if not yet build Builds e2y make yambo_ph Builds the internal and external libraries if not yet build Builds yambo_ph make ypp_ph Builds the internal and external libraries if not yet build Builds ypp_ph make yambo_rt Builds the internal and external libraries if not yet build Builds yambo_rt make ypp_rt Builds the internal and external libraries if not yet build Builds ypp_rt make yambo_nl Builds the internal and external libraries if not yet build Builds yambo_nl make ypp_nl Builds the internal and external libraries if not yet build Builds `ypp_nl make yambo_kerr Builds the internal and external libraries if not yet build Builds yambo_kerr Single-target builds: make libs is equivalent to make ext-libs int-libs make core is equivalent to make ext-libs int-libs yambo ypp a2y p2y e2y , but leaving out the interfaces that are irrelevant. make interfaces is a shortcut to build a2y , p2y and e2y in that order. make ph_project is equivalent to make ext-libs int-libs yambo_ph ypp_ph make rt_project is equivalent to make ext-libs int-libs yambo_rt ypp_rt make nl_project is equivalent to make ext-libs int-libs yambo_nl ypp_nl make kerr_project is equivalent to make ext-libs int-libs yambo_kerr make all is equivalent to make ext-libs int-libs yambo ypp a2y p2y e2y yambo_ph ypp_ph yambo_rt ypp_rt yambo_nl ypp_nl yambo_kerr Issue with separators in some Makefiles The build process for some of the included libraries such as qe_pseudo can fail with an error message about a separator in the Makefile. These Makefiles are generated by the sbin/make_makefile.sh -script and use hidden .objects -files in the library directories. The root of the problem is that make_makefile.sh processes those files using cpp and relies on the traditional, pre-C99-behaviour when dealing with lines in .objects that end with a backslash (to denote continuation lines). If the -traditional -flag is missing, .objects can be expanded into something that is not legal in a Makefile as the backslashes are removed while the lines are not joined together. The crucial block of code is cp $cdir / $ofile $cdir / $ofile .c $cpp $cppflags $dopts -D_ $os -D_ $target $cdir / $ofile .c >> $cdir /Makefile rm -f $cdir / $ofile .c in make_makefile.sh.in (and after running configure in make_makefile.sh ). If the environment variable CPPFLAGS is set when calling configure , $cpp contains cpp $CPPFLAGS (with $CPPFLAGS actually expanded) instead of cpp -traditional . There are two possible solutions * Unset CPPFLAGS in preconfigopts in EasyBuild if the options are not really needed (and they are not needed when using the Intel toolchain). * Patch make_makefile.sh.in to use $cpp -traditional in that line or maybe even simply cpp -traditional . Note that configure script will actually select the cpp from GCCcore when using the Intel toolchain. Bug 1: --prefix in configure The standard configure flag --prefix which is used to ensure that a make install at the end installs the code in the proper directory, doesn't work as it should. In fact, not only is a working make install missing, but pointing --prefix to anything else as the build directory causes a failure during the build as the code goes looking for include files in the wrong directories. Bug 2: --enable-dp processing According to the help information of configure, --enable-dp=no should be equivalent to --disable-dp , or, given that the default is to configure for single precision, not specifying this option. This however does not work and is a bug in the option handling for --enable-dp , where only action is taken if nothing is specified or if the value is yes, leaving the variable build_precision uninitialized (and causing the build of PETSc to fail). The actual bug is in config/yambo_specific.m4 . Option enableval enable_dp def_dp build_precision --enable-dp yes yes -D_DOUBLE double --disable-dp no no --enable-dp=yes yes yes -D_DOUBLE double --enable-dp=no no no / single build_precision is actually left uninitialized when --disable-dp or --enable-dp=no is given. Solution: in config/yambo_specific.m4 (or directly in configure if you don't want to regenerate): def_dp = \"\" if test x \" $enable_dp \" = \"x\" ; then enable_dp = \"no\" ; build_precision = \"single\" ; fi if test x \" $enable_dp \" = \"xyes\" ; then def_dp = \"-D_DOUBLE\" ; build_precision = \"double\" ; fi should become, e.g., if test x \" $enable_dp \" = \"x\" ; then enable_dp = \"no\" ; fi def_dp = \"\" build_precision = \"single\" if test x \" $enable_dp \" = \"xyes\" ; then def_dp = \"-D_DOUBLE\" ; build_precision = \"double\" ; fi (and the first line is only needed if you want to ensure that enable_dp is not empty which may help avoiding trouble further down the configure script.) This would be robust even if the user made a typo (e.g., specifying `--enable-dp=ja ), but no error message would be shown. Even safer would be: if test x \" $enable_dp \" = \"x\" ; then enable_dp = \"no\" ; fi if test x \" $enable_dp \" = \"xno\" ; then def_dp = \"\" ; build_precision = \"single\" ; elif test x \" $enable_dp \" = \"xyes\" ; then def_dp = \"-D_DOUBLE\" ; build_precision = \"double\" ; else AC_MSG_ERROR ([ Illegal value for --enable-dp ]) fi There are many more cases where the code in the configure script could be made more robust in this way, though it doesn't seem to be common practice to do so. SUBMITTED AS ISSUE #31 Bug 3: --disable-iotk does not work When disabling IOTK, compilation fails in lib/qe_pseudo/read_upf_v2.f90 as it still tries to call IOTK functions. SUBMITTED AS ISSUE #30 . Bug 4: --enable-etsf-io does not work if no library is given with the --with-options The build process fails to download the etsf-io package. TThe cause is a download failure, leaving an empty file. The download is done by the Makefile in lib/archive . On our system, this executes the command wget --no-check-certificate -O etsf_io-1.0.4.tar.gz https://github.com/yambo-code/yambo/files/845218/ This is not what is intended, and the cause is a typo in package.list. Line 56 should read url_etsf_io = https://github.com/yambo-code/yambo/files/845218/ $( tarball_etsf_io ) (the _io at the end is missing). SUBMITTED AS ISSUE #29 . Bug 5: The versions of PETSc and SLEPc that are being used are so old they still requires Python 2 The versions of PETSc and SLEPc that are included in Yambo 4.5.3 still require that python points to a Python 2 interpreter. As Python 2 is no longer maintained, more and more operating systems default to Python 3 so Yambo fails to configure PETSc. Workaround: Modify Makefile.loc to call python2 ./configure ... rather then simply ./configure ... . It is definitely easier for system manager to make sure that there is still a Python 2 on their systems that can be called as python2 then to change the default python back to Python 2 just to compile Yambo. A better solution would be to use a more recent version of PETSc and SLEPc as they support both Python 2.7 and 3.4+. UPDATE: This appears to be fixed in the (as of writing) upcoming version 5.0, where much more recent versions of PETSc and SLEPc are used. Bug 6: Processing of --enable-keep-src Not exactly a bug, but there is something strange in config/yambo_specific.m4 : if test x \" $enable_keep_src \" = \"x\" ; then enable_keep_src = \"no\" ; fi if test x \" $enable_keep_src \" = \"xyes\" ; then enable_keep_src = \"yes\" ; fi What is the point of the second line? If enable_keep_src is yes we set it again to yes ? Bug 7: Inconsistent handling of --with-*-libs -options and corresponding --enable-* options In some cases, --with-*-libs=no will disable that feature, while in most cases it is assumed that that option if specified refers to a set of options for the link line and should not be either yes or no . The problem is that --with-petsc-libs and --with-slepc-libs are given a double meaning that way and function also as --with-petsc and --with-slepc . They can overwrite the effect of --enable-slepc-linalg . --enable-par-linalg does not just enable BLACS and ScaLAPACK but enforces internally compiled ones. So you should omit the flag or set it to no if you want to use an external BLACS/ScaLAPACK which is very confusing. Other --enable-* flags work a bit more as expected though it looks like a value of no can be overwritten by --with-*-libs and similar options, which may not be a good idea. Another thing that is not taken into account is that on some supercomputers (or workstations set up the same way) there are environment variables that point to the libraries and the include files so that compilers may find them without `-L and -I -options, which is why --enable- or --with- `` flags make sense. Bug 8: Build process of libxc The build process does not support a parallel make due to errors in the dependencies. So sometimes the compiler tries to open a module file that is not yet generated. Missing information BLAS: In which order are BLAS libraries searched for? First those given with the with-options? Then there is a list of predefined libraries that is searched for (MKL is not one of them, the list is very outdated, several of those libraries do not exist anymore) This can always be overwritten with --enable-int-linalg? --enable-openmp-int-linalg does not automatically do --enable-int-linalg? When are single-threaded BLAS libraries preferred and when should we use multithreaded ones? FFT: When to use which flag? When is which FFT library preferred? Or can we use multiple at ones? A decision tree would be nice to have. Which FFTW interfaces are used if available? Can MKL be used for FFTW, and if so, how? config/fft.m4 clearly does something with it. It looks like configure first tries to find a FFTW-compatible library specified through the -with-options and then goes on trying other FFT libraries. When can we use --enable-3d-fft and when can we not? And the default value is very unclear. It seems to be enabled by default when using the FFT from ESSL. It also seems to be enabled by default if the internal FFT QE (which is also FFTW2) is used. It doesn't seem to be used with the internal FFT SG. FFT QE takes precedence over FFT SG and any FFTW-compatible library found if specified. FFT SG takes precedense over any FFTW-compatible library found if specified. The whole story of --enable-slepc-linalg, --with-petsc-libs and --with-slepc-libs is also downright confusing as these options have effect onto each other. Does it make sense to use PETSc without SLEPc? And --with-petsc-libs=no undoes some of the effect of --enable-slepc-linalg. Are there cases where SLEPc makes sense without PETSc? It doesn't seem so. Does it make sense to use PETSc without SLEPc? Likely not? Do these routines make sense for a non-MPI build? --enable-par-linalg is confusing. It enforces an internal BLACS and ScaLAPACK. And do the checks take into account that we should have a pETSc library suitably configured according to the requested precision? A warning should also be printed if the flag conflicts the MPI setting (i.e., parallel linear algebra requested for a non-MPI build). Which configurations for an external netCDF are supported? It does need the netCDF Fortran interfaces. Furthermore, the following configurations can be generated internally: netCDF without HDF5 support netCDF classic, whatever that is netCDF with HDF5 support With or without HDF5 compression With or without HDF5 parallel I/O. --enable-iotk=no will be overwritten if --with-iotk-path points to a directory or if --with-iotk-path points to a directory or if --with-iotk-libs is specified and non-empty. --enable-slepc-linalg and --enable-par-linalg are very confusing. There should be a clear default. Moreover. * if slecpc_linalg is disabled, a warning should be printed that all --with-petsc-* and --with-slepc-* settings are neglected if any of them is given, * if slecpc_linalg is enabled is disabled, it should try to find the corresponding libraries using the --with-slepc-* and --with-petsc-* variables or if these are not given, use internal ones. * Moreover, it may not make sense to use an external SLEPc with an internal PETSc as that one may be incompatible with the external SLEPc. * if par_linalg is disabled, a warning should be printed that all --with-blacs-* and --with-scalapack-* flags will be neglected if any of them is given. * If par_linalg is enabled, the configure script should try to locate suitable BLACS and ScaLAPACK libraries with the help of the --with-blacs-* and --with-scalapack-* flags and if not revert to internal ones. And again it may not make sense to use an external ScaLAPACK with an internal BLACS.","title":"Yambo install instructions"},{"location":"generated/easyconfigs/y/Yambo/#yambo-install-instructions","text":"Yambo home page Yambo development on GitHub GitHub releases . Note that there are often hidden tags on that page, even at the top, hiding newer versions. The documentation is poor and outdated. Many of the features mentioned do not work anymore or do no exist anymore (such as the test suite), and when installing version 4.5, the information about building Yambo was still for 4.4 and some pages even for earlier versions. Yambo Wiki Yambo forum may provide useful information also. There is a scary page with test results , unfortunately the testing is mostly done with a single compiler, GNU Fortran, except for some GPU builds, and with a single set of linear algebra libraries, where it is not even clear where they come from. It may simply be compiles of the reference libraries from NetLib, or maybe OpenBLAS for BLAS (though on our systems that one is called libopenblas.a). Some articles that may provide additional information: Many-body perturbation theory calculations using the yambo code : Likely for yambo 4.4 as that is the most recent version mentioned in the paper, but contains more information than the documentation does.","title":"Yambo install instructions"},{"location":"generated/easyconfigs/y/Yambo/#general-information","text":"","title":"General information"},{"location":"generated/easyconfigs/y/Yambo/#dependencies","text":"libxc : Yambo needs libxc. It however only seems to work with very old versions of libxc. It downloads its own libxc if it cannot find a compatible one. Search for version.*used in `config/libxc.m4'.","title":"Dependencies"},{"location":"generated/easyconfigs/y/Yambo/#yambo-binaries-updated-for-45x","text":"Executables in the bin directory: Yambo interfaces: a2y : ABINIT interface based on binary Fortran files, lways part of the interfaces e2y : ABINIT interface based on ETSF-IO. Only built when etsf-io support is turned on p2y : This stands for pwscf-2-yambo, the interface to QuantumESPRESSO. Only built when IOTK is enabled, additional features depending on the HDF5 configuration ( `--enable-hdf5-p2y-support ). Core of Yambo consists of: yambo ypp , the Yambo Post- and Pre-Processing tool The Yambo interfaces The main Makefile suggests 4 Yambo projects: PH: ''yambo_ph'' ''ypp_ph'' RT: ''yambo_rt'' ''ypp_rt'': Pre-and postpprocessing for `yambo_rt NL: ''yambo_nl'' ''ypp_nl'': Pre-and postpprocessing for yambo_nl KERR: ''yambo_kerr''","title":"Yambo binaries (updated for 4.5.x)"},{"location":"generated/easyconfigs/y/Yambo/#build-process","text":"Building with iotk does not make much sense if QuantumESPRESSO supports netCDF/HDF5. It is then better to build with netCDF and HDF5 support. The Yambo build process does download its own internal dependencies during the build step. This can be avoided by putting the files into lib/archive beforehand. The problem is that some of those files don't seem to be the regular downloads, but come from elsewhere. E.g., current downloads of libxc don't contain the configure script ; it has to be generated with autoreconf -i , but the Yambo build process assumes a file that does contain the configure script already. Only in-place builds work. The Makefiles and configure script is buggy as hell and doesn't follow the GNU conventions at all (we checked 4.2.1 and 4.4.1). A proper configure script and associated Makefiles are assumed to distinguish between the source tree (where the configure script is located), the build tree (where configure is run) and the install tree. But the Yambo build process only works OK if all tree are the same (and there is no formal install target for make). Quite often the wrong variable is used in the Makefiles leading to various problems: The Yambo build process has an option to keep object files. That option doesn't seem to work in a non-inplace build as it goes looking for some scripts in the install directory rather than where they are in the build directories. According to the spack package for Yambo, this bug is present since version 4.2.1 (\"As of version 4.2.1 there are hard-coded paths that make the build process fail if the target prefix is not the configure directory\"). It is definitely still present in the version 4.4.1. There is an configure option to not keep them, but that is pretty useless also due to other bugs. A very similar bug also hits in other circumstances. When iotk is enabled, it tries to copy a file to the install directories rather than to the build directories. Although Yambo claims to have a make install target it is actually not there (see also remarks in the spack code which confirm this). --enable-dp has a different meaning than what one would expect. When it is set to yes, it effectively promotes variables that would be single precision otherwise to double precision. There used to be a test suite and it is still mentioned in the documentation . However, the repo is nowhere to be found. The conclusion can only be that Yambo can only be compiled in-place due to a very buggy and non-standard install procedure. Therefore a clean central install is rather difficult. One would have to build with --prefix set to the build directory during the configure phase and then copy whatever need to be copied. There is also no proper make install support, and the configure process has several options that are documented but do not work as advertised.","title":"Build process"},{"location":"generated/easyconfigs/y/Yambo/#easybuild","text":"The most elegant solution to compile Yambo is to write a custom EasyBlock that does an in-place compile rather than compiling on /dev/shm due to bugs in the build procedure. There is however a workaround as a second --prefix argument to configure will overwrite the first one. As of September 2020, spack only supports up to 4.2.2 (version from May 2018) and EasyBuild has given up at version 3.4.2. We made our EasyConfig for Yambo 4.4.1 based on the spack recipe for Yambo .","title":"EasyBuild"},{"location":"generated/easyconfigs/y/Yambo/#yambo-441","text":"Dependencies: Yambo 4.4.1 relies on libxc 2.2.3. The files it auto-downloads are different though from those in the standard libxc repositories (see also the build process). We used an externally compiled version as the Yambo build process otherwise failed. Yambo 4.4.1 downloads etsf_io 1.0.4 if needed Yambo 4.4.1 downloads iotk 1.2.2. Even though it can be disabled via configure, it shouldn't be disabled as due to bugs in the build process, Yambo will always try to compile some code that needs iotk. It doesn't make much sense though with modern versions of QuantumESPRESSO that use HDF5. Problems Only an in-place build works, so we had to add --prefix to configopts . Luckily it overwrites the default option added by EasyBuild. There is no install target despite claims on some places. Our files_to_copy is derived form the spack recipe. etsf_io-1.0.4.tar.gz could not be downloaded from the location indicated in the code.","title":"Yambo 4.4.1"},{"location":"generated/easyconfigs/y/Yambo/#yambo-453-on-intel2020a","text":"Based on the setup of a user who reported problems with 4.4.1 Yambo 4.5.x still does not support a proper make install (nor a proper make help to get decent information about what the Makefiles support). In fact, giving --prefix to configure and pointing to the directory where you ultimately want to install the software, still causes a failure as it did before, so we still have to use the same trick with --prefix as in earlier versions. The use of flags in configure is inconsistent and some have buggy implementations, see below. Dependencies downloaded as much as possible by Yambo to let Yambo configure them Exception: FFTW as that needs a patch to compile with Intel. We did chech the checsum of unzipped tar files with those that we downloaded manually, and all were different. It is not clear if the Yambo authors made any significant changes or if they simple generated their own tar files from a clone repository which may have, e.g., one file different. We did extract the URLs for the dependencies from lib/archive/package.list to download them by EasyBuild and put them in lib/archive to ensure that we can reproduce the install at a later date, even if those links may be dead by then. --disable-iotk still does not work. Neither does --enable-etsf-io. Contact with the author: ETSF-IO is not used anymore in practice and should be removed in a future version. We did try to expand all options that the user did not specify to their default options just to make it as reproducible as possible to later versions which may have different defaults. We need to unset a number of variables the EasyBuild sets. The most crucial one is CPPFLAGS . If that is set, it may be the trigger for very strange compile problems in some routines such as those in lib/qe_pseudo with compilation failing with an error message about a separator in the Makefile. See the issue below. Option Value Where? --enable-keep-objects no (yes needed for debug only) yambo_specific.m4 --enable-keep-src no (default) yambo_specific.m4 --enable-keep-extlibs yes (default) yambo_specific.m4 --enable-dp no (default) BUG yambo_specific.m4 --enable-time-profile yes (user+default) yambo_specific.m4 --enable-memory-profile yes (user) yambo_specific.m4 --enable-uspp no (default) yambo_specific.m4 --enable-msgs-comps yes (user, default is no) yambo_specific.m4 --enable-options-check yes (default) configure.ac --enable-debug-flags no (default) acx_fortran_flags.m4 --enable-open-mp no (default) configure.ac --enable-mpi yes (default+user) configure.ac --enable-int-linalg no (default) blas.m4 --enable-openmp-int-linalg no (default) blas.m4 --enable-internal-fftqe no (default) fft.m4 --enable-internal-fftsg no (default) fft.m4 --enable-3d-fft depends on FFT implementation fft.m4 --enable-slepc-linalg yes (as we want PETSc and SLEPc) petsc_slepc.m4 --enable-par-linalg no (default) scalapack.m4 --enable-netcdf-classic no (default) netcdf_f90.m4 --enable-netcdf-hdf5 no (default) netcdf_f90.m4 --enable-hdf5-comnpression no (default) netcdf_f90.m4 --enable-hdf5-par-io no (default) netcdf_f90.m4 --enable-hdf5-p2y-support no (default) netcdf_f90.m4 --enable-iotk yes (default) iotk.m4 --enable-etsf-io no (default) etsf_io.m4 --disable-cuda cuda.m4 --disable-nvtx cuda.m4","title":"Yambo 4.5.3 on intel/2020a"},{"location":"generated/easyconfigs/y/Yambo/#order-of-the-build-process","text":"make ext-libs : Build the libraries that need to be downloaded from external sources. The netCDF Fortran build has problems with a parallel build. make int-libs : Builds the external libraries if not yet build Build the internal libraries ( qe_pseudo , slatec , math77 , `local ) make yambo : Builds the internal and external libraries if not yet build Builds yambo . make ypp : Builds the internal and external libraries if not yet build Builds ypp make a2y (if relevant) Builds the internal and external libraries if not yet build Builds a2y make p2y (if relevant) Builds the internal and external libraries if not yet build Builds p2y make e2y (if relevant) Builds the internal and external libraries if not yet build Builds e2y make yambo_ph Builds the internal and external libraries if not yet build Builds yambo_ph make ypp_ph Builds the internal and external libraries if not yet build Builds ypp_ph make yambo_rt Builds the internal and external libraries if not yet build Builds yambo_rt make ypp_rt Builds the internal and external libraries if not yet build Builds ypp_rt make yambo_nl Builds the internal and external libraries if not yet build Builds yambo_nl make ypp_nl Builds the internal and external libraries if not yet build Builds `ypp_nl make yambo_kerr Builds the internal and external libraries if not yet build Builds yambo_kerr Single-target builds: make libs is equivalent to make ext-libs int-libs make core is equivalent to make ext-libs int-libs yambo ypp a2y p2y e2y , but leaving out the interfaces that are irrelevant. make interfaces is a shortcut to build a2y , p2y and e2y in that order. make ph_project is equivalent to make ext-libs int-libs yambo_ph ypp_ph make rt_project is equivalent to make ext-libs int-libs yambo_rt ypp_rt make nl_project is equivalent to make ext-libs int-libs yambo_nl ypp_nl make kerr_project is equivalent to make ext-libs int-libs yambo_kerr make all is equivalent to make ext-libs int-libs yambo ypp a2y p2y e2y yambo_ph ypp_ph yambo_rt ypp_rt yambo_nl ypp_nl yambo_kerr","title":"Order of the build process"},{"location":"generated/easyconfigs/y/Yambo/#issue-with-separators-in-some-makefiles","text":"The build process for some of the included libraries such as qe_pseudo can fail with an error message about a separator in the Makefile. These Makefiles are generated by the sbin/make_makefile.sh -script and use hidden .objects -files in the library directories. The root of the problem is that make_makefile.sh processes those files using cpp and relies on the traditional, pre-C99-behaviour when dealing with lines in .objects that end with a backslash (to denote continuation lines). If the -traditional -flag is missing, .objects can be expanded into something that is not legal in a Makefile as the backslashes are removed while the lines are not joined together. The crucial block of code is cp $cdir / $ofile $cdir / $ofile .c $cpp $cppflags $dopts -D_ $os -D_ $target $cdir / $ofile .c >> $cdir /Makefile rm -f $cdir / $ofile .c in make_makefile.sh.in (and after running configure in make_makefile.sh ). If the environment variable CPPFLAGS is set when calling configure , $cpp contains cpp $CPPFLAGS (with $CPPFLAGS actually expanded) instead of cpp -traditional . There are two possible solutions * Unset CPPFLAGS in preconfigopts in EasyBuild if the options are not really needed (and they are not needed when using the Intel toolchain). * Patch make_makefile.sh.in to use $cpp -traditional in that line or maybe even simply cpp -traditional . Note that configure script will actually select the cpp from GCCcore when using the Intel toolchain.","title":"Issue with separators in some Makefiles"},{"location":"generated/easyconfigs/y/Yambo/#bug-1-prefix-in-configure","text":"The standard configure flag --prefix which is used to ensure that a make install at the end installs the code in the proper directory, doesn't work as it should. In fact, not only is a working make install missing, but pointing --prefix to anything else as the build directory causes a failure during the build as the code goes looking for include files in the wrong directories.","title":"Bug 1: --prefix in configure"},{"location":"generated/easyconfigs/y/Yambo/#bug-2-enable-dp-processing","text":"According to the help information of configure, --enable-dp=no should be equivalent to --disable-dp , or, given that the default is to configure for single precision, not specifying this option. This however does not work and is a bug in the option handling for --enable-dp , where only action is taken if nothing is specified or if the value is yes, leaving the variable build_precision uninitialized (and causing the build of PETSc to fail). The actual bug is in config/yambo_specific.m4 . Option enableval enable_dp def_dp build_precision --enable-dp yes yes -D_DOUBLE double --disable-dp no no --enable-dp=yes yes yes -D_DOUBLE double --enable-dp=no no no / single build_precision is actually left uninitialized when --disable-dp or --enable-dp=no is given. Solution: in config/yambo_specific.m4 (or directly in configure if you don't want to regenerate): def_dp = \"\" if test x \" $enable_dp \" = \"x\" ; then enable_dp = \"no\" ; build_precision = \"single\" ; fi if test x \" $enable_dp \" = \"xyes\" ; then def_dp = \"-D_DOUBLE\" ; build_precision = \"double\" ; fi should become, e.g., if test x \" $enable_dp \" = \"x\" ; then enable_dp = \"no\" ; fi def_dp = \"\" build_precision = \"single\" if test x \" $enable_dp \" = \"xyes\" ; then def_dp = \"-D_DOUBLE\" ; build_precision = \"double\" ; fi (and the first line is only needed if you want to ensure that enable_dp is not empty which may help avoiding trouble further down the configure script.) This would be robust even if the user made a typo (e.g., specifying `--enable-dp=ja ), but no error message would be shown. Even safer would be: if test x \" $enable_dp \" = \"x\" ; then enable_dp = \"no\" ; fi if test x \" $enable_dp \" = \"xno\" ; then def_dp = \"\" ; build_precision = \"single\" ; elif test x \" $enable_dp \" = \"xyes\" ; then def_dp = \"-D_DOUBLE\" ; build_precision = \"double\" ; else AC_MSG_ERROR ([ Illegal value for --enable-dp ]) fi There are many more cases where the code in the configure script could be made more robust in this way, though it doesn't seem to be common practice to do so. SUBMITTED AS ISSUE #31","title":"Bug 2: --enable-dp processing"},{"location":"generated/easyconfigs/y/Yambo/#bug-3-disable-iotk-does-not-work","text":"When disabling IOTK, compilation fails in lib/qe_pseudo/read_upf_v2.f90 as it still tries to call IOTK functions. SUBMITTED AS ISSUE #30 .","title":"Bug 3: --disable-iotk does not work"},{"location":"generated/easyconfigs/y/Yambo/#bug-4-enable-etsf-io-does-not-work-if-no-library-is-given-with-the-with-options","text":"The build process fails to download the etsf-io package. TThe cause is a download failure, leaving an empty file. The download is done by the Makefile in lib/archive . On our system, this executes the command wget --no-check-certificate -O etsf_io-1.0.4.tar.gz https://github.com/yambo-code/yambo/files/845218/ This is not what is intended, and the cause is a typo in package.list. Line 56 should read url_etsf_io = https://github.com/yambo-code/yambo/files/845218/ $( tarball_etsf_io ) (the _io at the end is missing). SUBMITTED AS ISSUE #29 .","title":"Bug 4: --enable-etsf-io does not work if no library is given with the --with-options"},{"location":"generated/easyconfigs/y/Yambo/#bug-5-the-versions-of-petsc-and-slepc-that-are-being-used-are-so-old-they-still-requires-python-2","text":"The versions of PETSc and SLEPc that are included in Yambo 4.5.3 still require that python points to a Python 2 interpreter. As Python 2 is no longer maintained, more and more operating systems default to Python 3 so Yambo fails to configure PETSc. Workaround: Modify Makefile.loc to call python2 ./configure ... rather then simply ./configure ... . It is definitely easier for system manager to make sure that there is still a Python 2 on their systems that can be called as python2 then to change the default python back to Python 2 just to compile Yambo. A better solution would be to use a more recent version of PETSc and SLEPc as they support both Python 2.7 and 3.4+. UPDATE: This appears to be fixed in the (as of writing) upcoming version 5.0, where much more recent versions of PETSc and SLEPc are used.","title":"Bug 5: The versions of PETSc and SLEPc that are being used are so old they still requires Python 2"},{"location":"generated/easyconfigs/y/Yambo/#bug-6-processing-of-enable-keep-src","text":"Not exactly a bug, but there is something strange in config/yambo_specific.m4 : if test x \" $enable_keep_src \" = \"x\" ; then enable_keep_src = \"no\" ; fi if test x \" $enable_keep_src \" = \"xyes\" ; then enable_keep_src = \"yes\" ; fi What is the point of the second line? If enable_keep_src is yes we set it again to yes ?","title":"Bug 6: Processing of --enable-keep-src"},{"location":"generated/easyconfigs/y/Yambo/#bug-7-inconsistent-handling-of-with-libs-options-and-corresponding-enable-options","text":"In some cases, --with-*-libs=no will disable that feature, while in most cases it is assumed that that option if specified refers to a set of options for the link line and should not be either yes or no . The problem is that --with-petsc-libs and --with-slepc-libs are given a double meaning that way and function also as --with-petsc and --with-slepc . They can overwrite the effect of --enable-slepc-linalg . --enable-par-linalg does not just enable BLACS and ScaLAPACK but enforces internally compiled ones. So you should omit the flag or set it to no if you want to use an external BLACS/ScaLAPACK which is very confusing. Other --enable-* flags work a bit more as expected though it looks like a value of no can be overwritten by --with-*-libs and similar options, which may not be a good idea. Another thing that is not taken into account is that on some supercomputers (or workstations set up the same way) there are environment variables that point to the libraries and the include files so that compilers may find them without `-L and -I -options, which is why --enable- or --with- `` flags make sense.","title":"Bug 7: Inconsistent handling of --with-*-libs-options and corresponding --enable-* options"},{"location":"generated/easyconfigs/y/Yambo/#bug-8-build-process-of-libxc","text":"The build process does not support a parallel make due to errors in the dependencies. So sometimes the compiler tries to open a module file that is not yet generated.","title":"Bug 8: Build process of libxc"},{"location":"generated/easyconfigs/y/Yambo/#missing-information","text":"BLAS: In which order are BLAS libraries searched for? First those given with the with-options? Then there is a list of predefined libraries that is searched for (MKL is not one of them, the list is very outdated, several of those libraries do not exist anymore) This can always be overwritten with --enable-int-linalg? --enable-openmp-int-linalg does not automatically do --enable-int-linalg? When are single-threaded BLAS libraries preferred and when should we use multithreaded ones? FFT: When to use which flag? When is which FFT library preferred? Or can we use multiple at ones? A decision tree would be nice to have. Which FFTW interfaces are used if available? Can MKL be used for FFTW, and if so, how? config/fft.m4 clearly does something with it. It looks like configure first tries to find a FFTW-compatible library specified through the -with-options and then goes on trying other FFT libraries. When can we use --enable-3d-fft and when can we not? And the default value is very unclear. It seems to be enabled by default when using the FFT from ESSL. It also seems to be enabled by default if the internal FFT QE (which is also FFTW2) is used. It doesn't seem to be used with the internal FFT SG. FFT QE takes precedence over FFT SG and any FFTW-compatible library found if specified. FFT SG takes precedense over any FFTW-compatible library found if specified. The whole story of --enable-slepc-linalg, --with-petsc-libs and --with-slepc-libs is also downright confusing as these options have effect onto each other. Does it make sense to use PETSc without SLEPc? And --with-petsc-libs=no undoes some of the effect of --enable-slepc-linalg. Are there cases where SLEPc makes sense without PETSc? It doesn't seem so. Does it make sense to use PETSc without SLEPc? Likely not? Do these routines make sense for a non-MPI build? --enable-par-linalg is confusing. It enforces an internal BLACS and ScaLAPACK. And do the checks take into account that we should have a pETSc library suitably configured according to the requested precision? A warning should also be printed if the flag conflicts the MPI setting (i.e., parallel linear algebra requested for a non-MPI build). Which configurations for an external netCDF are supported? It does need the netCDF Fortran interfaces. Furthermore, the following configurations can be generated internally: netCDF without HDF5 support netCDF classic, whatever that is netCDF with HDF5 support With or without HDF5 compression With or without HDF5 parallel I/O. --enable-iotk=no will be overwritten if --with-iotk-path points to a directory or if --with-iotk-path points to a directory or if --with-iotk-libs is specified and non-empty. --enable-slepc-linalg and --enable-par-linalg are very confusing. There should be a clear default. Moreover. * if slecpc_linalg is disabled, a warning should be printed that all --with-petsc-* and --with-slepc-* settings are neglected if any of them is given, * if slecpc_linalg is enabled is disabled, it should try to find the corresponding libraries using the --with-slepc-* and --with-petsc-* variables or if these are not given, use internal ones. * Moreover, it may not make sense to use an external SLEPc with an internal PETSc as that one may be incompatible with the external SLEPc. * if par_linalg is disabled, a warning should be printed that all --with-blacs-* and --with-scalapack-* flags will be neglected if any of them is given. * If par_linalg is enabled, the configure script should try to locate suitable BLACS and ScaLAPACK libraries with the help of the --with-blacs-* and --with-scalapack-* flags and if not revert to internal ones. And again it may not make sense to use an external ScaLAPACK with an internal BLACS.","title":"Missing information"},{"location":"generated/easyconfigs/y/yaff/","text":"YAFF installation instructions YAFF is software from the molmod research group at UGent. However, there is a LAMMPS user module that uses it. Yaff documentation - When I checked this page the documentation was for the most recent version on PyPi but that one was older then the most recent one available on GitHug (see below). Yaff on GitHub GitHub releases EasyBuild There is support for yaff in the EasyBuilders repository . Yaff 1.6.0 in the 2020a toolchains Only installed with Python 3.8.3 as it was really installed for use with LAMMPS. Started from the default EasyConfig, but changed the version of the molmod dependency to the one that was already installed and deleted the h5py dependency as that package is part of our standard Python module.","title":"YAFF installation instructions"},{"location":"generated/easyconfigs/y/yaff/#yaff-installation-instructions","text":"YAFF is software from the molmod research group at UGent. However, there is a LAMMPS user module that uses it. Yaff documentation - When I checked this page the documentation was for the most recent version on PyPi but that one was older then the most recent one available on GitHug (see below). Yaff on GitHub GitHub releases","title":"YAFF installation instructions"},{"location":"generated/easyconfigs/y/yaff/#easybuild","text":"There is support for yaff in the EasyBuilders repository .","title":"EasyBuild"},{"location":"generated/easyconfigs/y/yaff/#yaff-160-in-the-2020a-toolchains","text":"Only installed with Python 3.8.3 as it was really installed for use with LAMMPS. Started from the default EasyConfig, but changed the version of the molmod dependency to the one that was already installed and deleted the h5py dependency as that package is part of our standard Python module.","title":"Yaff 1.6.0 in the 2020a toolchains"},{"location":"generated/f/FLAC/package/","text":"FLAC Technical documentation","title":"FLAC"},{"location":"generated/f/FLAC/package/#flac","text":"Technical documentation","title":"FLAC"},{"location":"generated/f/Flye/package/","text":"Flye Technical documentation","title":"Flye"},{"location":"generated/f/Flye/package/#flye","text":"Technical documentation","title":"Flye"},{"location":"generated/f/fastp/package/","text":"fastp Technical documentation","title":"fastp"},{"location":"generated/f/fastp/package/#fastp","text":"Technical documentation","title":"fastp"},{"location":"generated/g/GATK/package/","text":"GATK Technical documentation","title":"GATK"},{"location":"generated/g/GATK/package/#gatk","text":"Technical documentation","title":"GATK"},{"location":"generated/g/GPAW/package/","text":"GPAW Technical documentation","title":"GPAW"},{"location":"generated/g/GPAW/package/#gpaw","text":"Technical documentation","title":"GPAW"},{"location":"generated/g/GROMACS/package/","text":"GROMACS Technical documentation","title":"GROMACS"},{"location":"generated/g/GROMACS/package/#gromacs","text":"Technical documentation","title":"GROMACS"},{"location":"generated/g/GenomeTools/package/","text":"GenomeTools Technical documentation","title":"GenomeTools"},{"location":"generated/g/GenomeTools/package/#genometools","text":"Technical documentation","title":"GenomeTools"},{"location":"generated/g/gnuplot/package/","text":"gnuplot Technical documentation","title":"gnuplot"},{"location":"generated/g/gnuplot/package/#gnuplot","text":"Technical documentation","title":"gnuplot"},{"location":"generated/h/HTSeq/package/","text":"HTSeq Technical documentation","title":"HTSeq"},{"location":"generated/h/HTSeq/package/#htseq","text":"Technical documentation","title":"HTSeq"},{"location":"generated/h/HTSlib/package/","text":"HTSlib Technical documentation","title":"HTSlib"},{"location":"generated/h/HTSlib/package/#htslib","text":"Technical documentation","title":"HTSlib"},{"location":"generated/i/IOzone/package/","text":"IOzone Technical documentation","title":"IOzone"},{"location":"generated/i/IOzone/package/#iozone","text":"Technical documentation","title":"IOzone"},{"location":"generated/i/IntelPython3/package/","text":"IntelPython3 Technical documentation","title":"IntelPython3"},{"location":"generated/i/IntelPython3/package/#intelpython3","text":"Technical documentation","title":"IntelPython3"},{"location":"generated/i/IntelPython3-Packages/package/","text":"IntelPython3-Packages Technical documentation","title":"IntelPython3-Packages"},{"location":"generated/i/IntelPython3-Packages/package/#intelpython3-packages","text":"Technical documentation","title":"IntelPython3-Packages"},{"location":"generated/i/i-PI/package/","text":"i-PI Technical documentation","title":"i-PI"},{"location":"generated/i/i-PI/package/#i-pi","text":"Technical documentation","title":"i-PI"},{"location":"generated/j/Java/package/","text":"Java Technical documentation","title":"Java"},{"location":"generated/j/Java/package/#java","text":"Technical documentation","title":"Java"},{"location":"generated/k/Kraken2/package/","text":"Kraken2 Technical documentation","title":"Kraken2"},{"location":"generated/k/Kraken2/package/#kraken2","text":"Technical documentation","title":"Kraken2"},{"location":"generated/k/kim-api/package/","text":"kim-api Technical documentation","title":"kim-api"},{"location":"generated/k/kim-api/package/#kim-api","text":"Technical documentation","title":"kim-api"},{"location":"generated/l/LAMMPS/package/","text":"LAMMPS Technical documentation","title":"LAMMPS"},{"location":"generated/l/LAMMPS/package/#lammps","text":"Technical documentation","title":"LAMMPS"},{"location":"generated/l/LMDB/package/","text":"LMDB Technical documentation","title":"LMDB"},{"location":"generated/l/LMDB/package/#lmdb","text":"Technical documentation","title":"LMDB"},{"location":"generated/l/libgd/package/","text":"libgd Technical documentation","title":"libgd"},{"location":"generated/l/libgd/package/#libgd","text":"Technical documentation","title":"libgd"},{"location":"generated/l/libogg/package/","text":"libogg Technical documentation","title":"libogg"},{"location":"generated/l/libogg/package/#libogg","text":"Technical documentation","title":"libogg"},{"location":"generated/l/librosa/package/","text":"librosa Technical documentation","title":"librosa"},{"location":"generated/l/librosa/package/#librosa","text":"Technical documentation","title":"librosa"},{"location":"generated/l/libsndfile/package/","text":"libsndfile Technical documentation","title":"libsndfile"},{"location":"generated/l/libsndfile/package/#libsndfile","text":"Technical documentation","title":"libsndfile"},{"location":"generated/l/libtheora/package/","text":"libtheora Technical documentation","title":"libtheora"},{"location":"generated/l/libtheora/package/#libtheora","text":"Technical documentation","title":"libtheora"},{"location":"generated/l/libvdwxc/package/","text":"libvdwxc Technical documentation","title":"libvdwxc"},{"location":"generated/l/libvdwxc/package/#libvdwxc","text":"Technical documentation","title":"libvdwxc"},{"location":"generated/l/libvorbis/package/","text":"libvorbis Technical documentation","title":"libvorbis"},{"location":"generated/l/libvorbis/package/#libvorbis","text":"Technical documentation","title":"libvorbis"},{"location":"generated/m/MATLAB/package/","text":"MATLAB Technical documentation","title":"MATLAB"},{"location":"generated/m/MATLAB/package/#matlab","text":"Technical documentation","title":"MATLAB"},{"location":"generated/m/MEGAHIT/package/","text":"MEGAHIT Technical documentation","title":"MEGAHIT"},{"location":"generated/m/MEGAHIT/package/#megahit","text":"Technical documentation","title":"MEGAHIT"},{"location":"generated/m/Maple/package/","text":"Maple Technical documentation","title":"Maple"},{"location":"generated/m/Maple/package/#maple","text":"Technical documentation","title":"Maple"},{"location":"generated/m/Mathematica/package/","text":"Mathematica Technical documentation","title":"Mathematica"},{"location":"generated/m/Mathematica/package/#mathematica","text":"Technical documentation","title":"Mathematica"},{"location":"generated/m/miniasm/package/","text":"miniasm Technical documentation","title":"miniasm"},{"location":"generated/m/miniasm/package/#miniasm","text":"Technical documentation","title":"miniasm"},{"location":"generated/m/minimap2/package/","text":"minimap2 Technical documentation","title":"minimap2"},{"location":"generated/m/minimap2/package/#minimap2","text":"Technical documentation","title":"minimap2"},{"location":"generated/m/molmod/package/","text":"molmod Technical documentation","title":"molmod"},{"location":"generated/m/molmod/package/#molmod","text":"Technical documentation","title":"molmod"},{"location":"generated/m/monitor/package/","text":"monitor Technical documentation","title":"monitor"},{"location":"generated/m/monitor/package/#monitor","text":"Technical documentation","title":"monitor"},{"location":"generated/n/NAMD/package/","text":"NAMD Technical documentation","title":"NAMD"},{"location":"generated/n/NAMD/package/#namd","text":"Technical documentation","title":"NAMD"},{"location":"generated/n/NEST/package/","text":"NEST Technical documentation","title":"NEST"},{"location":"generated/n/NEST/package/#nest","text":"Technical documentation","title":"NEST"},{"location":"generated/n/NTPoly/package/","text":"NTPoly Technical documentation","title":"NTPoly"},{"location":"generated/n/NTPoly/package/#ntpoly","text":"Technical documentation","title":"NTPoly"},{"location":"generated/n/NetPyNE/package/","text":"NetPyNE Technical documentation","title":"NetPyNE"},{"location":"generated/n/NetPyNE/package/#netpyne","text":"Technical documentation","title":"NetPyNE"},{"location":"generated/n/netCDF/package/","text":"netCDF Technical documentation","title":"netCDF"},{"location":"generated/n/netCDF/package/#netcdf","text":"Technical documentation","title":"netCDF"},{"location":"generated/n/numba/package/","text":"numba Technical documentation","title":"numba"},{"location":"generated/n/numba/package/#numba","text":"Technical documentation","title":"numba"},{"location":"generated/o/OpenFOAM/package/","text":"OpenFOAM Technical documentation","title":"OpenFOAM"},{"location":"generated/o/OpenFOAM/package/#openfoam","text":"Technical documentation","title":"OpenFOAM"},{"location":"generated/o/OpenMX/package/","text":"OpenMX Technical documentation","title":"OpenMX"},{"location":"generated/o/OpenMX/package/#openmx","text":"Technical documentation","title":"OpenMX"},{"location":"generated/p/PLUMED/package/","text":"PLUMED Technical documentation","title":"PLUMED"},{"location":"generated/p/PLUMED/package/#plumed","text":"Technical documentation","title":"PLUMED"},{"location":"generated/p/PROJ/package/","text":"PROJ Technical documentation","title":"PROJ"},{"location":"generated/p/PROJ/package/#proj","text":"Technical documentation","title":"PROJ"},{"location":"generated/p/Pandoc/package/","text":"Pandoc Technical documentation","title":"Pandoc"},{"location":"generated/p/Pandoc/package/#pandoc","text":"Technical documentation","title":"Pandoc"},{"location":"generated/p/ParaView/package/","text":"ParaView Technical documentation","title":"ParaView"},{"location":"generated/p/ParaView/package/#paraview","text":"Technical documentation","title":"ParaView"},{"location":"generated/p/Perl/package/","text":"Perl Technical documentation","title":"Perl"},{"location":"generated/p/Perl/package/#perl","text":"Technical documentation","title":"Perl"},{"location":"generated/p/Pysam/package/","text":"Pysam Technical documentation","title":"Pysam"},{"location":"generated/p/Pysam/package/#pysam","text":"Technical documentation","title":"Pysam"},{"location":"generated/p/Python/package/","text":"Python Technical documentation","title":"Python"},{"location":"generated/p/Python/package/#python","text":"Technical documentation","title":"Python"},{"location":"generated/p/parallel/package/","text":"parallel Technical documentation","title":"parallel"},{"location":"generated/p/parallel/package/#parallel","text":"Technical documentation","title":"parallel"},{"location":"generated/p/phonopy/package/","text":"phonopy Technical documentation","title":"phonopy"},{"location":"generated/p/phonopy/package/#phonopy","text":"Technical documentation","title":"phonopy"},{"location":"generated/p/protobuf/package/","text":"protobuf Technical documentation","title":"protobuf"},{"location":"generated/p/protobuf/package/#protobuf","text":"Technical documentation","title":"protobuf"},{"location":"generated/q/Quantum-KITE/package/","text":"Quantum-KITE Technical documentation","title":"Quantum-KITE"},{"location":"generated/q/Quantum-KITE/package/#quantum-kite","text":"Technical documentation","title":"Quantum-KITE"},{"location":"generated/q/QuantumESPRESSO/package/","text":"QuantumESPRESSO Technical documentation","title":"QuantumESPRESSO"},{"location":"generated/q/QuantumESPRESSO/package/#quantumespresso","text":"Technical documentation","title":"QuantumESPRESSO"},{"location":"generated/r/R/package/","text":"R Technical documentation","title":"R"},{"location":"generated/r/R/package/#r","text":"Technical documentation","title":"R"},{"location":"generated/r/RAxML-NG/package/","text":"RAxML-NG Technical documentation","title":"RAxML-NG"},{"location":"generated/r/RAxML-NG/package/#raxml-ng","text":"Technical documentation","title":"RAxML-NG"},{"location":"generated/r/Racon/package/","text":"Racon Technical documentation","title":"Racon"},{"location":"generated/r/Racon/package/#racon","text":"Technical documentation","title":"Racon"},{"location":"generated/r/Roary/package/","text":"Roary Technical documentation","title":"Roary"},{"location":"generated/r/Roary/package/#roary","text":"Technical documentation","title":"Roary"},{"location":"generated/r/re2c/package/","text":"re2c Technical documentation","title":"re2c"},{"location":"generated/r/re2c/package/#re2c","text":"Technical documentation","title":"re2c"},{"location":"generated/s/SAMtools/package/","text":"SAMtools Technical documentation","title":"SAMtools"},{"location":"generated/s/SAMtools/package/#samtools","text":"Technical documentation","title":"SAMtools"},{"location":"generated/s/SCOTCH/package/","text":"SCOTCH Technical documentation","title":"SCOTCH"},{"location":"generated/s/SCOTCH/package/#scotch","text":"Technical documentation","title":"SCOTCH"},{"location":"generated/s/SCons/package/","text":"SCons Technical documentation","title":"SCons"},{"location":"generated/s/SCons/package/#scons","text":"Technical documentation","title":"SCons"},{"location":"generated/s/SMALT/package/","text":"SMALT Technical documentation","title":"SMALT"},{"location":"generated/s/SMALT/package/#smalt","text":"Technical documentation","title":"SMALT"},{"location":"generated/s/SPAdes/package/","text":"SPAdes Technical documentation","title":"SPAdes"},{"location":"generated/s/SPAdes/package/#spades","text":"Technical documentation","title":"SPAdes"},{"location":"generated/s/ScaFaCos/package/","text":"ScaFaCos Technical documentation","title":"ScaFaCos"},{"location":"generated/s/ScaFaCos/package/#scafacos","text":"Technical documentation","title":"ScaFaCos"},{"location":"generated/s/SuperLU_DIST/package/","text":"SuperLU_DIST Technical documentation","title":"SuperLU_DIST"},{"location":"generated/s/SuperLU_DIST/package/#superlu_dist","text":"Technical documentation","title":"SuperLU_DIST"},{"location":"generated/s/snappy/package/","text":"snappy Technical documentation","title":"snappy"},{"location":"generated/s/snappy/package/#snappy","text":"Technical documentation","title":"snappy"},{"location":"generated/t/TensorFlow/package/","text":"TensorFlow Technical documentation","title":"TensorFlow"},{"location":"generated/t/TensorFlow/package/#tensorflow","text":"Technical documentation","title":"TensorFlow"},{"location":"generated/t/Towhee/package/","text":"Towhee Technical documentation","title":"Towhee"},{"location":"generated/t/Towhee/package/#towhee","text":"Technical documentation","title":"Towhee"},{"location":"generated/t/Trimmomatic/package/","text":"Trimmomatic Technical documentation","title":"Trimmomatic"},{"location":"generated/t/Trimmomatic/package/#trimmomatic","text":"Technical documentation","title":"Trimmomatic"},{"location":"generated/u/Unicycler/package/","text":"Unicycler Technical documentation","title":"Unicycler"},{"location":"generated/u/Unicycler/package/#unicycler","text":"Technical documentation","title":"Unicycler"},{"location":"generated/v/VSEARCH/package/","text":"VSEARCH Technical documentation","title":"VSEARCH"},{"location":"generated/v/VSEARCH/package/#vsearch","text":"Technical documentation","title":"VSEARCH"},{"location":"generated/w/Wannier90/package/","text":"Wannier90 Technical documentation","title":"Wannier90"},{"location":"generated/w/Wannier90/package/#wannier90","text":"Technical documentation","title":"Wannier90"},{"location":"generated/w/WannierTools/package/","text":"WannierTools Technical documentation","title":"WannierTools"},{"location":"generated/w/WannierTools/package/#wanniertools","text":"Technical documentation","title":"WannierTools"},{"location":"generated/x/X11/package/","text":"X11 Technical documentation","title":"X11"},{"location":"generated/x/X11/package/#x11","text":"Technical documentation","title":"X11"},{"location":"generated/y/Yambo/package/","text":"Yambo Technical documentation","title":"Yambo"},{"location":"generated/y/Yambo/package/#yambo","text":"Technical documentation","title":"Yambo"},{"location":"generated/y/yaff/package/","text":"yaff Technical documentation","title":"yaff"},{"location":"generated/y/yaff/package/#yaff","text":"Technical documentation","title":"yaff"}]}